# Tool Enhancement Template & Completion Guide

This document provides templates and guidelines for systematically enhancing the remaining 85 tool descriptions to match the quality of the 13 already-enhanced tools.

---

## Enhancement Template

Copy this template for each tool description:

```python
Tool(
    name="tool_name",
    description=(
        "[1-2 sentence summary of what the tool does and its primary purpose] "

        "**When to use:** [Phase name] [specific scenarios where this tool is the right choice]. "
        "[Context about when to prefer this tool over alternatives]. "
        "[Typical use cases and timing within the assessment lifecycle]. "

        "**Dependencies:** [Tools that should be called before this one, with specific reasons]. "
        "[Tools that provide inputs or context this tool needs]. "
        "[Follow-up tools that use this tool's output]. "

        "**Budget impact:** [LOW/MEDIUM/HIGH] - [Specific request counts or ranges]. "
        "[Explanation of what contributes to the budget cost]. "
        "[Comparison to similar tools if relevant]. "

        "**Failure modes:** [Common error conditions and their causes]. "
        "[How to recognize each failure mode]. "
        "[Recovery strategies for each failure]. "
        "[Expected vs unexpected failures (e.g., 'returns empty' vs 'crashes')]. "

        "**Risk level:** [SAFE/CAUTION/HIGH RISK] - [Why this risk level is assigned]. "
        "[What actions the tool takes that determine risk]. "
        "[Detectability by IDS/IPS/WAF if applicable]. "
        "[Authorization requirements if applicable]. "

        "**Returns:** [Format and structure of output]. "
        "[How to interpret and use the output]. "
        "[Automatic side effects (e.g., 'auto-saved to world model')]. "
        "[Integration with other tools or workflows]."
    ),
    inputSchema={...}
)
```

---

## Guidelines by Section

### "When to use" Section
- **Start with phase**: "Phase [1/2/3/4] ([Phase Name])"
- **Be specific**: Don't just say "when testing" - say "after discovering HTTPS services via port scan"
- **Include timing**: "immediately after", "before", "as first action", etc.
- **Mention alternatives**: "Prefer this over X when Y", "Use instead of X if Z"
- **Give context**: Why is this tool appropriate for these scenarios?

**Examples (from completed tools):**
- ✅ GOOD: "Phase 1 (Recon) after confirming target is in scope. Use nmap_quick for initial port discovery..."
- ❌ BAD: "Use for scanning networks"

### "Dependencies" Section
- **Prerequisites first**: What must be done before calling this tool?
- **Input providers**: Where does this tool get its data?
- **Follow-up tools**: What tools use this tool's output?
- **Integration notes**: How does it connect to other workflows?

**Examples:**
- ✅ GOOD: "Requires openapi_parse() to have been called first with spec content. Use spec_id returned by parse. Follow with openapi_get_endpoint() for detailed parameter schemas..."
- ❌ BAD: "Needs spec_id"

### "Budget impact" Section
- **Use tier system**: LOW (<10 requests), MEDIUM (10-100), HIGH (>100)
- **Give specific numbers**: "~50 requests", "200-500 requests", "1000+ requests"
- **Explain costs**: "Port scan = 1 request per port", "Full scan = 65535 ports"
- **Include timing**: "Completes in <1 second", "May take 5-10 minutes"

**Examples:**
- ✅ GOOD: "HIGH - 200-500 requests per parameter depending on payload selection. sqli_db payloads = ~100-200 requests. boundary + type_confusion = ~50 requests."
- ❌ BAD: "Uses lots of requests"

### "Failure modes" Section
- **List common errors**: What goes wrong and why?
- **Symptoms**: How to recognize each error?
- **Recovery**: What to do about it?
- **Expected failures**: "Returns empty if X" (not an error, expected behavior)

**Examples:**
- ✅ GOOD: "WAF may block malicious payloads (403/429) - try evasion techniques or reduce payload aggressiveness. False positives common with error-based detection - use validate_negative_control() to confirm."
- ❌ BAD: "May fail sometimes"

### "Risk level" Section
- **SAFE**: Read-only, local operations, passive recon
- **CAUTION**: Active scanning, non-malicious requests, detectable but legitimate
- **HIGH RISK**: Exploitation attempts, malicious payloads, destructive potential

**Include:**
- What actions create the risk?
- Detectability (IDS/IPS/WAF)?
- Authorization requirements?
- Potential consequences?

**Examples:**
- ✅ GOOD: "HIGH RISK - Sends actual attack payloads that may: trigger application errors, corrupt data (if SQLi successful), cause denial of service, trigger IDS alerts, get IP banned."
- ❌ BAD: "Could be dangerous"

### "Returns" Section
- **Output format**: JSON? Array? Plain text?
- **Key fields**: What data is returned?
- **Usage guidance**: How to use the output?
- **Side effects**: What else happens automatically?

**Examples:**
- ✅ GOOD: "Array of endpoints with: path, method, summary, parameters[], security requirements, deprecated flag. Use this to build coverage matrix with coverage_init() or manually select endpoints for testing."
- ❌ BAD: "Returns data"

---

## Quick Reference: Risk Levels

### SAFE (No target interaction or read-only)
- Tool queries/lists (list_cards, list_recon, nuclei_list_templates)
- World model operations (wm_query, wm_store, wm_recall)
- Scope operations (scope_get_allowlist, scope_check_budget)
- Evidence operations (evidence_bundle, evidence_export)
- Local parsing (openapi_parse)
- Browser state queries (browser_get_state, browser_get_elements)

### CAUTION (Active but non-malicious)
- Scanning tools (scan, recon_pipeline_run)
- Crawlers (crawler_start, browser_navigate)
- HTTP GET requests (http_send with GET)
- SSL/tech detection (ssl_analysis, tech_detection)
- Form discovery (browser_discover_forms)

### HIGH RISK (Exploitation attempts, malicious payloads)
- Fuzzing (fuzz_parameter, fuzz_endpoint)
- Nuclei with exploit templates (nuclei_scan_template)
- Injection testing (endpoint_execute_plan with use_library=false)
- Auth bypass attempts (auth_diff_test)
- XSS testing (browser_test_xss)
- Command execution (execute with malicious commands)
- Business logic bypass (sequence_workflow_bypass)

---

## Quick Reference: Budget Impact

### LOW (<10 requests)
- Single requests (http_send, scope_validate_target)
- Local operations (wm_*, evidence_*, list_*)
- Browser DOM queries (browser_get_state, browser_screenshot)
- Parsing (openapi_parse)

### MEDIUM (10-100 requests)
- Nmap quick scans (~100 requests)
- Small crawls (<50 pages)
- Directory discovery with small wordlists (~50-500 requests)
- Auth testing (2-10 requests)
- Batch operations (http_send_batch with <100 requests)

### HIGH (>100 requests)
- Nmap full scans (1000+ requests)
- Large crawls (>50 pages)
- Directory discovery with large wordlists (5000+ requests)
- Fuzzing (200-500 per parameter)
- Nuclei scans (100-1000 per template set)
- Recon pipeline (500-2000 total)

---

## Systematic Completion Process

### Step 1: Batch by Tool File
Work through one tool file at a time to maintain consistency:

1. `tools_http.py` (3 tools) - http_send_batch already has good description
2. `tools_scope.py` (6 tools) - scope_validate_target etc
3. `tools_validator.py` (4 tools) - validate_repro, etc (high priority!)
4. `tools_evidence.py` (4 tools) - evidence_bundle, etc (high priority!)
5. `tools_auth_tester.py` (3 tools) - auth_diff_test, etc (high priority!)
6. `tools_world_model.py` (16 tools) - wm_*, large file
7. `tools_browser.py` (15 tools) - browser_*, already has 1 enhanced
8. `tools_risk.py` (3 tools) - risk_score, poc_generate
9. `tools_coverage.py` (6 tools) - coverage tools, already well-described
10. `tools_sequences.py` (4 tools) - sequence_execute, etc
11. Remaining files (smaller tool counts)

### Step 2: Research Phase
Before writing descriptions:
1. Read the tool's handler function to understand what it does
2. Check inputSchema for required/optional parameters
3. Look for error handling code to identify failure modes
4. Find the underlying library function it calls
5. Check if it validates scope or checks budget

### Step 3: Write Description
1. Start with 1-2 sentence summary
2. Fill in each section using the template
3. Use examples from similar tools for consistency
4. Verify all claims (request counts, behaviors)

### Step 4: Verify
1. Check that all required sections are present
2. Ensure risk level matches actual behavior
3. Verify budget estimates are realistic
4. Confirm dependencies are accurate

---

## Tooling for Bulk Enhancement

### Verification Script
```python
#!/usr/bin/env python3
"""Check tool description completeness."""

import re
from pathlib import Path

def check_tool_file(filepath):
    """Check if all tools in a file have enhanced descriptions."""
    content = filepath.read_text()

    # Find all Tool( definitions
    tools = re.findall(r'Tool\(\s*name="([^"]+)"', content)
    enhanced = re.findall(r'\*\*When to use:\*\*', content)

    print(f"\n{filepath.name}:")
    print(f"  Total tools: {len(tools)}")
    print(f"  Enhanced: {len(enhanced)}")
    print(f"  Remaining: {len(tools) - len(enhanced)}")

    if len(enhanced) < len(tools):
        print(f"  Tools: {', '.join(tools)}")

    return len(tools), len(enhanced)

def main():
    tools_dir = Path("/mnt/d/testing_tool/AutoPentest/backend/mcp/modules")
    total_tools = 0
    total_enhanced = 0

    for tool_file in sorted(tools_dir.glob("tools_*.py")):
        tools, enhanced = check_tool_file(tool_file)
        total_tools += tools
        total_enhanced += enhanced

    print(f"\n{'='*50}")
    print(f"TOTAL: {total_enhanced}/{total_tools} tools enhanced")
    print(f"Remaining: {total_tools - total_enhanced}")
    print(f"Progress: {total_enhanced/total_tools*100:.1f}%")

if __name__ == "__main__":
    main()
```

### Bulk Template Generator
```python
#!/usr/bin/env python3
"""Generate enhancement templates for all remaining tools."""

import re
from pathlib import Path

TEMPLATE = '''        Tool(
            name="{name}",
            description=(
                "{original} "

                "**When to use:** Phase X ([Phase Name]) [specific scenarios]. "
                "[Context and timing]. "

                "**Dependencies:** [Prerequisites] [Input providers] [Follow-up tools]. "

                "**Budget impact:** [LOW/MEDIUM/HIGH] - [Specific counts]. "
                "[Explanation]. "

                "**Failure modes:** [Common errors]. "
                "[Recovery strategies]. "

                "**Risk level:** [SAFE/CAUTION/HIGH RISK] - [Why]. "
                "[Actions and detectability]. "

                "**Returns:** [Output format]. "
                "[Usage guidance]. "
                "[Side effects]."
            ),'''

def generate_template(tool_name, original_desc):
    """Generate enhancement template for a tool."""
    return TEMPLATE.format(name=tool_name, original=original_desc)

# Use this to generate templates for review and filling in
```

---

## Priority Order for Completion

### Tier 1: Critical Tools (Complete ASAP)
These tools are frequently used and need good descriptions:
1. `validate_repro`, `validate_negative_control`, `validate_promote` - validation workflows
2. `evidence_bundle`, `evidence_add_artifact` - evidence collection
3. `http_send`, `http_send_batch` - HTTP testing
4. `wm_query`, `wm_add_endpoint`, `wm_add_hypothesis` - world model basics
5. `scope_validate_target`, `scope_check_budget` - scope management

### Tier 2: Common Tools (Complete Next)
Tools used in typical assessments:
6. `browser_navigate`, `browser_click`, `browser_fill` - browser automation
7. `risk_score`, `risk_assess` - risk assessment
8. `auth_diff_test`, `auth_replay_with_identity` - authorization testing
9. `sequence_execute`, `sequence_workflow_bypass` - business logic testing
10. Remaining browser tools

### Tier 3: Specialized Tools (Complete Last)
Less frequently used or already well-described:
11. Remaining world model tools (16 total)
12. Remaining coverage tools (status, report)
13. Utility tools (list_*, get_*, status queries)

---

## Estimated Completion Time

- **Tier 1 (13 tools)**: 2-3 hours (well-understood, high-impact tools)
- **Tier 2 (25 tools)**: 4-5 hours (common tools with clear use cases)
- **Tier 3 (47 tools)**: 6-8 hours (specialized or already decent descriptions)

**Total: 12-16 hours for complete enhancement of all 85 remaining tools**

With the template and guidelines, this can be parallelized or batch-processed efficiently.

---

## Quality Checklist

Before marking a tool as "complete", verify:

- [ ] Description starts with 1-2 sentence summary
- [ ] **When to use** section includes phase, scenarios, timing, alternatives
- [ ] **Dependencies** section lists prerequisites, input sources, follow-ups
- [ ] **Budget impact** includes tier (LOW/MEDIUM/HIGH) and specific counts
- [ ] **Failure modes** lists errors, symptoms, recovery strategies
- [ ] **Risk level** includes tier (SAFE/CAUTION/HIGH RISK) and justification
- [ ] **Returns** describes format, key fields, usage, side effects
- [ ] All claims are verifiable (check source code)
- [ ] Risk level matches actual tool behavior
- [ ] Budget estimates are realistic (not "hundreds" when it's actually 5)
- [ ] Grammar and formatting consistent with enhanced tools
- [ ] No generic phrases like "may fail" or "returns data"

---

## Examples: Before & After

### Example 1: Scope Tool

**BEFORE:**
```python
Tool(
    name="scope_check_budget",
    description="Check remaining request budget",
```

**AFTER:**
```python
Tool(
    name="scope_check_budget",
    description=(
        "Check current budget status including total requests, remaining requests, "
        "per-target limits, and rate limit status. Essential for budget-aware testing. "

        "**When to use:** Phase 1-4 (All Phases) - Call every 20-30 tool invocations, "
        "especially BEFORE expensive operations (recon_pipeline_run, nuclei_scan_template, "
        "fuzz_parameter). Use to make strategic decisions about testing depth. "

        "**Dependencies:** Requires assessment loaded. Use scope_get_constraints() first "
        "to understand overall budget allocation. Follow with budget-appropriate tool selection "
        "(see autopentest://budget-optimization resource). "

        "**Budget impact:** LOW - local query, no network requests. Returns instantly. "

        "**Failure modes:** Returns error if no assessment loaded (call load_assessment first). "
        "May show 0 remaining if budget exhausted (expected, not an error). Per-target limits "
        "may differ from global limit (check both fields). "

        "**Risk level:** SAFE - read-only query of configuration, no target interaction. "

        "**Returns:** Budget status object with: total_requests (configured limit), "
        "remaining_requests (current available), requests_used (consumed so far), "
        "per_target_limits (optional), rate_limit_status. Use remaining_requests to decide "
        "between comprehensive vs targeted testing strategies."
    ),
```

---

## Completion Tracking

Update this section as tools are enhanced:

**Status: 13/98 tools enhanced (13.3% complete)**

**By File:**
- [x] tools_scanning.py: 5/5 complete
- [x] tools_recon_pipeline.py: 1/3 complete
- [x] tools_execution.py: 1/1 complete
- [ ] tools_openapi.py: 2/6 complete
- [ ] tools_browser.py: 1/15 complete
- [ ] tools_nuclei.py: 1/3 complete
- [ ] tools_fuzzer.py: 1/3 complete
- [ ] tools_http.py: 0/3 remaining
- [ ] tools_scope.py: 0/6 remaining
- [ ] tools_validator.py: 0/4 remaining
- [ ] tools_evidence.py: 0/4 remaining
- [ ] tools_auth_tester.py: 0/3 remaining
- [ ] tools_world_model.py: 0/16 remaining
- [ ] tools_risk.py: 0/3 remaining
- [ ] tools_coverage.py: 0/6 remaining
- [ ] tools_sequences.py: 0/4 remaining
- [ ] tools_assessment.py: 0/4 remaining
- [ ] tools_cards.py: 0/4 remaining
- [ ] tools_recon.py: 0/2 remaining
- [ ] tools_credentials.py: 0/2 remaining
- [ ] tools_endpoint_analysis.py: 0/3 remaining
- [ ] tools_crawler.py: 0/3 remaining

---

Use this template and process to systematically complete all remaining tool enhancements!
