# CLAUDE.md — AutoPentest Testing Workflow Guide

**For LLM agents performing penetration tests using AutoPentest MCP**

---

## 1. Identity & Core Rules

You are conducting **authorized penetration testing** using AutoPentest — an AI-driven security assessment platform with 118 MCP tools across 23 modules.

### CRITICAL RULES

1. **Never fabricate results** — Do not generate fake scan output, vulnerabilities, findings, or exploits
2. **Be autonomous** — Do not ask for confirmation between phases. Keep testing until explicitly told to stop
3. **Scope validation is MANDATORY** — Call `scope_validate_target(url)` before EVERY HTTP request. Fail closed for safety
4. **Check budget regularly** — Call `scope_check_budget()` every 20-30 tool calls. Stop testing when budget is exhausted
5. **Never propose reports** — Only document findings with `add_card()`. Reports are generated ONLY when user explicitly requests via `evidence_generate_report()`

---

## 2. Assessment Lifecycle (5 Phases)

AutoPentest uses a gated 5-phase lifecycle. Each phase has entry requirements.

### Phase Overview

**Note:** Gate requirements are **minimum thresholds**, not targets. Complete thorough work in each phase before advancing.

| Phase | Name | Description | Minimum Gate Requirements |
|-------|------|-------------|---------------------------|
| **1** | **Reconnaissance** | Discover attack surface: subdomains, hosts, services | No gates (entry phase) |
| **2** | **Mapping & Enumeration** | Map endpoints, parameters, authentication flows | ≥3 assets, ≥5 endpoints (but aim for completeness) |
| **3** | **Vulnerability Assessment** | Test endpoints for vulnerability classes | ≥5 endpoints, ≥1 finding (but aim for 18+ findings, 80% coverage) |
| **4** | **Exploitation** | Exploit confirmed vulnerabilities with PoC generation | ≥1 confirmed hypothesis, ≥3 findings, ≥25% coverage |
| **5** | **Post-Exploitation & Reporting** | Document findings, generate report, clean up | ≥1 confirmed finding |

### Phase Gate Requirements (Minimum Thresholds)

**CRITICAL UNDERSTANDING:** Gates are **minimum requirements to ALLOW advancement**, not automatic triggers to FORCE advancement. They exist to **prevent premature phase transitions**, not to rush you through phases.

**Do NOT advance immediately upon hitting minimum thresholds.** Complete thorough work in each phase before advancing.

---

**Phase 1 → 2: Reconnaissance Complete**

**Minimum Gates (necessary but not sufficient):**
- ≥3 assets discovered
- ≥5 endpoints identified

**What "Complete" Actually Means:**
- ✅ All in-scope domains/IPs discovered and added as assets
- ✅ All open ports and services identified (nmap complete)
- ✅ Technology stack documented for each service
- ✅ Subdomain enumeration complete
- ✅ Initial vulnerability scanning done (nuclei misconfig/default-login checks)
- ✅ SSL/TLS analysis complete
- ✅ Recon data stored in world model (`wm_store`)

**Time Expectation:** 15-30 minutes depending on scope size

**Advance when:** You've extracted maximum reconnaissance value and are confident you've discovered the complete attack surface.

---

**Phase 2 → 3: Mapping Complete**

**Minimum Gates (necessary but not sufficient):**
- ≥5 endpoints mapped
- ≥1 finding recorded

**What "Complete" Actually Means:**
- ✅ All endpoints documented in world model (`wm_add_endpoint`)
- ✅ OpenAPI specs parsed (if available)
- ✅ Web application crawled (authenticated + unauthenticated)
- ✅ All forms and input vectors catalogued (`browser_discover_forms`)
- ✅ Coverage matrix initialized (`coverage_init` if using automated testing)
- ✅ At least one initial finding from simple header checks or recon
- ✅ Hypotheses created for major attack vectors (`wm_add_hypothesis`)

**Time Expectation:** 10-20 minutes depending on application complexity

**Advance when:** You have a complete map of the attack surface and understand all entry points for testing.

---

**Phase 3 → 4: Vulnerability Assessment Complete**

**Minimum Gates (necessary but not sufficient):**
- ≥1 confirmed hypothesis
- ≥3 findings recorded
- ≥25% coverage matrix tested

**What "Complete" Actually Means:**
- ✅ **8 simple vulnerability tests completed** (at least 6 of 8 from mandatory checklist)
- ✅ **≥18 findings recorded** (target: 20+ findings, not the minimum 3)
- ✅ **≥80% coverage matrix tested** (target: comprehensive coverage, not the minimum 25%)
- ✅ All high-priority endpoints tested (priority >70)
- ✅ All high-risk vulnerability classes tested (SQLi, XSS, IDOR, auth bypass)
- ✅ Findings include diverse vulnerability types (not just missing headers)
- ✅ Risk signals from exchange analysis reviewed and acted upon
- ✅ Budget remaining is appropriate for validation phase (≥50 requests)

**Time Expectation:** 30-60 minutes minimum (this is THE MOST IMPORTANT PHASE)

**Advance when:** You've systematically tested the application and are confident you've found the majority of vulnerabilities. If you're finding 1-2 new vulnerabilities every 10 minutes, keep testing. Only advance when new discovery rate drops significantly.

**WARNING:** Phase 3 is where most findings come from. Rushing through Phase 3 is the #1 reason Assessments 10/11 found only 8 findings vs Assessment 4's 24 findings. Spend adequate time here.

---

**Phase 4 → 5: Exploitation Complete**

**Minimum Gates (necessary but not sufficient):**
- ≥1 confirmed finding

**What "Complete" Actually Means:**
- ✅ All potential findings validated (`validate_repro` 3x minimum)
- ✅ All validated findings promoted to confirmed status (`validate_promote`)
- ✅ Evidence bundles created for all confirmed findings
- ✅ PoC scripts generated (`poc_generate`)
- ✅ Risk scores calculated for prioritization
- ✅ All artifacts collected (requests, responses, screenshots)

**Time Expectation:** 10-20 minutes depending on finding count

**Advance when:** All findings are validated, documented, and ready for reporting.

### Phase Management

- Check current phase: `orchestration_status()`
- Advance to next phase: `orchestration_advance(target_phase=N)`
- Force advance (skip gate): `orchestration_advance(target_phase=N, force=true)`
- Complete assessment: `orchestration_advance(target_phase=6)` (marks Phase 5 as done)

### ⚠️ Critical Phase Transition Rules

**DON'T rush through phases just because gates are met:**
- ❌ DON'T advance with only 3 assets when you could discover 10 more subdomains
- ❌ DON'T advance with only 5 endpoints when crawling would find 50 more
- ❌ DON'T advance from Phase 3 with only 3 findings when you could find 20+ more
- ❌ DON'T advance after 5 minutes when the phase should take 30-60 minutes

**DO complete thorough work before advancing:**
- ✅ DO spend 30-60 minutes in Phase 3 (the most important phase)
- ✅ DO aim for 18-24 findings in Phase 3 (matching Assessment 4's success)
- ✅ DO aim for 80% coverage in Phase 3 (not the minimum 25%)
- ✅ DO continue testing while finding new vulnerabilities at a good rate
- ✅ DO advance only when discovery rate drops and you're confident you've found most issues

**Think of gates as "Can I advance?" not "Should I advance?"** Just because you CAN advance doesn't mean you SHOULD.

---

## 3. THE PRIMARY Testing Workflow (Phase 3)

**This is the most important section.** Phase 3 is where vulnerabilities are discovered. Failed assessments skipped these steps.

### The Problem

**Assessments 10 & 11 found only 8 findings each. Assessment 4 found 24 findings.**

Why? Assessments 10/11 rushed through Phase 3 in <1 minute, skipped simple vulnerability tests, and never called the right tools.

### The Solution: Use the 8 LLM-in-the-Loop Pentest Tools

These tools expose **full HTTP traffic** to you for manual analysis. You see every request/response, control the flow, and make decisions.

### 8-Step Pentest Workflow

#### 1. `discover_attack_surface()`

**Purpose:** See what endpoints, parameters, and findings are already known

**Returns:**
- All endpoints discovered
- Known parameters on each endpoint
- Existing findings count
- Coverage status (tested/vulnerable/pending)
- Assets in scope

**When to use:** At the start of testing to understand the attack surface

---

#### 2. `recon_endpoint(url, method, ...)`

**Purpose:** Send baseline HTTP request, examine FULL response (up to 8KB+ body)

**Returns:**
- Complete request/response pair
- Response status, headers, body (up to `max_response_body` chars, default 8192)
- Timing data
- Content type detection (JSON/HTML)
- Exchange analysis (ONLY if `include_analysis=true` - opt-in for quick triage)

**What to look for:**
- Response status and error messages
- Reflected parameters in body
- JSON structure or HTML templates
- Technology signatures (framework errors, server headers)
- Authentication requirements (401/403)
- Rate limiting headers

**When to use:**
- Baseline each endpoint before testing
- Understand normal behavior before injecting payloads
- See full responses (unlike `endpoint_probe` which truncates to 500 chars)

---

#### 3. `analyze_headers(url, method, ...)`

**Purpose:** Check security headers and cookie configuration

**Returns:**
- Security headers present/missing (CSP, HSTS, X-Frame-Options, etc.)
- Cookies without HttpOnly or Secure flags
- Permissive CORS (Access-Control-Allow-Origin: *)
- Server version disclosure
- Weak cache headers
- Exchange analysis with risk signals

**What to look for:**
- Missing CSP, HSTS, X-Frame-Options
- Cookies without HttpOnly/Secure/SameSite flags
- Permissive CORS allowing any origin
- Server version disclosure in headers
- Cache-Control: public on sensitive endpoints

**When to use:** As part of simple vulnerability checklist (mandatory test #1)

---

#### 4. `get_test_payloads(vuln_class, limit, ...)`

**Purpose:** Get actual payload strings from PayloadRegistry (300+ payloads across 46 vulnerability classes)

**Returns:**
- Array of payload strings with metadata (technique, platform)
- Total available payloads count
- List of all available vulnerability classes

**Available vulnerability classes:**
- SQL injection: `sqli_error`, `sqli_blind_boolean`, `sqli_blind_time`, `sqli_union`
- XSS: `xss_reflected`, `xss_stored`, `xss_dom`
- Others: `ssti`, `ssrf`, `path_traversal`, `command_injection`, `ldap_injection`, `xml_injection`, `header_injection`, `nosql_injection`, `idor`, `auth_bypass`, `jwt_manipulation`, `cors_misconfig`, `open_redirect`, `mass_assignment`, `param_pollution`, `rate_limit_bypass`, etc.

**Decision tree (what vuln_class to use):**
- SQL error in response → `sqli_error`, `sqli_union`, `sqli_blind_time`
- Template rendering detected → `ssti`
- Parameter reflected in HTML → `xss_reflected`, `xss_dom`
- JSON API with IDs → `idor`, `nosql_injection`, `mass_assignment`
- File upload found → `path_traversal`, `command_injection`
- External URL parameter → `ssrf`, `open_redirect`
- XML content type → `xml_injection`
- JWT authentication → `jwt_manipulation`

**When to use:** Before running `inject_batch` to see available payloads for suspected vulnerability

---

#### 5. `inject_batch(url, parameter, location, payloads, ...)`

**Purpose:** Sweep multiple payloads against same injection point, compare to baseline

**Parameters:**
- `payloads`: Array of payload strings to test
- `location`: Where to inject (`query`, `body`, `path`, `header`, `cookie`)
- `include_baseline`: Send clean request first for comparison (default: true)
- `compact`: Return full body only for outliers (status change, body diff >10%, timing >2x, payload reflected). Non-outliers get metrics only. (default: true)
- `max_batch_size`: Cap payload count (default: 50, respects budget)
- `max_response_body`: Max response body to return per payload (default: 8192)
- `include_analysis`: Include exchange analysis (default: false - opt-in for quick triage)

**Returns:**
- Baseline response (if `include_baseline=true`)
- Array of results with full request/response for each payload
- Summary statistics:
  - Unique status codes
  - Payloads reflected count
  - Status changes count (vs baseline)
  - Timing anomalies count (>2x baseline)
- Baseline exchange analysis (ONLY if `include_analysis=true`)

**What to look for:**
- **Status code changes** (500 errors = likely SQLi/SSTI)
- **Response body length variance** (significant diff = injection success)
- **Timing anomalies** (>2x baseline = blind time-based injection)
- **Payloads reflected** in response
- **Error messages** revealing injection success

**Best practices:**
- Always include baseline for comparison
- Start with small payload set (10-25) for quick triage
- Cap `max_batch_size` to respect budget
- Look for outliers in summary stats first, then examine individual responses

**When to use:** Broad payload sweep to identify anomalies, then follow up with `inject_payload` for targeted tests

---

#### 6. `inject_payload(url, parameter, location, payload, ...)`

**Purpose:** Inject single payload, return full request/response for analysis

**Parameters:**
- `payload`: Single payload string to inject
- `location`: Where to inject (`query`, `body`, `path`, `header`, `cookie`)
- `max_response_body`: Max response body (default: 8192)
- `include_analysis`: Include exchange analysis (default: false - opt-in for quick triage)

**Returns:**
- Complete injected request/response pair
- Injection metadata (parameter, location, payload)
- Payload reflection status (whether payload appears in response body)
- Exchange analysis (ONLY if `include_analysis=true`)

**Injection locations explained:**
- `query`: URL query parameters (`?id=PAYLOAD`)
- `body`: JSON body `{"param": "PAYLOAD"}` or form data
- `path`: URL path segments `/users/{id}` where `{id}=PAYLOAD`
- `header`: Custom headers (e.g., `X-Forwarded-For: PAYLOAD`)
- `cookie`: Cookie values (e.g., `session=PAYLOAD`)

**Use cases:**
- Test custom payloads crafted for specific framework/DB
- Follow up on promising batch results with refined payloads
- Test context-specific injection (JSON vs URL-encoded vs path)
- Verify payload reflection and encoding behavior
- Check for WAF bypass with obfuscated payloads

**When to use:** Targeted testing after batch sweep identifies interesting behavior

---

#### 7. `record_finding(title, severity, url, vuln_class, parameter, evidence, ...)`

**Purpose:** Record confirmed vulnerability to world model + card system

**Parameters:**
- `title`: Finding title (e.g., "SQL Injection in /api/users")
- `severity`: `info` | `low` | `medium` | `high` | `critical`
- `url`: Vulnerable endpoint URL
- `vuln_class`: Vulnerability class (e.g., `sqli_error`, `xss_reflected`)
- `parameter`: Vulnerable parameter name
- `evidence`: Object with `request`, `response`, `payload`, `description` (stored as JSON)
- `remediation`: Remediation guidance (optional)
- `confidence`: Confidence score 0.0-1.0 (default: 0.8)
- `cvss_vector`: CVSS v3.1 vector string (optional, e.g., "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H")
- `cvss_score`: CVSS score 0.0-10.0 (optional, auto-computed from vector if not provided)
- `affected_endpoints`: Comma-separated URLs or JSON array (optional, defaults to url parameter)
- `attack_scenario`: Detailed description of how an attacker would exploit this (optional)
- `description`: Detailed vulnerability description (optional, auto-generated from evidence if not provided)

**Returns:**
- `finding_id`: ID of created finding
- `card_id`: ID of created card (same as finding_id)
- `success`: boolean

**When to use:**
- After confirming vulnerability through multiple payloads
- When you have clear evidence (error message, reflected payload, status change)
- When vulnerability is reproducible (not intermittent)
- When you understand the root cause and impact

**Severity guidelines:**
- **critical**: RCE, SQL injection with data exfil, auth bypass to admin
- **high**: XSS in admin panel, IDOR on sensitive data, SSRF to internal network
- **medium**: XSS in user context, CSRF, info disclosure
- **low**: Verbose errors, missing security headers
- **info**: Observations, potential misconfigurations

**Evidence structure:**
```json
{
  "request": {"method": "GET", "url": "...", "headers": {...}, "body": "..."},
  "response": {"status": 500, "body": "SQL error message..."},
  "payload": "' OR '1'='1",
  "description": "Confirmed SQLi via error message. MySQL syntax error returned."
}
```

**Finding display in UI:**
- All text fields (description, attack_scenario, recommendation) support **Markdown** formatting
- Use `**bold**`, `*italic*`, `` `code` ``, triple backticks for code blocks, lists, etc.
- Evidence is displayed as formatted HTTP request/response (request in green, response in amber, payload highlighted)
- CVSS scores are color-coded: Critical (9.0+) red, High (7.0+) orange, Medium (4.0+) yellow, Low (<4.0) blue

**Important:**
- Pentest tools return risk signals but do NOT auto-create findings. You MUST call `record_finding` to persist confirmed vulnerabilities.
- Auto-detected findings (from exchange analysis) automatically get CVSS vectors, attack scenarios, and status="confirmed"

---

#### 8. `get_test_progress(base_url)`

**Purpose:** Show what's tested, findings count, coverage completeness

**Returns:**
- Total endpoints count
- Tested endpoints count
- Coverage percentage
- Findings by severity (critical/high/medium/low/info)
- Budget used/remaining
- Vulnerability classes tested/untested
- Per-endpoint status (tests run, findings, status)

**Use to answer:**
- How many endpoints have I tested?
- What's my coverage percentage?
- How many findings by severity?
- How much budget remains?
- Which vuln classes are untested?
- Which endpoints need more testing?

**Success criteria:**
- Coverage >80% for comprehensive assessment
- All high-risk endpoints tested
- Budget usage proportional to phase (40% recon, 25% mapping, 25% testing, 10% validation)
- Findings validated and documented

**When to use:** Regularly during Phase 3 to verify coverage and track progress

---

## 4. Mandatory Simple Vulnerability Checklist (Phase 3)

**CRITICAL:** These 8 simple tests MUST be run in Phase 3 before advancing to Phase 4.

**Why:** Assessment 4 found 24 findings because it ran these basic tests. Assessments 10/11 found only 8 findings because they skipped these.

**Requirement:** Complete at least **6 of 8 tests** before Phase 4. These are low-hanging fruit that yield high-impact findings.

### Checklist

#### 1. Security Headers (CRITICAL)
- **Tool:** `analyze_headers(url)`
- **Effort:** 1-2 minutes
- **How:** Call `analyze_headers` on main URL and 2-3 key endpoints
- **Check for:**
  - Content-Security-Policy (CSP) header present?
  - HTTP Strict-Transport-Security (HSTS) header present?
  - X-Content-Type-Options: nosniff present?
  - X-Frame-Options present?
  - Cookies have HttpOnly, Secure, and SameSite flags?
- **Record if:** Missing CSP, HSTS, or insecure cookie flags
- **Severity:** medium (missing CSP/HSTS), low (other headers)
- **Typical findings:** Missing CSP, missing HSTS, cookies without HttpOnly/Secure flags

#### 2. Weak Password Policy (HIGH)
- **Tool:** `inject_payload`
- **Effort:** 2-3 minutes
- **How:** If registration or password-change endpoint exists, try setting passwords: `123456`, `password`, `a`, `test`
- **Check for:**
  - Are passwords shorter than 8 chars accepted?
  - Are common passwords (`password`, `123456`) accepted?
  - Is there a minimum complexity requirement?
  - Does the API return specific errors for weak passwords?
- **Record if:** Passwords shorter than 8 chars OR common passwords accepted
- **Severity:** medium

#### 3. Username Enumeration (HIGH)
- **Tool:** `inject_batch`
- **Effort:** 3-5 minutes
- **How:** Send login requests with known-valid username (if available) and known-invalid username (e.g., `nonexistent_user_xyz`). Compare responses.
- **Check for:**
  - Do valid and invalid usernames return different status codes (200 vs 404)?
  - Do valid and invalid usernames return different response bodies ("User not found" vs "Invalid password")?
  - Do valid and invalid usernames have different response times (>100ms difference)?
- **Record if:** Different responses for valid vs invalid usernames
- **Severity:** medium
- **Typical finding:** Login endpoint returns "User not found" for invalid users and "Invalid password" for valid users

#### 4. CSRF Protection (MEDIUM)
- **Tool:** `recon_endpoint`
- **Effort:** 2-3 minutes
- **How:** For state-changing endpoints (POST/PUT/DELETE), send request WITHOUT CSRF token
- **Check for:**
  - Do POST/PUT/DELETE endpoints require a CSRF token?
  - Do forms include hidden CSRF token fields?
  - Does the API use SameSite cookies as CSRF protection?
  - Can you perform state-changing actions without CSRF protection?
- **Record if:** State-changing requests succeed without CSRF token
- **Severity:** medium
- **Note:** Exchange analyzer auto-detects missing CSRF tokens in forms

#### 5. Default Credentials (HIGH)
- **Tool:** `inject_batch`
- **Effort:** 2-3 minutes
- **How:** Try default credential pairs: `admin:admin`, `admin:password`, `admin:123456`, `test:test`, `root:root`
- **Check for:**
  - Do any default credential pairs return 200 OK or session token?
  - Are there error messages revealing valid usernames?
  - Is there an admin or test account with default password?
- **Record if:** Any default credential pair grants access
- **Severity:** critical
- **Typical finding:** Admin account with password "admin" grants full access

#### 6. Input Validation (MEDIUM)
- **Tool:** `inject_batch`
- **Effort:** 3-5 minutes
- **How:** Send boundary values: empty string ``, very long string (5000 chars), special chars `<>"&`, negative numbers `-1`, zero, null
- **Check for:**
  - Does the API accept invalid input (negative IDs, empty required fields)?
  - Does the API return 500 errors or stack traces for invalid input?
  - Are there verbose error messages revealing internal logic?
  - Does the API validate data types?
- **Record if:** API accepts invalid input OR returns 500 errors with stack traces
- **Severity:** low (weak validation), medium (error disclosure)

#### 7. Rate Limiting (MEDIUM)
- **Tool:** `inject_batch`
- **Effort:** 2-3 minutes
- **How:** Send 20-30 rapid identical requests to login endpoint. Check for rate limiting (429 status, lockout, delay)
- **Check for:**
  - Does the API return 429 Too Many Requests after N attempts?
  - Does response time increase after repeated requests (throttling)?
  - Is there account lockout after N failed login attempts?
  - Are there rate-limiting headers (X-RateLimit-Remaining)?
- **Record if:** No rate limiting after 20+ rapid requests on sensitive endpoint
- **Severity:** medium
- **Typical finding:** Login endpoint accepts 1000+ login attempts with no rate limiting

#### 8. Error Handling (MEDIUM)
- **Tool:** `recon_endpoint`
- **Effort:** 2-3 minutes
- **How:** Request non-existent URLs (404), use invalid HTTP methods (OPTIONS on restricted endpoint), send malformed JSON
- **Check for:**
  - Do 404 pages reveal framework/version info (Django debug page, Spring error)?
  - Do 500 errors include stack traces with file paths?
  - Do error messages reveal internal logic?
  - Is debug mode enabled in production?
- **Record if:** Stack traces, debug info, or detailed error messages exposed
- **Severity:** medium (stack traces), low (verbose errors)
- **Note:** Exchange analyzer auto-detects stack traces

### Checklist Completion

**Minimum:** 6 of 8 tests completed
**Recommended:** All 8 tests completed
**Expected findings:** 3-8 findings from simple tests alone
**Total time:** 15-20 minutes
**Phase gate:** Verify at least 6 of 8 tests completed before advancing to Phase 4

---

## 5. DO / DON'T Rules (Derived from Real Failures)

### DO

✅ **Use `recon_endpoint` (full 8KB+ responses), NOT `endpoint_probe` (500-char truncated)**
✅ **Use pentest tools as PRIMARY workflow in Phase 3**
✅ **Run the 8 simple tests FIRST before anything else**
✅ **Call `testing_build_matrix` for systematic coverage** (if using automated testing engine)
✅ **Record ALL confirmed findings with `record_finding` immediately**
✅ **Compare injected responses to baseline** (status, length, timing)
✅ **Test all 5 injection locations** (query, body, path, header, cookie)
✅ **Use `inject_batch` for sweeps, then `inject_payload` for targeted follow-up**
✅ **Reason from raw HTTP responses** — look for status changes, error messages, reflected input, timing anomalies YOURSELF
✅ **Run `analyze_headers` first for automated signals, then reason independently from raw responses**
✅ **Check `get_test_progress` regularly**
✅ **Reach 25% coverage + 3 findings before advancing to Phase 4**
✅ **Spend 30-60 minutes minimum in Phase 3**

### DON'T

❌ **DON'T use `fuzz_endpoint`, `fuzz_parameter`, `fuzz_list_payloads` — DEPRECATED** (mock mode only, no real requests)
❌ **DON'T use `endpoint_probe` for testing** — it truncates responses to 500 chars
❌ **DON'T rush through Phase 3** — the gate requires 3 findings + 25% coverage
❌ **DON'T skip the simple vulnerability checklist**
❌ **DON'T rely on exchange analysis to tell you what to test** — analyze raw responses yourself
❌ **DON'T forget to call `record_finding` for confirmed vulns**
❌ **DON'T generate reports** unless user explicitly asks
❌ **DON'T classify CRITICAL severity** without confirmed exploitation
❌ **DON'T skip scope validation** — ever
❌ **DON'T use expensive tools** (`recon_pipeline_run`, `nuclei_scan_template`) when budget < 100

---

## 6. Phase Gate Requirements (Detailed)

### Phase 1 → 2

**Conditions:**
- ≥3 assets discovered
- ≥5 endpoints identified

**What to do:**
1. `subdomain_enum(domain)` or `recon_pipeline_run(domain)`
2. `scan(target, type="nmap_quick")`
3. `wm_add_asset(...)` for each discovered target
4. `openapi_parse(...)` or `crawler_start(...)` to discover endpoints
5. `orchestration_status()` to check metrics
6. `orchestration_advance(target_phase=2)` when ready

### Phase 2 → 3

**Conditions:**
- ≥5 endpoints mapped
- ≥1 finding recorded

**What to do:**
1. `crawler_start(...)` or `openapi_parse(...)`
2. `wm_add_endpoint(...)` for each discovered endpoint
3. Run simple vulnerability checklist (at least 1 test to get 1 finding)
4. `orchestration_status()` to check metrics
5. `orchestration_advance(target_phase=3)` when ready

### Phase 3 → 4

**Conditions:**
- ≥1 confirmed hypothesis
- ≥3 findings recorded
- ≥25% coverage matrix tested

**What to do:**
1. Run 8-step pentest workflow
2. Complete 6 of 8 simple vulnerability tests
3. Call `record_finding` for each confirmed vulnerability
4. Check `get_test_progress()` to verify coverage ≥25%
5. `wm_add_hypothesis(...)` and `wm_update_hypothesis(..., status="confirmed")`
6. `orchestration_status()` to check metrics
7. `orchestration_advance(target_phase=4)` when ready

### Phase 4 → 5

**Conditions:**
- ≥1 confirmed finding

**What to do:**
1. `validate_repro(card_id)` on findings
2. `validate_promote(card_id)` to confirm
3. `evidence_bundle(finding_id)` for confirmed findings
4. `orchestration_status()` to check metrics
5. `orchestration_advance(target_phase=5)` when ready

---

## 7. Budget Strategy

Request budget is a **limited resource**. Use it wisely based on remaining allocation.

### Budget Check Schedule

Call `scope_check_budget()` every 20-30 tool calls, especially **before:**
- `recon_pipeline_run` (500-2000 requests per domain)
- `nuclei_scan_template` (100-1000 requests per template set)
- `fuzz_endpoint` or `fuzz_parameter` (500+ requests per endpoint) — **DEPRECATED, avoid**
- `crawler_start` (50-200 requests per site depending on size)
- `scan` with `type='nmap_full'` or directory discovery (100-1000 requests)

### Budget-Based Strategy

**HIGH BUDGET (>500 requests remaining)**
- **Strategy:** Comprehensive discovery
- **Use:** `recon_pipeline_run`, `crawler_start(max_pages=200)`, `nuclei_scan_template`
- **Rationale:** Early phases need broad discovery

**MEDIUM BUDGET (100-500 requests remaining)**
- **Strategy:** Targeted testing only
- **Use:** `recon_endpoint`, `inject_batch`, `inject_payload`, `analyze_headers`, `http_send_batch`
- **Avoid:** `recon_pipeline_run`, `nuclei_scan_template` with broad tags, `scan(type='nmap_full')`
- **Rationale:** Focus on likely vulnerabilities

**LOW BUDGET (20-100 requests remaining)**
- **Strategy:** Validation only, stop new discovery
- **Use:** `validate_repro`, `validate_promote`, `evidence_bundle`, `http_send` (surgical)
- **Avoid:** ANY scanning tools, `crawler_start`
- **Rationale:** Preserve budget for confirming findings

**CRITICAL BUDGET (<20 requests remaining)**
- **Strategy:** Documentation only, stop all testing
- **Use:** `evidence_export`, `poc_generate`, `evidence_generate_report`, `wm_query`, `list_cards`
- **Stop Testing:** No more requests to targets
- **Rationale:** Budget exhausted, complete documentation

### Budget Costs Per Tool

**Pentest tools (LLM-in-the-loop):**
- `recon_endpoint`: 1 request
- `inject_payload`: 1 request
- `inject_batch`: 1 + N requests (N≤50)
- `analyze_headers`: 1 request
- `get_test_payloads`: 0 requests (local registry)
- `discover_attack_surface`: 0 requests (world model query)
- `record_finding`: 0 requests (world model write)
- `get_test_progress`: 0 requests (world model query)

**Other tools:**
- `http_send`: 1 request
- `http_send_batch`: N requests
- `crawler_start`: 50-200 requests
- `recon_pipeline_run`: 500-2000 requests
- `nuclei_scan_template`: 100-1000 requests
- `scan(type='nmap_quick')`: ~100 requests
- `scan(type='nmap_full')`: 1000+ requests

---

## 8. MCP Resources (Detailed Guidance)

AutoPentest provides 9 MCP resources with detailed guidance. Read these when you need in-depth information.

**Available resources:**
- `autopentest://pentest-workflow` — 8-step manual testing workflow + SQL injection example
- `autopentest://workflow-guide` — dynamic per-phase recommendations
- `autopentest://attack-patterns` — multi-tool exploitation chains
- `autopentest://error-recovery` — common errors + recovery steps
- `autopentest://budget-optimization` — budget strategies per phase
- `autopentest://tool-dependencies` — tool sequencing and prerequisites
- `autopentest://tool-metadata` — machine-readable tool categories, dependencies, risk levels
- `kali://status` — current assessment status and container info
- `kali://containers` — list of all Kali pentesting containers

**How to read:** Use `ReadMcpResourceTool` or reference URLs directly in your reasoning.

---

## 9. Finding Decision Trees

What to do when you find specific vulnerability types.

### SQL Injection

```
1. validate_repro (confirm injection works consistently)
2. validate_negative_control (test benign input doesn't trigger error)
3. validate_promote (mark as confirmed)
4. execute("sqlmap -u 'URL' -p 'param' --batch --risk=3 --level=5") (advanced exploitation)
5. execute("sqlmap ... --dbs --tables --dump") (enumerate database if authorized)
6. evidence_bundle + evidence_add_artifact (add sqlmap output, extracted data)
7. poc_generate (create PoC script for reproduction)
```

### XSS (Cross-Site Scripting)

```
1. validate_repro (confirm payload executes in browser)
2. browser_navigate + browser_eval("document.cookie") (verify cookie access)
3. browser_screenshot (capture visual proof of execution)
4. validate_negative_control (test HTML-encoded payload doesn't execute)
5. evidence_bundle + evidence_add_artifact (add screenshot, HTML, request/response)
6. poc_generate (create PoC showing cookie exfiltration - don't actually exfiltrate!)
```

### IDOR (Insecure Direct Object Reference)

```
1. auth_diff_test (compare access across users for same resource IDs)
2. sequence_data_ownership (test across multiple resource IDs systematically)
3. http_send_batch (enumerate all accessible IDs - respect budget limits!)
4. evidence_bundle + evidence_add_artifact (package findings with examples)
5. risk_score (calculate business impact based on data sensitivity)
```

### Authentication Bypass

```
1. validate_repro (confirm bypass is real, not timing-dependent)
2. validate_promote (mark as confirmed finding)
3. evidence_bundle + evidence_add_artifact (capture proof before state changes)
4. credentials_add (store bypassed credentials as usable identity)
5. crawler_start (re-crawl with new access level → discover authenticated endpoints)
6. coverage_discover (detect newly accessible endpoints, extend coverage matrix)
7. coverage_next → endpoint_execute_plan (test new attack surface for secondary vulns)
```

---

## 10. Error Recovery (Quick Reference)

### "Target out of scope"

```
1. scope_get_allowlist() to verify allowed targets
2. If target should be in scope: Inform user, ask them to update allowlist
3. If legitimately out of scope: Document as observation, skip testing
```

### "Budget exhausted"

```
1. scope_check_budget() to see remaining requests
2. If >50 requests remain: Prioritize high-value tests
3. If 20-50 requests: Only validate confirmed findings, skip new discovery
4. If <20 requests: Evidence collection and reporting only
5. If 0 requests: Stop testing, generate report
```

### "Crawl returned 0 pages"

```
1. browser_navigate (needs JS rendering? Try real browser)
2. crawler_start with identity_id (needs authentication? Try with creds)
3. execute("gobuster dir ...") (static site? Try directory discovery)
4. openapi_parse (API-only app? Parse spec instead of crawling)
```

### "Validation failed (cannot reproduce)"

```
1. Timing-dependent? Retry validate_repro() 2-3 times
2. Credentials expired? Re-authenticate and retry
3. WAF detected? Check for 403/429, try different payload encoding
4. Request parameters wrong? Verify exact payload format
5. Consistently fails after 3 attempts? Downgrade to "observation" not "finding"
```

### "Tool not available in container"

```
1. Check spelling: tool_help(tool_name) to verify correct name
2. Use alternative: gobuster instead of ffuf, subfinder instead of amass
3. Install tool: execute("apt-get update && apt-get install -y tool")
4. Use execute() with full path: /usr/bin/tool instead of tool
```

---

## Summary

**Read this file at the start of every AutoPentest session.** It contains the authoritative testing workflow.

**Key takeaways:**
1. Use the 8 LLM-in-the-loop pentest tools as your PRIMARY workflow in Phase 3
2. Complete the 8 simple vulnerability tests BEFORE anything else
3. Spend 30-60 minutes minimum in Phase 3 to reach 3 findings + 25% coverage
4. Always call `record_finding` for confirmed vulnerabilities
5. Check `get_test_progress` regularly to verify coverage
6. Never skip scope validation or budget checks
7. Never use deprecated fuzzer tools (`fuzz_endpoint`, `fuzz_parameter`)
8. Use `recon_endpoint` (full responses), NOT `endpoint_probe` (truncated)

**Expected outcome:** 18-24 findings per assessment (matching Assessment 4's manual testing results).

---

*This document supersedes `Docs/PrePrompt.txt` which is outdated.*
