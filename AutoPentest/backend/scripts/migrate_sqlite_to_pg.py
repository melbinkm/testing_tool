#!/usr/bin/env python3
"""
One-time migration script: SQLite world model databases -> PostgreSQL wm_* tables.

Finds all world-model-*.db files in the data directory, reads every row from
each SQLite table, and inserts into the corresponding wm_* PostgreSQL table
with the assessment_id extracted from the filename.

Usage:
    python3 backend/scripts/migrate_sqlite_to_pg.py [--data-dir ./data] [--db-url postgresql://...]

Requirements:
    pip install asyncpg aiosqlite
"""

from __future__ import annotations

import argparse
import asyncio
import glob
import json
import logging
import os
import re
import sqlite3
import sys
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple

# Add parent to path for config import
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

try:
    import asyncpg
except ImportError:
    print("ERROR: asyncpg is required. Install with: pip install asyncpg")
    sys.exit(1)

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger("migrate-sqlite-to-pg")

# ---------------------------------------------------------------------------
# Table mapping: SQLite table name -> (PostgreSQL table name, columns)
# ---------------------------------------------------------------------------

# Columns that are stored as TEXT in SQLite but should be JSONB in PostgreSQL
_JSON_COLUMNS = {
    "assets": ["metadata", "tags"],
    "endpoints": ["parameters", "metadata"],
    "identities": ["permissions", "metadata"],
    "hypotheses": ["evidence"],
    "observations": ["metadata"],
    "findings": ["evidence_ids", "metadata"],
    "knowledge": ["metadata", "tags"],
    "plans": ["steps"],
    "relationships": ["metadata"],
    "coverage_matrix": ["tool_args"],
}

# Column lists per table (matching PostgreSQL wm_* schema)
_TABLE_COLUMNS = {
    "assets": [
        "id", "kind", "name", "metadata", "tags", "discovered_at", "updated_at",
    ],
    "endpoints": [
        "id", "asset_id", "method", "path", "parameters", "auth_required",
        "status", "metadata", "discovered_at", "updated_at",
    ],
    "identities": [
        "id", "name", "auth_type", "scope", "permissions", "status",
        "metadata", "created_at", "updated_at",
    ],
    "hypotheses": [
        "id", "title", "description", "severity", "status", "evidence",
        "target_id", "created_at", "updated_at",
    ],
    "observations": [
        "id", "hypothesis_id", "type", "content", "metadata", "observed_at",
    ],
    "findings": [
        "id", "hypothesis_id", "title", "severity", "confidence", "status",
        "evidence_ids", "remediation", "metadata", "created_at", "updated_at",
    ],
    "knowledge": [
        "id", "source_tool", "category", "target", "title", "content",
        "chunk_index", "chunk_total", "parent_id", "content_size",
        "metadata", "tags", "created_at",
    ],
    "plans": [
        "id", "title", "goal", "steps", "status", "reflection",
        "created_at", "updated_at",
    ],
    "relationships": [
        "id", "source_type", "source_id", "target_type", "target_id",
        "rel_type", "metadata", "created_at",
    ],
    "coverage_matrix": [
        "id", "endpoint_id", "vuln_class", "parameter", "status", "tool_name",
        "tool_args", "finding_id", "result_summary", "priority",
        "attempted_at", "completed_at", "created_at", "updated_at",
    ],
}


def _parse_json_value(value: Any, col: str, json_cols: List[str]) -> Any:
    """Parse TEXT JSON column into a Python object for JSONB insertion."""
    if col not in json_cols:
        return value
    if value is None:
        return "{}" if col in ("metadata", "parameters") else "[]"
    if isinstance(value, str):
        try:
            parsed = json.loads(value)
            return json.dumps(parsed)  # re-serialize for asyncpg
        except (json.JSONDecodeError, TypeError):
            return value
    return json.dumps(value)


def _extract_assessment_id(filename: str) -> Optional[int]:
    """Extract assessment_id from filename like 'world-model-42.db'."""
    match = re.search(r"world-model-(\d+)\.db$", filename)
    if match:
        return int(match.group(1))
    # Fallback: if it's just 'world-model.db', assume assessment_id=1
    if filename.endswith("world-model.db"):
        return 1
    return None


def read_sqlite_table(db_path: str, table: str, columns: List[str]) -> List[Dict[str, Any]]:
    """Read all rows from a SQLite table, returning list of dicts."""
    conn = sqlite3.connect(db_path)
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    # Check if table exists
    cursor.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table,)
    )
    if not cursor.fetchone():
        conn.close()
        return []

    # Get actual columns in the SQLite table
    cursor.execute(f"PRAGMA table_info({table})")
    actual_cols = {row["name"] for row in cursor.fetchall()}

    # Filter to columns that exist in both SQLite and target schema
    read_cols = [c for c in columns if c in actual_cols]

    if not read_cols:
        conn.close()
        return []

    sql = f"SELECT {', '.join(read_cols)} FROM {table}"
    cursor.execute(sql)
    rows = []
    for row in cursor.fetchall():
        d = {}
        for col in read_cols:
            d[col] = row[col]
        rows.append(d)

    conn.close()
    return rows


async def insert_rows(
    pool: asyncpg.Pool,
    pg_table: str,
    columns: List[str],
    rows: List[Dict[str, Any]],
    assessment_id: int,
    json_cols: List[str],
) -> int:
    """Insert rows into PostgreSQL wm_* table. Returns count of inserted rows."""
    if not rows:
        return 0

    inserted = 0
    all_cols = ["assessment_id"] + columns
    placeholders = ", ".join(f"${i+1}" for i in range(len(all_cols)))
    col_list = ", ".join(all_cols)

    sql = f"INSERT INTO {pg_table} ({col_list}) VALUES ({placeholders}) ON CONFLICT DO NOTHING"

    async with pool.acquire() as conn:
        async with conn.transaction():
            for row in rows:
                values = [assessment_id]
                for col in columns:
                    val = row.get(col)
                    val = _parse_json_value(val, col, json_cols)
                    values.append(val)
                try:
                    await conn.execute(sql, *values)
                    inserted += 1
                except Exception as exc:
                    logger.warning(
                        "  Skipped row in %s (id=%s): %s",
                        pg_table, row.get("id", "?"), exc,
                    )

    return inserted


async def migrate_file(pool: asyncpg.Pool, db_path: str, assessment_id: int) -> Dict[str, int]:
    """Migrate a single SQLite world model file to PostgreSQL."""
    stats = {}

    for table, columns in _TABLE_COLUMNS.items():
        pg_table = f"wm_{table}"
        json_cols = _JSON_COLUMNS.get(table, [])

        rows = read_sqlite_table(db_path, table, columns)
        if not rows:
            stats[table] = 0
            continue

        count = await insert_rows(pool, pg_table, columns, rows, assessment_id, json_cols)
        stats[table] = count
        logger.info("  %s: %d/%d rows migrated", pg_table, count, len(rows))

    return stats


async def main(data_dir: str, db_url: str) -> None:
    """Main migration entry point."""
    # Find all world-model SQLite files
    pattern = os.path.join(data_dir, "world-model*.db")
    db_files = sorted(glob.glob(pattern))

    if not db_files:
        logger.info("No world-model*.db files found in %s", data_dir)
        return

    logger.info("Found %d SQLite database(s) to migrate", len(db_files))

    # Connect to PostgreSQL
    pool = await asyncpg.create_pool(db_url, min_size=1, max_size=5)

    total_stats: Dict[str, int] = {}
    migrated_files = 0

    try:
        for db_path in db_files:
            filename = os.path.basename(db_path)
            assessment_id = _extract_assessment_id(filename)

            if assessment_id is None:
                logger.warning("Cannot extract assessment_id from %s, skipping", filename)
                continue

            logger.info("Migrating %s (assessment_id=%d)", filename, assessment_id)
            stats = await migrate_file(pool, db_path, assessment_id)

            for table, count in stats.items():
                total_stats[table] = total_stats.get(table, 0) + count

            migrated_files += 1

    finally:
        await pool.close()

    # Print summary
    logger.info("=" * 60)
    logger.info("Migration complete: %d file(s) processed", migrated_files)
    logger.info("-" * 60)
    total_rows = 0
    for table, count in sorted(total_stats.items()):
        logger.info("  wm_%-20s %d rows", table, count)
        total_rows += count
    logger.info("-" * 60)
    logger.info("  Total: %d rows migrated", total_rows)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Migrate SQLite world model to PostgreSQL")
    parser.add_argument(
        "--data-dir",
        default="./data",
        help="Directory containing world-model-*.db files (default: ./data)",
    )
    parser.add_argument(
        "--db-url",
        default=None,
        help="PostgreSQL connection URL (default: from config.py settings)",
    )
    args = parser.parse_args()

    if args.db_url is None:
        try:
            from config import settings
            args.db_url = settings.DATABASE_URL.replace(
                "postgresql://", "postgresql://"
            )
        except ImportError:
            args.db_url = "postgresql://autopentest:autopentest@localhost:5432/autopentest_db"

    # asyncpg needs postgresql:// not postgresql+asyncpg://
    db_url = args.db_url.replace("postgresql+asyncpg://", "postgresql://")

    asyncio.run(main(args.data_dir, db_url))
