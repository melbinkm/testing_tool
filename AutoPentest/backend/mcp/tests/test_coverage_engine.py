"""
Tests for the Deterministic Coverage Engine.

Covers:
- World model DB: relationships table, coverage_matrix table, graph BFS/path
- Coverage tracker: endpoint classification, tool resolution, priority, matrix build
- Phase orchestrator: gate conditions, metrics, phase transitions
- Payload libraries: count verification, filtering
"""

from __future__ import annotations

import asyncio
import json
import os
import sys
import unittest
from unittest.mock import AsyncMock, MagicMock

# Add modules to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "modules"))

# Mock mcp.types before importing anything that depends on it
import types as pytypes
mock_mcp = pytypes.ModuleType("mcp")
mock_mcp_types = pytypes.ModuleType("mcp.types")

class MockTool:
    def __init__(self, name="", description="", inputSchema=None):
        self.name = name
        self.description = description
        self.inputSchema = inputSchema or {}

class MockTextContent:
    def __init__(self, type="text", text=""):
        self.type = type
        self.text = text

mock_mcp_types.Tool = MockTool
mock_mcp_types.TextContent = MockTextContent
sys.modules["mcp"] = mock_mcp
sys.modules["mcp.types"] = mock_mcp_types


def run_async(coro):
    """Helper to run async code in sync tests."""
    return asyncio.get_event_loop().run_until_complete(coro)


# ---------------------------------------------------------------------------
# In-memory mock pool for WorldModelDatabase (replaces SQLite test databases)
# ---------------------------------------------------------------------------

class MockRecord(dict):
    """Dict subclass that supports both dict['key'] and dict.key access, like asyncpg.Record."""
    def __getattr__(self, name):
        try:
            return self[name]
        except KeyError:
            raise AttributeError(name)


class MockConnection:
    """Simulates an asyncpg connection backed by in-memory tables."""

    def __init__(self, tables):
        self._tables = tables
        self._in_transaction = False

    async def execute(self, sql, *params):
        """Execute INSERT/UPDATE/DELETE statements against in-memory tables."""
        sql_lower = sql.strip().lower()

        if sql_lower.startswith("insert into"):
            self._handle_insert(sql, params)
        elif sql_lower.startswith("update"):
            self._handle_update(sql, params)
        elif sql_lower.startswith("delete"):
            self._handle_delete(sql, params)
        return "OK"

    async def fetchrow(self, sql, *params):
        rows = await self.fetch(sql, *params)
        return rows[0] if rows else None

    async def fetch(self, sql, *params):
        """Execute SELECT statements against in-memory tables."""
        return self._handle_select(sql, params)

    def transaction(self):
        return MockTransaction(self)

    def _get_table_name(self, sql):
        """Extract table name from SQL."""
        sql_lower = sql.lower()
        for keyword in ("from", "into", "update", "join"):
            if keyword in sql_lower:
                parts = sql_lower.split(keyword)
                if len(parts) > 1:
                    table_part = parts[1].strip().split()[0]
                    # Remove schema prefix or alias
                    table_name = table_part.strip().rstrip(',').split(' ')[0]
                    return table_name
        return None

    def _handle_insert(self, sql, params):
        """Handle INSERT INTO ... VALUES (...) statements."""
        sql_lower = sql.lower()

        # Extract table name
        table = None
        if "insert into" in sql_lower:
            after_into = sql[sql_lower.index("insert into") + len("insert into"):].strip()
            table = after_into.split()[0].strip()

        if not table or table not in self._tables:
            if table:
                self._tables[table] = []
            else:
                return

        # Extract column names from parentheses after table name
        cols_start = sql.index("(", sql_lower.index(table) + len(table))
        cols_end = sql.index(")", cols_start)
        columns = [c.strip() for c in sql[cols_start+1:cols_end].split(",")]

        # Build record from params
        record = MockRecord()
        for i, col in enumerate(columns):
            if i < len(params):
                val = params[i]
                # Parse JSON strings for JSONB columns
                if isinstance(val, str) and col in ('metadata', 'tags', 'parameters',
                    'permissions', 'evidence', 'evidence_ids', 'steps', 'tool_args'):
                    try:
                        val = json.loads(val)
                    except (json.JSONDecodeError, TypeError):
                        pass
                record[col] = val

        # Handle ON CONFLICT DO NOTHING
        if "on conflict" in sql_lower and "do nothing" in sql_lower:
            # Check for unique constraint violations
            existing = self._tables.get(table, [])
            if self._check_unique_conflict(table, record, existing):
                return  # Skip insert

        self._tables.setdefault(table, []).append(record)

    def _check_unique_conflict(self, table, record, existing):
        """Check if a record would violate a unique constraint."""
        if table == "wm_relationships":
            for r in existing:
                if (r.get("assessment_id") == record.get("assessment_id") and
                    r.get("source_type") == record.get("source_type") and
                    r.get("source_id") == record.get("source_id") and
                    r.get("target_type") == record.get("target_type") and
                    r.get("target_id") == record.get("target_id") and
                    r.get("rel_type") == record.get("rel_type")):
                    return True
        elif table == "wm_coverage_matrix":
            for r in existing:
                if (r.get("assessment_id") == record.get("assessment_id") and
                    r.get("endpoint_id") == record.get("endpoint_id") and
                    r.get("vuln_class") == record.get("vuln_class") and
                    r.get("parameter") == record.get("parameter")):
                    return True
        return False

    def _handle_update(self, sql, params):
        """Handle UPDATE ... SET ... WHERE ... statements."""
        sql_lower = sql.lower()

        # Extract table name
        table = sql[sql_lower.index("update") + 6:sql_lower.index("set")].strip()

        if table not in self._tables:
            return

        # Parse SET clause to get column=value assignments
        set_clause = sql[sql_lower.index("set") + 3:sql_lower.index("where")].strip()

        # Parse WHERE clause to find matching rows
        where_clause = sql[sql_lower.index("where") + 5:].strip()

        # Build param mapping ($1, $2, ...) -> values
        param_map = {}
        for i, p in enumerate(params):
            param_map[f"${i+1}"] = p

        # Parse SET assignments
        set_assignments = {}
        for part in set_clause.split(","):
            part = part.strip()
            if "=" in part:
                col, val_placeholder = part.split("=", 1)
                col = col.strip()
                val_placeholder = val_placeholder.strip()
                if val_placeholder in param_map:
                    val = param_map[val_placeholder]
                    # Parse JSON strings for JSONB columns
                    if isinstance(val, str) and col in ('metadata', 'evidence', 'steps', 'tool_args'):
                        try:
                            val = json.loads(val)
                        except (json.JSONDecodeError, TypeError):
                            pass
                    set_assignments[col] = val

        # Parse WHERE conditions
        where_conditions = {}
        for part in where_clause.split(" AND "):
            part = part.strip()
            if "=" in part:
                col, val_placeholder = part.split("=", 1)
                col = col.strip()
                val_placeholder = val_placeholder.strip()
                if val_placeholder in param_map:
                    where_conditions[col] = param_map[val_placeholder]

        # Apply updates
        for row in self._tables.get(table, []):
            match = all(row.get(k) == v for k, v in where_conditions.items())
            if match:
                row.update(set_assignments)

    def _handle_delete(self, sql, params):
        """Handle DELETE FROM ... WHERE ... statements."""
        sql_lower = sql.lower()

        # Extract table name
        table_start = sql_lower.index("from") + 4
        table_end = sql_lower.index("where") if "where" in sql_lower else len(sql)
        table = sql[table_start:table_end].strip()

        if table not in self._tables:
            return

        if "where" not in sql_lower:
            self._tables[table] = []
            return

        # Parse WHERE conditions
        where_clause = sql[sql_lower.index("where") + 5:].strip()
        param_map = {}
        for i, p in enumerate(params):
            param_map[f"${i+1}"] = p

        where_conditions = {}
        for part in where_clause.split(" AND "):
            part = part.strip()
            if "=" in part:
                col, val_placeholder = part.split("=", 1)
                col = col.strip()
                val_placeholder = val_placeholder.strip()
                if val_placeholder in param_map:
                    where_conditions[col] = param_map[val_placeholder]

        self._tables[table] = [
            r for r in self._tables[table]
            if not all(r.get(k) == v for k, v in where_conditions.items())
        ]

    def _handle_select(self, sql, params):
        """Handle SELECT statements against in-memory tables."""
        sql_lower = sql.lower()
        param_map = {}
        for i, p in enumerate(params):
            param_map[f"${i+1}"] = p

        # Parse table info from FROM clause
        tables_involved = []
        table_aliases = {}  # alias -> real table name
        if "from" not in sql_lower:
            return []

        from_part = sql_lower.split("from")[1]
        # Find where the FROM clause ends
        table_part = from_part
        for kw in ("where", "group by", "order by", "limit"):
            if kw in table_part:
                table_part = table_part[:table_part.index(kw)]

        # Handle JOINs
        if "join" in table_part:
            before_join = table_part.split("join")[0].strip()
            tokens = before_join.split()
            main_table = tokens[0].strip()
            main_alias = tokens[1].strip() if len(tokens) > 1 else main_table
            tables_involved.append(main_table)
            table_aliases[main_alias] = main_table

            join_parts = table_part.split("join")[1:]
            for jp in join_parts:
                tokens = jp.strip().split()
                jt = tokens[0].strip()
                ja = tokens[1].strip() if len(tokens) > 1 and tokens[1].strip() not in ("on",) else jt
                tables_involved.append(jt)
                table_aliases[ja] = jt
        else:
            tokens = table_part.strip().split()
            main_table = tokens[0].strip()
            main_alias = tokens[1].strip() if len(tokens) > 1 else main_table
            tables_involved.append(main_table)
            table_aliases[main_alias] = main_table

        if not tables_involved:
            return []

        main_table = tables_involved[0]
        rows = list(self._tables.get(main_table, []))

        # Handle COUNT(*) queries
        if "count(*)" in sql_lower:
            # For JOINs, do the join first before filtering
            if len(tables_involved) > 1:
                rows = self._handle_joins(sql, rows, tables_involved, param_map, table_aliases)
            rows = self._filter_rows(rows, sql, param_map, main_table)

            if "group by" in sql_lower:
                return self._handle_group_by(sql, rows, param_map)

            count = len(rows)
            result = MockRecord({"cnt": count})

            if "sum(" in sql_lower:
                total_size = sum(r.get("content_size", 0) for r in rows)
                result["total_size"] = total_size

            return [result]

        # Handle GROUP BY
        if "group by" in sql_lower:
            if len(tables_involved) > 1:
                rows = self._handle_joins(sql, rows, tables_involved, param_map, table_aliases)
            rows = self._filter_rows(rows, sql, param_map, main_table)
            return self._handle_group_by(sql, rows, param_map)

        # Handle HAVING
        if "having" in sql_lower:
            rows = self._filter_rows(rows, sql, param_map, main_table)
            return [MockRecord(r) for r in rows]

        # Handle JOINs first, then filter
        if len(tables_involved) > 1:
            rows = self._handle_joins(sql, rows, tables_involved, param_map, table_aliases)

        # Apply WHERE filters
        rows = self._filter_rows(rows, sql, param_map, main_table)

        # Handle ORDER BY
        if "order by" in sql_lower:
            order_part = sql_lower.split("order by")[1]
            if "limit" in order_part:
                order_part = order_part[:order_part.index("limit")]
            order_part = order_part.strip()

            desc = "desc" in order_part
            order_col = order_part.split()[0].strip().rstrip(",")
            if "." in order_col:
                order_col = order_col.split(".")[-1]

            rows.sort(key=lambda r: (r.get(order_col) is None, r.get(order_col, "")), reverse=desc)

        # Handle LIMIT
        if "limit" in sql_lower:
            limit_part = sql_lower.split("limit")[-1].strip()
            limit_val = limit_part.split()[0].strip()
            if limit_val.startswith("$"):
                limit_num = param_map.get(limit_val, 100)
            else:
                try:
                    limit_num = int(limit_val)
                except ValueError:
                    limit_num = 100
            rows = rows[:limit_num]

        # Handle OFFSET
        if "offset" in sql_lower:
            offset_part = sql_lower.split("offset")[-1].strip()
            offset_val = offset_part.split()[0].strip()
            if offset_val.startswith("$"):
                offset_num = param_map.get(offset_val, 0)
            else:
                try:
                    offset_num = int(offset_val)
                except ValueError:
                    offset_num = 0
            rows = rows[offset_num:]

        return [MockRecord(r) if not isinstance(r, MockRecord) else r for r in rows]

    def _filter_rows(self, rows, sql, param_map, table_alias=None):
        """Filter rows based on WHERE clause."""
        sql_lower = sql.lower()
        if "where" not in sql_lower:
            return rows

        where_start = sql_lower.index("where") + 5
        where_end = len(sql_lower)
        for keyword in ("group by", "order by", "limit", "having"):
            if keyword in sql_lower[where_start:]:
                idx = sql_lower.index(keyword, where_start)
                where_end = min(where_end, idx)

        where_clause = sql[where_start:where_end].strip()

        filtered = []
        for row in rows:
            if self._matches_where(row, where_clause, param_map):
                filtered.append(row)
        return filtered

    def _matches_where(self, row, where_clause, param_map):
        """Check if a row matches WHERE conditions.

        Handles nested parenthesized OR groups by parsing balanced parens
        before splitting on AND.
        """
        conditions = self._split_and_conditions(where_clause)
        for cond in conditions:
            cond = cond.strip()
            if not cond:
                continue

            # Check for OR group (parenthesized)
            if " OR " in cond.upper() or " or " in cond:
                # Strip only balanced outer parens
                inner = self._strip_balanced_parens(cond)
                or_parts = self._split_or_conditions(inner)
                if not any(self._eval_compound_condition(row, part.strip(), param_map) for part in or_parts):
                    return False
                continue

            if not self._eval_compound_condition(row, cond, param_map):
                return False
        return True

    def _strip_balanced_parens(self, s):
        """Strip outer parentheses only if they are balanced around the whole expression."""
        s = s.strip()
        while s.startswith("(") and s.endswith(")"):
            # Check if the opening paren matches the closing one
            depth = 0
            match_end = False
            for i, ch in enumerate(s):
                if ch == "(":
                    depth += 1
                elif ch == ")":
                    depth -= 1
                    if depth == 0:
                        if i == len(s) - 1:
                            match_end = True
                        break
            if match_end:
                s = s[1:-1].strip()
            else:
                break
        return s

    def _split_and_conditions(self, clause):
        """Split WHERE clause on top-level AND, respecting parentheses."""
        parts = []
        depth = 0
        current = []
        tokens = clause.split()
        i = 0
        while i < len(tokens):
            token = tokens[i]
            depth += token.count("(") - token.count(")")
            if token.upper() == "AND" and depth == 0:
                parts.append(" ".join(current))
                current = []
            else:
                current.append(token)
            i += 1
        if current:
            parts.append(" ".join(current))
        return parts

    def _split_or_conditions(self, clause):
        """Split on top-level OR, respecting parentheses."""
        parts = []
        depth = 0
        current = []
        tokens = clause.split()
        i = 0
        while i < len(tokens):
            token = tokens[i]
            depth += token.count("(") - token.count(")")
            if token.upper() == "OR" and depth == 0:
                parts.append(" ".join(current))
                current = []
            else:
                current.append(token)
            i += 1
        if current:
            parts.append(" ".join(current))
        return parts

    def _eval_compound_condition(self, row, cond, param_map):
        """Evaluate a condition that may contain AND subclauses (e.g., inside an OR group)."""
        cond = cond.strip()
        # Strip outer parens
        while cond.startswith("(") and cond.endswith(")"):
            cond = cond[1:-1].strip()
        # If it contains AND, split and require all
        if " AND " in cond or " and " in cond:
            sub_parts = self._split_and_conditions(cond)
            return all(self._eval_condition(row, p.strip(), param_map) for p in sub_parts)
        return self._eval_condition(row, cond, param_map)

    def _eval_condition(self, row, cond, param_map):
        """Evaluate a single condition against a row."""
        cond = cond.strip()
        # Strip leftover parens
        while cond.startswith("("):
            cond = cond[1:]
        while cond.endswith(")"):
            cond = cond[:-1]
        cond = cond.strip()

        # Handle IN clauses
        if " in (" in cond.lower():
            col = cond[:cond.lower().index(" in")].strip()
            if "." in col:
                col = col.split(".")[-1]
            in_values = []
            in_part = cond[cond.index("(") + 1:cond.index(")")]
            for val in in_part.split(","):
                val = val.strip()
                if val.startswith("$"):
                    in_values.append(param_map.get(val))
                elif val.startswith("'"):
                    in_values.append(val.strip("'"))
                else:
                    in_values.append(val)
            return row.get(col) in in_values

        # Handle >= and <=
        if ">=" in cond:
            col, val = cond.split(">=", 1)
            col = col.strip()
            if "." in col:
                col = col.split(".")[-1]
            val = val.strip()
            if val.startswith("$"):
                val = param_map.get(val)
            else:
                try:
                    val = int(val)
                except ValueError:
                    pass
            return row.get(col, 0) >= val

        # Handle = (equality)
        if "=" in cond and "!" not in cond:
            parts = cond.split("=", 1)
            col = parts[0].strip()
            if "." in col:
                col = col.split(".")[-1]
            val = parts[1].strip()

            if val.startswith("$"):
                val = param_map.get(val)
            elif val.startswith("'") and val.endswith("'"):
                val = val[1:-1]

            return row.get(col) == val

        return True

    def _handle_joins(self, sql, rows, tables, param_map, table_aliases=None):
        """Handle JOIN operations with proper alias tracking."""
        sql_lower = sql.lower()

        if " on " not in sql_lower:
            return rows

        join_table = tables[1] if len(tables) > 1 else None
        if not join_table:
            return rows

        join_rows = list(self._tables.get(join_table, []))

        # Parse ON condition
        on_idx = sql_lower.index(" on ") + 4
        on_clause = sql_lower[on_idx:]
        # Truncate at WHERE or next keyword
        for kw in (" where", " and ", " order", " group", " limit"):
            if kw in on_clause:
                on_clause = on_clause[:on_clause.index(kw)]
        on_clause = on_clause.strip()

        result_rows = []
        if "=" in on_clause:
            left, right = on_clause.split("=", 1)
            left_parts = left.strip().split(".")
            right_parts = right.strip().split(".")

            # Determine which column belongs to which table
            left_alias = left_parts[0].strip() if len(left_parts) > 1 else None
            left_col = left_parts[-1].strip()
            right_alias = right_parts[0].strip() if len(right_parts) > 1 else None
            right_col = right_parts[-1].strip()

            # Resolve which side is the main table and which is the join table
            main_table_name = tables[0]
            join_table_name = tables[1]

            # Build alias-to-table mapping
            aliases = table_aliases or {}
            left_is_join = aliases.get(left_alias) == join_table_name if left_alias else False
            right_is_join = aliases.get(right_alias) == join_table_name if right_alias else False

            for row in rows:
                for jrow in join_rows:
                    match = False
                    if left_is_join:
                        # left is join table col, right is main table col
                        match = jrow.get(left_col) == row.get(right_col)
                    elif right_is_join:
                        # right is join table col, left is main table col
                        match = row.get(left_col) == jrow.get(right_col)
                    else:
                        # Fallback: try both directions
                        match = (row.get(left_col) == jrow.get(right_col) or
                                row.get(right_col) == jrow.get(left_col))

                    if match:
                        merged = MockRecord({**jrow, **row})
                        # Preserve aliased columns (e.g., e.parameters AS ep_parameters)
                        if "as ep_parameters" in sql_lower:
                            merged["ep_parameters"] = jrow.get("parameters", {})
                        result_rows.append(merged)

        return result_rows

    def _handle_group_by(self, sql, rows, param_map):
        """Handle GROUP BY queries."""
        sql_lower = sql.lower()

        gb_idx = sql_lower.index("group by") + 8
        gb_end = len(sql_lower)
        for keyword in ("order by", "limit", "having"):
            if keyword in sql_lower[gb_idx:]:
                gb_end = min(gb_end, sql_lower.index(keyword, gb_idx))
        group_cols_str = sql[gb_idx:gb_end].strip()
        group_cols = [c.strip().split(".")[-1] for c in group_cols_str.split(",")]

        # Group rows
        groups = {}
        for row in rows:
            key = tuple(row.get(col) for col in group_cols)
            groups.setdefault(key, []).append(row)

        # Build result rows with aggregates
        results = []
        for key, group_rows in groups.items():
            result = MockRecord()
            for i, col in enumerate(group_cols):
                result[col] = key[i]
            result["cnt"] = len(group_rows)
            result["count"] = len(group_rows)

            # Handle SUM aggregates
            if "sum(" in sql_lower:
                for row in group_rows:
                    for k in row:
                        if isinstance(row[k], (int, float)):
                            result.setdefault(k, 0)

            # Handle string_agg
            if "string_agg" in sql_lower:
                for row in group_rows:
                    for k in ("vuln_class",):
                        if k in row:
                            if "vuln_classes" not in result:
                                result["vuln_classes"] = row[k]
                            else:
                                result["vuln_classes"] += "," + row[k]

            result["finding_count"] = len(group_rows)
            results.append(result)

        return results


class MockTransaction:
    def __init__(self, conn):
        self.conn = conn

    async def __aenter__(self):
        self.conn._in_transaction = True
        return self

    async def __aexit__(self, *args):
        self.conn._in_transaction = False


class MockPool:
    """In-memory mock of asyncpg pool for testing."""

    def __init__(self):
        self._tables = {}
        self._conn = MockConnection(self._tables)

    def acquire(self):
        return MockPoolAcquire(self._conn)


class MockPoolAcquire:
    def __init__(self, conn):
        self.conn = conn

    async def __aenter__(self):
        return self.conn

    async def __aexit__(self, *args):
        pass


def _make_test_db(assessment_id=1):
    """Create a WorldModelDatabase backed by in-memory mock pool."""
    from lib.world_model_db import WorldModelDatabase
    pool = MockPool()
    db = WorldModelDatabase(pool=pool, assessment_id=assessment_id)
    run_async(db.init())
    return db


# ---------------------------------------------------------------------------
# World Model Database Tests
# ---------------------------------------------------------------------------

class TestWorldModelDB(unittest.TestCase):
    """Test the relationships and coverage_matrix tables + methods."""

    # -- Relationships --

    def test_add_relationship(self):
        db = _make_test_db()
        rel = run_async(db.add_relationship(
            source_type="asset", source_id="a1",
            target_type="endpoint", target_id="e1",
            rel_type="has_endpoint",
        ))
        self.assertIn("id", rel)
        self.assertEqual(rel["source_type"], "asset")
        self.assertEqual(rel["target_type"], "endpoint")
        self.assertEqual(rel["rel_type"], "has_endpoint")
        run_async(db.close())

    def test_add_relationship_idempotent(self):
        db = _make_test_db()
        run_async(db.add_relationship("asset", "a1", "endpoint", "e1", "has_endpoint"))
        run_async(db.add_relationship("asset", "a1", "endpoint", "e1", "has_endpoint"))
        rels = run_async(db.query_relationships(
            source_type="asset", source_id="a1", rel_type="has_endpoint"
        ))
        self.assertEqual(len(rels), 1)
        run_async(db.close())

    def test_add_relationship_invalid_type(self):
        db = _make_test_db()
        with self.assertRaises(ValueError):
            run_async(db.add_relationship("invalid", "a1", "endpoint", "e1", "has_endpoint"))
        with self.assertRaises(ValueError):
            run_async(db.add_relationship("asset", "a1", "endpoint", "e1", "invalid_rel"))
        run_async(db.close())

    def test_query_relationships(self):
        db = _make_test_db()
        run_async(db.add_relationship("asset", "a1", "endpoint", "e1", "has_endpoint"))
        run_async(db.add_relationship("asset", "a1", "endpoint", "e2", "has_endpoint"))
        run_async(db.add_relationship("finding", "f1", "hypothesis", "h1", "confirms"))

        rels = run_async(db.query_relationships(source_type="asset", source_id="a1"))
        self.assertEqual(len(rels), 2)

        rels = run_async(db.query_relationships(rel_type="confirms"))
        self.assertEqual(len(rels), 1)
        run_async(db.close())

    # -- Graph BFS --

    def test_get_neighbors_single_hop(self):
        db = _make_test_db()
        run_async(db.add_relationship("asset", "a1", "endpoint", "e1", "has_endpoint"))
        run_async(db.add_relationship("asset", "a1", "endpoint", "e2", "has_endpoint"))

        result = run_async(db.get_neighbors("asset", "a1", depth=1))
        self.assertEqual(len(result["nodes"]), 2)
        self.assertEqual(len(result["edges"]), 2)
        run_async(db.close())

    def test_get_neighbors_multi_hop(self):
        db = _make_test_db()
        run_async(db.add_relationship("asset", "a1", "endpoint", "e1", "has_endpoint"))
        run_async(db.add_relationship("endpoint", "e1", "finding", "f1", "has_finding"))

        result = run_async(db.get_neighbors("asset", "a1", depth=2))
        self.assertEqual(len(result["nodes"]), 2)  # e1 and f1
        self.assertEqual(len(result["edges"]), 2)
        run_async(db.close())

    def test_get_neighbors_direction_outgoing(self):
        db = _make_test_db()
        run_async(db.add_relationship("asset", "a1", "endpoint", "e1", "has_endpoint"))
        run_async(db.add_relationship("finding", "f1", "asset", "a1", "targets"))

        result = run_async(db.get_neighbors("asset", "a1", direction="outgoing", depth=1))
        # Should only find e1 (outgoing from a1)
        self.assertEqual(len(result["nodes"]), 1)
        run_async(db.close())

    def test_get_neighbors_depth_capped(self):
        db = _make_test_db()
        result = run_async(db.get_neighbors("asset", "a1", depth=10))
        # Depth should be capped at 4
        self.assertEqual(result["depth"], 4)
        run_async(db.close())

    # -- Graph shortest path --

    def test_get_attack_path_direct(self):
        db = _make_test_db()
        run_async(db.add_relationship("asset", "a1", "endpoint", "e1", "has_endpoint"))

        path = run_async(db.get_attack_path("asset", "a1", "endpoint", "e1"))
        self.assertIsNotNone(path)
        self.assertEqual(len(path), 1)
        self.assertEqual(path[0]["rel_type"], "has_endpoint")
        run_async(db.close())

    def test_get_attack_path_two_hops(self):
        db = _make_test_db()
        run_async(db.add_relationship("asset", "a1", "endpoint", "e1", "has_endpoint"))
        run_async(db.add_relationship("endpoint", "e1", "finding", "f1", "has_finding"))

        path = run_async(db.get_attack_path("asset", "a1", "finding", "f1"))
        self.assertIsNotNone(path)
        self.assertEqual(len(path), 2)
        run_async(db.close())

    def test_get_attack_path_no_path(self):
        db = _make_test_db()
        run_async(db.add_relationship("asset", "a1", "endpoint", "e1", "has_endpoint"))

        path = run_async(db.get_attack_path("asset", "a1", "finding", "f_nonexistent"))
        self.assertIsNone(path)
        run_async(db.close())

    # -- Coverage matrix --

    def test_coverage_init_rows(self):
        db = _make_test_db()
        ep = run_async(db.add_asset(kind="domain", name="test.com"))
        endpoint = run_async(db.add_endpoint(
            asset_id=ep["id"], method="GET", path="/api/users",
            parameters={"query": {"id": "string"}},
        ))

        rows = [{
            "endpoint_id": endpoint["id"],
            "vuln_class": "sqli",
            "tool_name": "fuzz_parameter",
            "tool_args": {"url": "https://test.com/api/users"},
            "priority": 70,
        }, {
            "endpoint_id": endpoint["id"],
            "vuln_class": "xss",
            "tool_name": "fuzz_parameter",
            "tool_args": {"url": "https://test.com/api/users"},
            "priority": 60,
        }]

        result = run_async(db.coverage_init_rows(rows))
        self.assertEqual(result["created"], 2)
        self.assertEqual(result["skipped_existing"], 0)
        run_async(db.close())

    def test_coverage_init_rows_idempotent(self):
        db = _make_test_db()
        ep = run_async(db.add_asset(kind="domain", name="test.com"))
        endpoint = run_async(db.add_endpoint(
            asset_id=ep["id"], method="GET", path="/api/users"
        ))

        rows = [{"endpoint_id": endpoint["id"], "vuln_class": "sqli",
                 "tool_name": "fuzz_parameter", "tool_args": {}, "priority": 50}]

        run_async(db.coverage_init_rows(rows))
        result2 = run_async(db.coverage_init_rows(rows))
        self.assertEqual(result2["created"], 0)
        self.assertEqual(result2["skipped_existing"], 1)
        run_async(db.close())

    def test_coverage_next(self):
        db = _make_test_db()
        ep = run_async(db.add_asset(kind="domain", name="test.com"))
        endpoint = run_async(db.add_endpoint(
            asset_id=ep["id"], method="POST", path="/api/login",
            parameters={"body": {"username": "string", "password": "string"}},
        ))

        rows = [
            {"endpoint_id": endpoint["id"], "vuln_class": "sqli",
             "tool_name": "fuzz_parameter", "tool_args": {}, "priority": 80},
            {"endpoint_id": endpoint["id"], "vuln_class": "xss",
             "tool_name": "fuzz_parameter", "tool_args": {}, "priority": 60},
        ]
        run_async(db.coverage_init_rows(rows))

        cells = run_async(db.coverage_next(limit=5))
        self.assertEqual(len(cells), 2)
        # Higher priority first
        self.assertEqual(cells[0]["vuln_class"], "sqli")
        self.assertEqual(cells[0]["priority"], 80)
        # Should include endpoint info
        self.assertIn("endpoint", cells[0])
        self.assertEqual(cells[0]["endpoint"]["method"], "POST")
        run_async(db.close())

    def test_coverage_mark(self):
        db = _make_test_db()
        ep = run_async(db.add_asset(kind="domain", name="test.com"))
        endpoint = run_async(db.add_endpoint(
            asset_id=ep["id"], method="GET", path="/test"
        ))

        run_async(db.coverage_init_rows([{
            "endpoint_id": endpoint["id"], "vuln_class": "sqli",
            "tool_name": "fuzz_parameter", "tool_args": {}, "priority": 50,
        }]))

        cells = run_async(db.coverage_next(limit=1))
        cell_id = cells[0]["id"]

        updated = run_async(db.coverage_mark(
            cell_id=cell_id, status="vulnerable",
            finding_id="f123", result_summary="SQL injection found",
        ))
        self.assertEqual(updated["status"], "vulnerable")
        self.assertEqual(updated["finding_id"], "f123")
        self.assertIsNotNone(updated["completed_at"])
        run_async(db.close())

    def test_coverage_mark_invalid_status(self):
        db = _make_test_db()
        with self.assertRaises(ValueError):
            run_async(db.coverage_mark("nonexistent", "invalid_status"))
        run_async(db.close())

    def test_coverage_report(self):
        db = _make_test_db()
        ep = run_async(db.add_asset(kind="domain", name="test.com"))
        endpoint = run_async(db.add_endpoint(
            asset_id=ep["id"], method="GET", path="/test"
        ))

        rows = [
            {"endpoint_id": endpoint["id"], "vuln_class": "sqli",
             "tool_name": "fuzz_parameter", "tool_args": {}, "priority": 70},
            {"endpoint_id": endpoint["id"], "vuln_class": "xss",
             "tool_name": "fuzz_parameter", "tool_args": {}, "priority": 60},
        ]
        run_async(db.coverage_init_rows(rows))

        # Mark one cell
        cells = run_async(db.coverage_next(vuln_class="sqli", limit=1))
        run_async(db.coverage_mark(cells[0]["id"], "passed"))

        report = run_async(db.coverage_report())
        self.assertEqual(report["total_cells"], 2)
        self.assertEqual(report["by_status"]["passed"], 1)
        self.assertEqual(report["by_status"]["pending"], 1)
        self.assertEqual(report["coverage_pct"], 50.0)
        self.assertEqual(len(report["gaps"]), 1)
        run_async(db.close())

    def test_coverage_query_types(self):
        db = _make_test_db()
        ep = run_async(db.add_asset(kind="domain", name="test.com"))
        endpoint = run_async(db.add_endpoint(
            asset_id=ep["id"], method="GET", path="/test"
        ))
        run_async(db.coverage_init_rows([
            {"endpoint_id": endpoint["id"], "vuln_class": "sqli",
             "tool_name": "fuzz_parameter", "tool_args": {}, "priority": 80},
        ]))

        # Test each query type
        for qt in ["untested_endpoints", "coverage_by_vuln_class",
                    "findings_by_endpoint", "vulnerable_endpoints",
                    "high_priority_gaps"]:
            results = run_async(db.coverage_query(qt))
            self.assertIsInstance(results, list)

        # Test invalid query type
        with self.assertRaises(ValueError):
            run_async(db.coverage_query("invalid_type"))

        run_async(db.close())


# ---------------------------------------------------------------------------
# Coverage Tracker Tests
# ---------------------------------------------------------------------------

class TestCoverageTracker(unittest.TestCase):
    """Test the coverage tracker logic."""

    def test_classify_endpoint_with_params(self):
        from lib.coverage_tracker import CoverageTracker

        endpoint = {
            "id": "e1",
            "method": "POST",
            "path": "/api/users",
            "parameters": {"body": {"username": "string", "password": "string"}},
            "auth_required": True,
        }
        classes = CoverageTracker.classify_endpoint(endpoint)
        self.assertIn("sqli", classes)
        self.assertIn("xss", classes)
        self.assertIn("idor", classes)
        self.assertIn("auth_bypass", classes)
        self.assertIn("info_disclosure", classes)
        self.assertIn("nuclei", classes)

    def test_classify_endpoint_minimal(self):
        from lib.coverage_tracker import CoverageTracker

        endpoint = {
            "id": "e2",
            "method": "GET",
            "path": "/health",
            "parameters": {},
            "auth_required": False,
        }
        classes = CoverageTracker.classify_endpoint(endpoint)
        self.assertIn("info_disclosure", classes)
        self.assertIn("misconfig", classes)
        self.assertIn("nuclei", classes)
        self.assertNotIn("sqli", classes)
        self.assertNotIn("xss", classes)

    def test_classify_endpoint_path_params(self):
        from lib.coverage_tracker import CoverageTracker

        endpoint = {
            "id": "e3",
            "method": "GET",
            "path": "/api/users/{id}/profile",
            "parameters": {},
            "auth_required": False,
        }
        classes = CoverageTracker.classify_endpoint(endpoint)
        self.assertIn("idor", classes)
        self.assertIn("ssrf", classes)

    def test_resolve_tool_call_sqli(self):
        from lib.coverage_tracker import CoverageTracker

        endpoint = {
            "method": "POST",
            "path": "/api/login",
            "parameters": {"body": {"username": "string"}},
        }
        result = CoverageTracker.resolve_tool_call(endpoint, "sqli", "https://example.com")
        self.assertEqual(result["tool_name"], "fuzz_parameter")
        self.assertEqual(result["arguments"]["url"], "https://example.com/api/login")
        self.assertIn("injection", result["arguments"]["payload_types"])

    def test_resolve_tool_call_nuclei(self):
        from lib.coverage_tracker import CoverageTracker

        endpoint = {"method": "GET", "path": "/api/v1/info", "parameters": {}}
        result = CoverageTracker.resolve_tool_call(endpoint, "nuclei", "https://example.com")
        self.assertEqual(result["tool_name"], "nuclei_scan_template")
        self.assertEqual(result["arguments"]["targets"], ["https://example.com/api/v1/info"])

    def test_resolve_tool_call_auth_diff(self):
        from lib.coverage_tracker import CoverageTracker

        endpoint = {"method": "GET", "path": "/api/admin", "parameters": {}}
        result = CoverageTracker.resolve_tool_call(endpoint, "idor", "https://example.com")
        self.assertEqual(result["tool_name"], "auth_diff_test")
        self.assertEqual(result["arguments"]["url"], "https://example.com/api/admin")

    def test_compute_priority_base(self):
        from lib.coverage_tracker import CoverageTracker

        endpoint = {"method": "GET", "path": "/test", "parameters": {},
                     "auth_required": False}
        priority = CoverageTracker.compute_priority(endpoint, "info_disclosure")
        self.assertEqual(priority, 40)

    def test_compute_priority_high(self):
        from lib.coverage_tracker import CoverageTracker

        endpoint = {
            "method": "POST", "path": "/api/login",
            "parameters": {"body": {"user": "s", "pass": "s", "token": "s"}},
            "auth_required": True,
        }
        priority = CoverageTracker.compute_priority(endpoint, "sqli")
        self.assertEqual(priority, 100)

    def test_build_matrix(self):
        from lib.coverage_tracker import CoverageTracker

        endpoints = [
            {
                "id": "e1",
                "method": "POST",
                "path": "/api/login",
                "parameters": {"body": {"username": "string"}},
                "auth_required": False,
            },
            {
                "id": "e2",
                "method": "GET",
                "path": "/health",
                "parameters": {},
                "auth_required": False,
            },
        ]
        rows = CoverageTracker.build_matrix(endpoints, "https://example.com")
        self.assertGreater(len(rows), 0)
        for row in rows:
            self.assertIn("endpoint_id", row)
            self.assertIn("vuln_class", row)
            self.assertIn("tool_name", row)
            self.assertIn("tool_args", row)
            self.assertIn("priority", row)

    def test_build_matrix_filtered(self):
        from lib.coverage_tracker import CoverageTracker

        endpoints = [{
            "id": "e1", "method": "POST", "path": "/api/login",
            "parameters": {"body": {"username": "string"}},
            "auth_required": False,
        }]
        rows = CoverageTracker.build_matrix(endpoints, "https://example.com",
                                             vuln_classes=["sqli", "xss"])
        vuln_classes = {r["vuln_class"] for r in rows}
        self.assertTrue(vuln_classes.issubset({"sqli", "xss"}))

    def test_build_matrix_deterministic(self):
        from lib.coverage_tracker import CoverageTracker

        endpoints = [{
            "id": "e1", "method": "GET", "path": "/api/test",
            "parameters": {"query": {"q": "string"}},
            "auth_required": False,
        }]
        rows1 = CoverageTracker.build_matrix(endpoints, "https://example.com")
        rows2 = CoverageTracker.build_matrix(endpoints, "https://example.com")

        self.assertEqual(len(rows1), len(rows2))
        for r1, r2 in zip(rows1, rows2):
            self.assertEqual(r1["endpoint_id"], r2["endpoint_id"])
            self.assertEqual(r1["vuln_class"], r2["vuln_class"])
            self.assertEqual(r1["tool_name"], r2["tool_name"])
            self.assertEqual(r1["priority"], r2["priority"])


# ---------------------------------------------------------------------------
# Phase Orchestrator Tests
# ---------------------------------------------------------------------------

class TestPhaseOrchestrator(unittest.TestCase):
    """Test the phase orchestrator gate conditions and transitions."""

    def test_check_gates_phase1(self):
        from lib.phase_orchestrator import PhaseOrchestrator
        db = _make_test_db()
        orch = PhaseOrchestrator(db)
        result = orch.check_gates(1, {})
        self.assertTrue(result["met"])
        run_async(db.close())

    def test_check_gates_phase2_not_met(self):
        from lib.phase_orchestrator import PhaseOrchestrator
        db = _make_test_db()
        orch = PhaseOrchestrator(db)
        metrics = {"assets": 1, "endpoints": 2}
        result = orch.check_gates(2, metrics)
        self.assertFalse(result["met"])
        unmet = [c for c in result["conditions"] if not c["met"]]
        self.assertGreater(len(unmet), 0)
        run_async(db.close())

    def test_check_gates_phase2_met(self):
        from lib.phase_orchestrator import PhaseOrchestrator
        db = _make_test_db()
        orch = PhaseOrchestrator(db)
        metrics = {"assets": 5, "endpoints": 10}
        result = orch.check_gates(2, metrics)
        self.assertTrue(result["met"])
        run_async(db.close())

    def test_check_gates_phase4(self):
        from lib.phase_orchestrator import PhaseOrchestrator
        db = _make_test_db()
        orch = PhaseOrchestrator(db)
        metrics = {"confirmed_hypotheses": 2, "findings": 3}
        result = orch.check_gates(4, metrics)
        self.assertTrue(result["met"])
        run_async(db.close())

    def test_get_metrics(self):
        from lib.phase_orchestrator import PhaseOrchestrator
        db = _make_test_db()
        # Add some data
        run_async(db.add_asset(kind="domain", name="test.com"))
        run_async(db.add_asset(kind="subdomain", name="api.test.com"))
        asset = run_async(db.add_asset(kind="domain", name="example.com"))
        run_async(db.add_endpoint(asset_id=asset["id"], method="GET", path="/test"))

        orch = PhaseOrchestrator(db)
        metrics = run_async(orch.get_metrics())
        self.assertEqual(metrics["assets"], 3)
        self.assertEqual(metrics["endpoints"], 1)
        run_async(db.close())

    def test_get_status(self):
        from lib.phase_orchestrator import PhaseOrchestrator
        db = _make_test_db()
        orch = PhaseOrchestrator(db)

        status = run_async(orch.get_status())
        self.assertEqual(status["current_phase"], 1)
        self.assertEqual(status["phase_name"], "Reconnaissance")
        self.assertIn("metrics", status)
        self.assertIn("phases", status)
        self.assertEqual(len(status["phases"]), 5)
        run_async(db.close())

    def test_advance_sequential(self):
        from lib.phase_orchestrator import PhaseOrchestrator
        db = _make_test_db()
        # Add enough data to pass phase 2 gates
        for i in range(5):
            asset = run_async(db.add_asset(kind="domain", name=f"test{i}.com"))
            for j in range(3):
                run_async(db.add_endpoint(
                    asset_id=asset["id"], method="GET", path=f"/api/v{j}"
                ))

        orch = PhaseOrchestrator(db)
        result = run_async(orch.advance(target_phase=2))
        self.assertTrue(result["success"])
        self.assertEqual(result["current_phase"], 2)
        run_async(db.close())

    def test_advance_skip_not_allowed(self):
        from lib.phase_orchestrator import PhaseOrchestrator
        db = _make_test_db()
        orch = PhaseOrchestrator(db)
        result = run_async(orch.advance(target_phase=3))
        self.assertFalse(result["success"])
        run_async(db.close())

    def test_advance_force(self):
        from lib.phase_orchestrator import PhaseOrchestrator
        db = _make_test_db()
        orch = PhaseOrchestrator(db)
        result = run_async(orch.advance(target_phase=2, force=True))
        self.assertTrue(result["success"])
        self.assertTrue(result.get("forced", False))
        run_async(db.close())

    def test_advance_invalid_phase(self):
        from lib.phase_orchestrator import PhaseOrchestrator
        db = _make_test_db()
        orch = PhaseOrchestrator(db)
        result = run_async(orch.advance(target_phase=6))
        self.assertFalse(result["success"])
        run_async(db.close())


# ---------------------------------------------------------------------------
# Payload Library Tests
# ---------------------------------------------------------------------------

class TestPayloadLibraries(unittest.TestCase):
    """Test payload library counts and filtering."""

    def test_xss_payloads_count(self):
        try:
            from lib.xss_payloads import get_all_payloads_flat, get_xss_payloads
            payloads = get_all_payloads_flat()
            self.assertGreaterEqual(len(payloads), 20)

            reflected = get_xss_payloads(technique="reflected")
            self.assertGreater(len(reflected), 0)
            for p in reflected:
                self.assertEqual(p.technique, "reflected")
        except ImportError:
            self.skipTest("xss_payloads not yet created")

    def test_ssrf_payloads_count(self):
        try:
            from lib.ssrf_payloads import get_all_payloads_flat, get_ssrf_payloads
            payloads = get_all_payloads_flat()
            self.assertGreaterEqual(len(payloads), 15)

            internal = get_ssrf_payloads(technique="internal_ip")
            self.assertGreater(len(internal), 0)
        except ImportError:
            self.skipTest("ssrf_payloads not yet created")

    def test_path_traversal_payloads_count(self):
        try:
            from lib.path_traversal_payloads import get_all_payloads_flat, get_path_traversal_payloads
            payloads = get_all_payloads_flat()
            self.assertGreaterEqual(len(payloads), 15)

            basic = get_path_traversal_payloads(technique="basic")
            self.assertGreater(len(basic), 0)
        except ImportError:
            self.skipTest("path_traversal_payloads not yet created")

    def test_ssti_payloads_count(self):
        try:
            from lib.ssti_payloads import get_all_payloads_flat, get_ssti_payloads
            payloads = get_all_payloads_flat()
            self.assertGreaterEqual(len(payloads), 15)

            jinja2 = get_ssti_payloads(engine="jinja2")
            self.assertGreater(len(jinja2), 0)
        except ImportError:
            self.skipTest("ssti_payloads not yet created")

    def test_sql_payloads_existing(self):
        """Verify existing SQL payloads still work."""
        from lib.sql_payloads import get_all_payloads_flat, get_sql_payloads
        payloads = get_all_payloads_flat()
        self.assertGreaterEqual(len(payloads), 80)

        mysql = get_sql_payloads(db_type="mysql")
        self.assertGreater(len(mysql), 0)


# ---------------------------------------------------------------------------
# Constants Validation Tests
# ---------------------------------------------------------------------------

class TestConstants(unittest.TestCase):
    """Validate that constants are consistent across modules."""

    def test_coverage_statuses(self):
        from lib.world_model_db import COVERAGE_STATUSES
        expected = ("pending", "in_progress", "passed", "vulnerable", "skipped", "error")
        self.assertEqual(COVERAGE_STATUSES, expected)

    def test_vuln_classes(self):
        from lib.world_model_db import VULN_CLASSES
        self.assertEqual(len(VULN_CLASSES), 13)
        self.assertIn("sqli", VULN_CLASSES)
        self.assertIn("nuclei", VULN_CLASSES)
        self.assertIn("ssti", VULN_CLASSES)

    def test_valid_entity_types(self):
        from lib.world_model_db import VALID_ENTITY_TYPES
        self.assertIn("asset", VALID_ENTITY_TYPES)
        self.assertIn("finding", VALID_ENTITY_TYPES)

    def test_valid_rel_types(self):
        from lib.world_model_db import VALID_REL_TYPES
        self.assertIn("has_endpoint", VALID_REL_TYPES)
        self.assertIn("has_finding", VALID_REL_TYPES)
        self.assertIn("confirms", VALID_REL_TYPES)

    def test_json_columns_include_new_tables(self):
        from lib.world_model_db import _JSON_COLUMNS
        self.assertIn("relationships", _JSON_COLUMNS)
        self.assertIn("coverage_matrix", _JSON_COLUMNS)
        self.assertEqual(_JSON_COLUMNS["relationships"], ["metadata"])
        self.assertEqual(_JSON_COLUMNS["coverage_matrix"], ["tool_args"])


# ---------------------------------------------------------------------------
# Integration: Full Coverage Flow
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# Discovery Monitor Tests
# ---------------------------------------------------------------------------

class TestDiscoveryMonitor(unittest.TestCase):
    """Test the DiscoveryMonitor feedback loop."""

    def test_classify_endpoint_priority_high_admin(self):
        from lib.discovery_monitor import DiscoveryMonitor
        p = DiscoveryMonitor._classify_endpoint_priority("GET", "/admin/dashboard")
        self.assertEqual(p, "high")

    def test_classify_endpoint_priority_high_auth(self):
        from lib.discovery_monitor import DiscoveryMonitor
        p = DiscoveryMonitor._classify_endpoint_priority("POST", "/api/login")
        self.assertEqual(p, "high")

    def test_classify_endpoint_priority_high_token(self):
        from lib.discovery_monitor import DiscoveryMonitor
        p = DiscoveryMonitor._classify_endpoint_priority("POST", "/oauth/token")
        self.assertEqual(p, "high")

    def test_classify_endpoint_priority_medium_api(self):
        from lib.discovery_monitor import DiscoveryMonitor
        p = DiscoveryMonitor._classify_endpoint_priority("GET", "/api/v1/users")
        self.assertEqual(p, "medium")

    def test_classify_endpoint_priority_medium_state_changing(self):
        from lib.discovery_monitor import DiscoveryMonitor
        p = DiscoveryMonitor._classify_endpoint_priority("DELETE", "/resource")
        self.assertEqual(p, "medium")

    def test_classify_endpoint_priority_medium_path_param(self):
        from lib.discovery_monitor import DiscoveryMonitor
        p = DiscoveryMonitor._classify_endpoint_priority("GET", "/users/{id}/profile")
        self.assertEqual(p, "medium")

    def test_classify_endpoint_priority_low_static(self):
        from lib.discovery_monitor import DiscoveryMonitor
        p = DiscoveryMonitor._classify_endpoint_priority("GET", "/static/style.css")
        self.assertEqual(p, "low")

    def test_classify_endpoint_priority_low_extension(self):
        from lib.discovery_monitor import DiscoveryMonitor
        p = DiscoveryMonitor._classify_endpoint_priority("GET", "/images/logo.png")
        self.assertEqual(p, "low")

    def test_classify_endpoint_priority_low_default(self):
        from lib.discovery_monitor import DiscoveryMonitor
        p = DiscoveryMonitor._classify_endpoint_priority("GET", "/about")
        self.assertEqual(p, "low")

    def test_classify_high_api_with_params(self):
        from lib.discovery_monitor import DiscoveryMonitor
        p = DiscoveryMonitor._classify_endpoint_priority(
            "GET", "/api/search", {"parameters": {"q": "string"}}
        )
        self.assertEqual(p, "high")

    def test_parse_datetime_iso_utc(self):
        from lib.discovery_monitor import DiscoveryMonitor
        dt = DiscoveryMonitor._parse_datetime("2025-01-15T10:30:00Z")
        self.assertIsNotNone(dt)
        self.assertEqual(dt.hour, 10)
        self.assertEqual(dt.minute, 30)

    def test_parse_datetime_iso_offset(self):
        from lib.discovery_monitor import DiscoveryMonitor
        dt = DiscoveryMonitor._parse_datetime("2025-01-15T10:30:00+00:00")
        self.assertIsNotNone(dt)

    def test_parse_datetime_space_separated(self):
        from lib.discovery_monitor import DiscoveryMonitor
        dt = DiscoveryMonitor._parse_datetime("2025-01-15 10:30:00")
        self.assertIsNotNone(dt)

    def test_parse_datetime_date_only(self):
        from lib.discovery_monitor import DiscoveryMonitor
        dt = DiscoveryMonitor._parse_datetime("2025-01-15")
        self.assertIsNotNone(dt)

    def test_parse_datetime_invalid(self):
        from lib.discovery_monitor import DiscoveryMonitor
        self.assertIsNone(DiscoveryMonitor._parse_datetime("not a date"))
        self.assertIsNone(DiscoveryMonitor._parse_datetime(""))
        self.assertIsNone(DiscoveryMonitor._parse_datetime(None))

    def test_generate_recommendations_no_events(self):
        from lib.discovery_monitor import DiscoveryMonitor
        recs = DiscoveryMonitor._generate_recommendations([], None)
        self.assertEqual(len(recs), 1)
        self.assertIn("No new endpoints", recs[0])

    def test_generate_recommendations_admin_endpoints(self):
        from lib.discovery_monitor import DiscoveryEvent, DiscoveryMonitor
        events = [
            DiscoveryEvent(
                endpoint_id="e1", method="GET", path="/admin/panel",
                source="crawler", phase="mapping",
                created_at="2025-01-15T10:30:00Z", priority="high",
            ),
        ]
        recs = DiscoveryMonitor._generate_recommendations(events, None)
        admin_recs = [r for r in recs if "admin" in r.lower()]
        self.assertGreater(len(admin_recs), 0)

    def test_generate_recommendations_auth_endpoints(self):
        from lib.discovery_monitor import DiscoveryEvent, DiscoveryMonitor
        events = [
            DiscoveryEvent(
                endpoint_id="e2", method="POST", path="/api/auth/login",
                source="crawler", phase="mapping",
                created_at="2025-01-15T10:30:00Z", priority="high",
            ),
        ]
        recs = DiscoveryMonitor._generate_recommendations(events, None)
        auth_recs = [r for r in recs if "auth" in r.lower()]
        self.assertGreater(len(auth_recs), 0)

    def test_generate_recommendations_coverage_gaps(self):
        from lib.discovery_monitor import DiscoveryEvent, DiscoveryMonitor
        events = [
            DiscoveryEvent(
                endpoint_id="e3", method="GET", path="/api/data",
                source="crawler", phase="mapping",
                created_at="2025-01-15T10:30:00Z", priority="medium",
            ),
        ]
        coverage = {
            "total_cells": 20,
            "by_status": {"pending": 15, "passed": 5},
            "coverage_pct": 25.0,
            "gaps": [{"priority": 80, "vuln_class": "sqli"}],
        }
        recs = DiscoveryMonitor._generate_recommendations(events, coverage)
        gap_recs = [r for r in recs if "gap" in r.lower() or "pending" in r.lower()]
        self.assertGreater(len(gap_recs), 0)

    def test_check_new_endpoints(self):
        """Test check_new_endpoints queries the DB correctly."""
        from lib.discovery_monitor import DiscoveryMonitor
        from datetime import datetime, timezone

        db = _make_test_db()
        monitor = DiscoveryMonitor(db)

        # Add some endpoints
        asset = run_async(db.add_asset(kind="domain", name="test.com"))
        run_async(db.add_endpoint(
            asset_id=asset["id"], method="GET", path="/admin/panel",
            metadata={"discovered_by": "crawler", "phase": "mapping"},
        ))
        run_async(db.add_endpoint(
            asset_id=asset["id"], method="POST", path="/api/login",
            metadata={"discovered_by": "crawler", "phase": "mapping"},
        ))

        # Query with a past timestamp
        events = run_async(monitor.check_new_endpoints("2020-01-01T00:00:00Z"))
        self.assertEqual(len(events), 2)

        # Verify priority classification
        priorities = {e.path: e.priority for e in events}
        self.assertEqual(priorities.get("/admin/panel"), "high")
        self.assertEqual(priorities.get("/api/login"), "high")

        run_async(db.close())

    def test_extend_coverage_empty_events(self):
        from lib.discovery_monitor import DiscoveryMonitor
        db = _make_test_db()
        monitor = DiscoveryMonitor(db)

        result = run_async(monitor.extend_coverage([], "https://test.com"))
        self.assertEqual(result["new_cells_created"], 0)
        self.assertEqual(result["endpoints_covered"], 0)
        run_async(db.close())


# ---------------------------------------------------------------------------
# Integration: Full Coverage Flow
# ---------------------------------------------------------------------------

class TestCoverageIntegration(unittest.TestCase):
    """End-to-end test: endpoints -> build matrix -> next -> mark -> report."""

    def test_full_coverage_flow(self):
        from lib.coverage_tracker import CoverageTracker

        db = _make_test_db()

        # 1. Create assets and endpoints
        asset = run_async(db.add_asset(kind="domain", name="test.com"))
        ep1 = run_async(db.add_endpoint(
            asset_id=asset["id"], method="POST", path="/api/login",
            parameters={"body": {"username": "string", "password": "string"}},
            auth_required=True,
        ))
        ep2 = run_async(db.add_endpoint(
            asset_id=asset["id"], method="GET", path="/api/health",
        ))

        # 2. Build coverage matrix
        endpoints = run_async(db.query(table="endpoints"))
        rows = CoverageTracker.build_matrix(endpoints, "https://test.com")
        self.assertGreater(len(rows), 0)

        # 3. Init rows
        result = run_async(db.coverage_init_rows(rows))
        total_created = result["created"]
        self.assertGreater(total_created, 0)

        # 4. Get next cells
        cells = run_async(db.coverage_next(limit=50))
        self.assertEqual(len(cells), total_created)

        # 5. Mark some cells
        marked_count = 0
        for cell in cells[:3]:
            run_async(db.coverage_mark(cell["id"], "passed"))
            marked_count += 1

        # 6. Mark one as vulnerable
        if len(cells) > 3:
            run_async(db.coverage_mark(
                cells[3]["id"], "vulnerable",
                finding_id="f-test", result_summary="Found vuln",
            ))
            marked_count += 1

        # 7. Check report
        report = run_async(db.coverage_report())
        self.assertEqual(report["total_cells"], total_created)
        self.assertGreater(report["coverage_pct"], 0)

        # 8. Verify determinism
        rows2 = CoverageTracker.build_matrix(endpoints, "https://test.com")
        self.assertEqual(len(rows), len(rows2))

        run_async(db.close())


if __name__ == "__main__":
    unittest.main()
