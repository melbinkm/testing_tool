"""
Tests for the WebCrawler from modules/lib/web_crawler.py.

Covers:
- URL normalization (relative resolution, fragment stripping, scheme/host lowercase)
- URL skip/exclude patterns (glob and regex)
- JS endpoint pattern extraction from JavaScript source
- Endpoint data extraction from URLs
- CrawlConfig defaults and validation
- Scope filtering integration
- BFS vs DFS queue strategy
- Query parameter extraction
"""

from __future__ import annotations

import asyncio
import collections
import os
import re
import sys
import unittest
from unittest.mock import AsyncMock, MagicMock, patch

# Add modules to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "modules"))

# Mock mcp.types before importing anything that depends on it
import types as pytypes
mock_mcp = pytypes.ModuleType("mcp")
mock_mcp_types = pytypes.ModuleType("mcp.types")


class MockTool:
    def __init__(self, name="", description="", inputSchema=None):
        self.name = name
        self.description = description
        self.inputSchema = inputSchema or {}


class MockTextContent:
    def __init__(self, type="text", text=""):
        self.type = type
        self.text = text


mock_mcp_types.Tool = MockTool
mock_mcp_types.TextContent = MockTextContent
sys.modules["mcp"] = mock_mcp
sys.modules["mcp.types"] = mock_mcp_types


def run_async(coro):
    """Helper to run async code in sync tests."""
    return asyncio.get_event_loop().run_until_complete(coro)


# ---------------------------------------------------------------------------
# Imports under test (after mcp mock is in place)
# ---------------------------------------------------------------------------

from lib.web_crawler import (
    CrawlConfig,
    CrawlResult,
    CrawlState,
    JS_ENDPOINT_PATTERNS,
    WebCrawler,
)


def _make_crawler(**overrides):
    """Create a WebCrawler with mock dependencies."""
    defaults = {
        "session_manager": MagicMock(),
        "scope_validator": MagicMock(),
        "db": MagicMock(),
        "mcp_service": MagicMock(),
    }
    defaults.update(overrides)
    return WebCrawler(**defaults)


# =========================================================================
# Test classes
# =========================================================================


class TestUrlNormalization(unittest.TestCase):
    """Test WebCrawler._normalize_url(url, base_url)."""

    def test_relative_url_resolution(self):
        result = WebCrawler._normalize_url("/about", "https://example.com/page")
        self.assertEqual(result, "https://example.com/about")

    def test_fragment_stripping(self):
        result = WebCrawler._normalize_url(
            "https://example.com/page#section", "https://example.com"
        )
        self.assertEqual(result, "https://example.com/page")

    def test_scheme_host_lowercase(self):
        result = WebCrawler._normalize_url(
            "HTTPS://EXAMPLE.COM/Path", "https://x.com"
        )
        self.assertEqual(result, "https://example.com/Path")

    def test_invalid_javascript_url_returns_none(self):
        result = WebCrawler._normalize_url(
            "javascript:void(0)", "https://example.com"
        )
        self.assertIsNone(result)

    def test_mailto_returns_none(self):
        result = WebCrawler._normalize_url(
            "mailto:test@test.com", "https://example.com"
        )
        self.assertIsNone(result)

    def test_empty_path_normalized_to_slash(self):
        result = WebCrawler._normalize_url(
            "https://example.com", "https://example.com"
        )
        self.assertIsNotNone(result)
        from urllib.parse import urlparse
        self.assertEqual(urlparse(result).path, "/")

    def test_relative_deep_path(self):
        result = WebCrawler._normalize_url(
            "sub/page.html", "https://example.com/dir/"
        )
        self.assertEqual(result, "https://example.com/dir/sub/page.html")

    def test_query_string_preserved(self):
        result = WebCrawler._normalize_url(
            "/search?q=test", "https://example.com"
        )
        self.assertEqual(result, "https://example.com/search?q=test")

    def test_data_uri_returns_none(self):
        result = WebCrawler._normalize_url(
            "data:text/html,<h1>hi</h1>", "https://example.com"
        )
        self.assertIsNone(result)

    def test_ftp_scheme_returns_none(self):
        result = WebCrawler._normalize_url(
            "ftp://files.example.com/pub", "https://example.com"
        )
        self.assertIsNone(result)


class TestUrlSkipping(unittest.TestCase):
    """Test WebCrawler._should_skip_url(url, patterns)."""

    def test_glob_match_returns_true(self):
        self.assertTrue(
            WebCrawler._should_skip_url(
                "https://example.com/static/file.css", ["*/static/*"]
            )
        )

    def test_no_match_returns_false(self):
        self.assertFalse(
            WebCrawler._should_skip_url(
                "https://example.com/api/users", ["*/static/*"]
            )
        )

    def test_regex_match_returns_true(self):
        self.assertTrue(
            WebCrawler._should_skip_url(
                "https://example.com/logout", ["^.*logout.*$"]
            )
        )

    def test_empty_patterns_returns_false(self):
        self.assertFalse(
            WebCrawler._should_skip_url("https://example.com/anything", [])
        )

    def test_multiple_patterns_first_matches(self):
        self.assertTrue(
            WebCrawler._should_skip_url(
                "https://example.com/static/app.js",
                ["*/static/*", "*/admin/*"],
            )
        )

    def test_multiple_patterns_second_matches(self):
        self.assertTrue(
            WebCrawler._should_skip_url(
                "https://example.com/admin/panel",
                ["*/static/*", "*/admin/*"],
            )
        )

    def test_regex_with_parentheses(self):
        self.assertTrue(
            WebCrawler._should_skip_url(
                "https://example.com/session/destroy",
                ["(logout|destroy|signout)"],
            )
        )

    def test_no_patterns_match_returns_false(self):
        self.assertFalse(
            WebCrawler._should_skip_url(
                "https://example.com/api/data",
                ["*/admin/*", "^.*logout.*$"],
            )
        )


class TestJsEndpointExtraction(unittest.TestCase):
    """Test that JS_ENDPOINT_PATTERNS regex constants correctly extract paths."""

    def _find_all_matches(self, js_code):
        """Apply all JS_ENDPOINT_PATTERNS and return matched paths."""
        found = set()
        for pat in JS_ENDPOINT_PATTERNS:
            for m in pat.finditer(js_code):
                found.add(m.group(1))
        return found

    def test_fetch_extraction(self):
        js = """fetch('/api/users')"""
        matches = self._find_all_matches(js)
        self.assertIn("/api/users", matches)

    def test_axios_get_extraction(self):
        js = """axios.get('/api/v1/items')"""
        matches = self._find_all_matches(js)
        self.assertIn("/api/v1/items", matches)

    def test_jquery_ajax_extraction(self):
        js = """$.ajax({url: '/api/data'})"""
        matches = self._find_all_matches(js)
        self.assertIn("/api/data", matches)

    def test_quoted_api_path_extraction(self):
        js = """var endpoint = '/api/auth/token';"""
        matches = self._find_all_matches(js)
        self.assertIn("/api/auth/token", matches)

    def test_fetch_with_double_quotes(self):
        js = '''fetch("/api/items/list")'''
        matches = self._find_all_matches(js)
        self.assertIn("/api/items/list", matches)

    def test_axios_post_extraction(self):
        js = """axios.post('/api/v2/orders')"""
        matches = self._find_all_matches(js)
        self.assertIn("/api/v2/orders", matches)

    def test_rest_path_in_string_literal(self):
        js = """const url = '/rest/config/settings';"""
        matches = self._find_all_matches(js)
        self.assertIn("/rest/config/settings", matches)

    def test_no_match_for_non_api_path(self):
        js = """var x = '/images/logo.png';"""
        matches = self._find_all_matches(js)
        # /images/logo.png does not match api/v\d/rest/graphql patterns
        self.assertNotIn("/images/logo.png", matches)

    def test_multiple_endpoints_in_same_code(self):
        js = """
        fetch('/api/users');
        axios.get('/api/v1/items');
        const ep = '/api/auth/token';
        """
        matches = self._find_all_matches(js)
        self.assertIn("/api/users", matches)
        self.assertIn("/api/v1/items", matches)
        self.assertIn("/api/auth/token", matches)


class TestEndpointDataExtraction(unittest.TestCase):
    """Test WebCrawler._url_to_endpoint_data(url)."""

    def test_simple_url(self):
        result = WebCrawler._url_to_endpoint_data("https://example.com/api/users")
        self.assertEqual(result["method"], "GET")
        self.assertEqual(result["path"], "/api/users")
        self.assertEqual(result["parameters"], {})

    def test_url_with_query_params(self):
        result = WebCrawler._url_to_endpoint_data(
            "https://example.com/search?q=test&page=1"
        )
        self.assertEqual(result["method"], "GET")
        self.assertEqual(result["path"], "/search")
        self.assertIn("q", result["parameters"])
        self.assertIn("page", result["parameters"])
        self.assertEqual(result["parameters"]["q"]["example"], "test")
        self.assertEqual(result["parameters"]["page"]["example"], "1")
        self.assertEqual(result["parameters"]["q"]["location"], "query")

    def test_root_path(self):
        result = WebCrawler._url_to_endpoint_data("https://example.com/")
        self.assertEqual(result["path"], "/")
        self.assertEqual(result["parameters"], {})

    def test_empty_path_becomes_slash(self):
        result = WebCrawler._url_to_endpoint_data("https://example.com")
        self.assertEqual(result["path"], "/")

    def test_blank_query_value_preserved(self):
        result = WebCrawler._url_to_endpoint_data(
            "https://example.com/search?q=&lang=en"
        )
        self.assertIn("q", result["parameters"])
        self.assertEqual(result["parameters"]["q"]["example"], "")
        self.assertEqual(result["parameters"]["lang"]["example"], "en")


class TestCrawlConfig(unittest.TestCase):
    """Test CrawlConfig defaults and validation."""

    def test_default_values(self):
        cfg = CrawlConfig(start_url="https://example.com")
        self.assertEqual(cfg.max_pages, 200)
        self.assertEqual(cfg.max_depth, 10)
        self.assertEqual(cfg.strategy, "bfs")
        self.assertTrue(cfg.extract_js)
        self.assertTrue(cfg.parse_sitemap)
        self.assertEqual(cfg.exclude_patterns, [])
        self.assertIsNone(cfg.identity_id)
        self.assertIsNone(cfg.session_id)

    def test_custom_values(self):
        cfg = CrawlConfig(
            start_url="https://example.com",
            max_pages=50,
            max_depth=5,
            strategy="dfs",
            extract_js=False,
            parse_sitemap=False,
        )
        self.assertEqual(cfg.max_pages, 50)
        self.assertEqual(cfg.max_depth, 5)
        self.assertEqual(cfg.strategy, "dfs")
        self.assertFalse(cfg.extract_js)
        self.assertFalse(cfg.parse_sitemap)

    def test_invalid_strategy_raises(self):
        with self.assertRaises(ValueError):
            CrawlConfig(start_url="https://example.com", strategy="random")

    def test_invalid_max_pages_raises(self):
        with self.assertRaises(ValueError):
            CrawlConfig(start_url="https://example.com", max_pages=0)

    def test_invalid_max_depth_raises(self):
        with self.assertRaises(ValueError):
            CrawlConfig(start_url="https://example.com", max_depth=-1)


class TestScopeFiltering(unittest.TestCase):
    """Test that the crawler respects scope validation."""

    def test_out_of_scope_url_skipped_in_crawl_loop_logic(self):
        """Verify that scope_validator.validate_target returning invalid
        causes a URL to be skipped during the main crawl loop.

        We test this by inspecting the crawl method's filtering logic:
        _normalize_url produces a valid URL, but scope validation rejects it.
        """
        scope_validator = MagicMock()
        invalid_result = MagicMock()
        invalid_result.valid = False
        scope_validator.validate_target.return_value = invalid_result

        crawler = _make_crawler(scope_validator=scope_validator)

        # The URL itself is valid and normalizable
        url = "https://evil.com/malicious"
        normalized = WebCrawler._normalize_url(url, "https://evil.com")
        self.assertIsNotNone(normalized)

        # But scope says it is not valid
        vr = scope_validator.validate_target(normalized)
        self.assertFalse(vr.valid)

    def test_in_scope_url_accepted(self):
        """Verify that scope_validator returning valid allows the URL."""
        scope_validator = MagicMock()
        valid_result = MagicMock()
        valid_result.valid = True
        scope_validator.validate_target.return_value = valid_result

        crawler = _make_crawler(scope_validator=scope_validator)

        normalized = WebCrawler._normalize_url(
            "https://target.com/api", "https://target.com"
        )
        vr = scope_validator.validate_target(normalized)
        self.assertTrue(vr.valid)

    def test_scope_validator_exception_treated_as_out_of_scope(self):
        """When scope_validator.validate_target raises, the URL should
        be skipped (per the crawl loop's try/except -> continue)."""
        scope_validator = MagicMock()
        scope_validator.validate_target.side_effect = RuntimeError("scope check failed")

        crawler = _make_crawler(scope_validator=scope_validator)
        normalized = WebCrawler._normalize_url(
            "https://target.com/api", "https://target.com"
        )
        with self.assertRaises(RuntimeError):
            scope_validator.validate_target(normalized)


class TestBfsDfsStrategy(unittest.TestCase):
    """Test BFS vs DFS queue behaviour."""

    def test_bfs_popleft(self):
        """BFS should process URLs in FIFO order (popleft)."""
        q = collections.deque()
        q.append(("https://example.com/a", 0))
        q.append(("https://example.com/b", 0))
        q.append(("https://example.com/c", 0))

        # BFS: popleft
        url, depth = q.popleft()
        self.assertEqual(url, "https://example.com/a")
        url, depth = q.popleft()
        self.assertEqual(url, "https://example.com/b")
        url, depth = q.popleft()
        self.assertEqual(url, "https://example.com/c")

    def test_dfs_pop(self):
        """DFS should process URLs in LIFO order (pop)."""
        q = collections.deque()
        q.append(("https://example.com/a", 0))
        q.append(("https://example.com/b", 0))
        q.append(("https://example.com/c", 0))

        # DFS: pop (from right)
        url, depth = q.pop()
        self.assertEqual(url, "https://example.com/c")
        url, depth = q.pop()
        self.assertEqual(url, "https://example.com/b")
        url, depth = q.pop()
        self.assertEqual(url, "https://example.com/a")

    def test_bfs_dfs_strategy_selects_correct_operation(self):
        """Verify the crawl loop expression matches the strategy string."""
        state = CrawlState(crawl_id="test")
        state.queue.append(("https://example.com/first", 0))
        state.queue.append(("https://example.com/second", 1))

        # BFS strategy -> popleft
        bfs_url, bfs_depth = state.queue.popleft()
        self.assertEqual(bfs_url, "https://example.com/first")
        self.assertEqual(bfs_depth, 0)

        # Remaining item
        state.queue.append(("https://example.com/third", 2))

        # DFS strategy -> pop (from right)
        dfs_url, dfs_depth = state.queue.pop()
        self.assertEqual(dfs_url, "https://example.com/third")
        self.assertEqual(dfs_depth, 2)

    def test_crawl_state_queue_is_deque(self):
        """CrawlState.queue should be a collections.deque by default."""
        state = CrawlState(crawl_id="test")
        self.assertIsInstance(state.queue, collections.deque)


class TestParameterExtraction(unittest.TestCase):
    """Test WebCrawler._extract_parameters(url)."""

    def test_url_with_params(self):
        result = WebCrawler._extract_parameters(
            "https://example.com/search?q=hello&page=2"
        )
        self.assertIsInstance(result, list)
        self.assertEqual(len(result), 2)

        names = {p["name"] for p in result}
        self.assertIn("q", names)
        self.assertIn("page", names)

        for p in result:
            self.assertEqual(p["location"], "query")
            if p["name"] == "q":
                self.assertEqual(p["value"], "hello")
            elif p["name"] == "page":
                self.assertEqual(p["value"], "2")

    def test_url_without_params(self):
        result = WebCrawler._extract_parameters("https://example.com/api/users")
        self.assertIsInstance(result, list)
        self.assertEqual(len(result), 0)

    def test_single_param(self):
        result = WebCrawler._extract_parameters(
            "https://example.com/items?id=42"
        )
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0]["name"], "id")
        self.assertEqual(result[0]["value"], "42")
        self.assertEqual(result[0]["location"], "query")

    def test_blank_value_param(self):
        result = WebCrawler._extract_parameters(
            "https://example.com/search?q=&lang=en"
        )
        self.assertEqual(len(result), 2)
        names_values = {p["name"]: p["value"] for p in result}
        self.assertEqual(names_values["q"], "")
        self.assertEqual(names_values["lang"], "en")


class TestGetExtension(unittest.TestCase):
    """Test WebCrawler._get_extension(path) helper."""

    def test_css_extension(self):
        self.assertEqual(WebCrawler._get_extension("/static/style.css"), ".css")

    def test_no_extension(self):
        self.assertEqual(WebCrawler._get_extension("/api/users"), "")

    def test_hidden_dot_in_directory(self):
        # /path.to/resource has no real extension (slash after dot)
        self.assertEqual(WebCrawler._get_extension("/path.to/resource"), "")

    def test_uppercase_extension_lowered(self):
        self.assertEqual(WebCrawler._get_extension("/file.PNG"), ".png")

    def test_html_extension(self):
        self.assertEqual(WebCrawler._get_extension("/index.html"), ".html")


class TestCrawlResultDataclass(unittest.TestCase):
    """Test CrawlResult dataclass defaults."""

    def test_defaults(self):
        r = CrawlResult(url="https://example.com", status_code=200)
        self.assertEqual(r.url, "https://example.com")
        self.assertEqual(r.status_code, 200)
        self.assertEqual(r.links, [])
        self.assertEqual(r.forms, [])
        self.assertEqual(r.js_endpoints, [])
        self.assertEqual(r.parameters, [])

    def test_with_data(self):
        r = CrawlResult(
            url="https://example.com/page",
            status_code=301,
            links=["https://example.com/other"],
            js_endpoints=["/api/v1/data"],
        )
        self.assertEqual(r.status_code, 301)
        self.assertEqual(len(r.links), 1)
        self.assertEqual(len(r.js_endpoints), 1)


class TestBuildSummary(unittest.TestCase):
    """Test WebCrawler._build_summary(state)."""

    def test_summary_fields(self):
        state = CrawlState(crawl_id="abc-123")
        state.pages_crawled = 5
        state.endpoints_found = 10
        state.forms_found = 2
        state.js_endpoints_found = 3
        state.errors = 1
        state.status = "completed"
        state.visited = {"https://a.com", "https://b.com"}
        state.queue.append(("https://c.com", 1))
        state.start_time = 1000.0

        with patch("time.time", return_value=1005.5):
            summary = WebCrawler._build_summary(state)

        self.assertEqual(summary["crawl_id"], "abc-123")
        self.assertEqual(summary["status"], "completed")
        self.assertEqual(summary["pages_crawled"], 5)
        self.assertEqual(summary["endpoints_found"], 10)
        self.assertEqual(summary["forms_found"], 2)
        self.assertEqual(summary["js_endpoints_found"], 3)
        self.assertEqual(summary["errors"], 1)
        self.assertEqual(summary["urls_visited"], 2)
        self.assertEqual(summary["queue_remaining"], 1)
        self.assertEqual(summary["elapsed_seconds"], 5.5)


if __name__ == "__main__":
    unittest.main()
