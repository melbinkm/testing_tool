"""
SAST Tools - Source Code Analysis Integration

11 tools for static application security testing:
1. sast_clone_repo - Clone git repository
2. sast_index_repo - Index source code into RAG (wm_knowledge)
3. sast_scan_semgrep - Run Semgrep scanner
4. sast_scan_bandit - Run Bandit (Python only)
5. sast_scan_gitleaks - Run Gitleaks secret detection
6. sast_list_files - List files with priority scoring
7. sast_read_file - Read source file for LLM review
8. sast_search_code - Regex search across codebase
9. sast_record_finding - Record code-level finding
10. sast_correlate - Cross-reference SAST↔DAST findings
11. sast_get_progress - Show SAST review progress
"""

import json
import os
import re
from typing import List, Dict, Any, Optional
from pathlib import Path

from mcp.types import Tool, TextContent

from lib.sast_runner import get_sast_runner
from lib.code_indexer import get_code_indexer
from lib.world_model_db import get_world_model_db


# ========== Shared Helpers ==========

async def _get_db(mcp_service):
    """Get WorldModelDatabase for current assessment."""
    if mcp_service.current_assessment_id is None:
        raise ValueError("No assessment loaded. Call load_assessment first.")
    return await get_world_model_db(mcp_service.current_assessment_id)


def _json_content(data: Any) -> List[TextContent]:
    """JSON-wrapped TextContent."""
    return [TextContent(type="text", text=json.dumps(data, indent=2))]


def _error_content(msg: str) -> List[TextContent]:
    """Error JSON TextContent."""
    return [TextContent(type="text", text=json.dumps({"error": msg}, indent=2))]


async def _get_git_repo_url(mcp_service) -> Optional[str]:
    """Get git_repo_url from current assessment."""
    import httpx
    import logging
    log = logging.getLogger("autopentest-mcp")
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"http://localhost:8000/api/assessments/{mcp_service.current_assessment_id}"
            )
            if response.status_code == 200:
                data = response.json()
                return data.get("git_repo_url")
            else:
                log.warning(f"Failed to fetch assessment {mcp_service.current_assessment_id}: HTTP {response.status_code}")
    except Exception as e:
        log.warning(f"Failed to fetch git_repo_url for assessment {mcp_service.current_assessment_id}: {e}")
    return None


# ========== Tool Definitions ==========

def get_sast_tools() -> List[Tool]:
    """Get SAST tool definitions."""
    return [
        Tool(
            name="sast_clone_repo",
            description=(
                "Clone git repository into Kali workspace for SAST analysis. "
                "Supports authentication via environment variable token. "
                "Per-assessment isolation: /workspace/repos/{assessment_id}/. "
                "If repo_url not provided, reads git_repo_url from assessment."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "repo_url": {"type": "string", "description": "Git repository URL (optional if assessment.git_repo_url is set)"},
                    "branch": {"type": "string", "default": "main", "description": "Branch to clone (default: main)"},
                    "depth": {"type": "integer", "default": 0, "description": "Clone depth (0 = full history)"},
                    "auth_token_env": {"type": "string", "description": "Environment variable containing auth token (optional)"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_index_repo",
            description=(
                "Index all source files into wm_knowledge for RAG-based semantic search. "
                "After indexing, use wm_recall(query='...', category='source_code') to find "
                "code sections by meaning, not just grep. Dramatically faster than iterating files manually. "
                "Generates embeddings from function signatures and imports. "
                "Excludes vendor/node_modules/build directories automatically."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "languages": {"type": "array", "items": {"type": "string"}, "description": "Language filter (optional, e.g., ['python', 'javascript'])"},
                    "exclude_patterns": {"type": "array", "items": {"type": "string"}, "description": "Additional exclude patterns (default: ['.git', 'node_modules', 'vendor', '__pycache__', '.venv'])"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_scan_semgrep",
            description=(
                "Run Semgrep static analyzer with configurable rulesets. "
                "Auto-persists high/critical findings to wm_findings + cards. "
                "Supports: auto (recommended), p/owasp-top-10, p/security-audit, p/ci, custom rulesets. "
                "Returns structured findings with file/line/code_snippet. "
                "**Error if semgrep not installed** (no mock mode)."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "rulesets": {"type": "array", "items": {"type": "string"}, "default": ["auto"], "description": "Semgrep rulesets (default: auto)"},
                    "paths": {"type": "array", "items": {"type": "string"}, "description": "Specific paths to scan (optional)"},
                    "exclude_paths": {"type": "array", "items": {"type": "string"}, "description": "Paths to exclude (optional)"},
                    "severity_filter": {"type": "string", "description": "Minimum severity: INFO, WARNING, ERROR (optional)"},
                    "max_findings": {"type": "integer", "default": 200, "description": "Max findings to return"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_scan_bandit",
            description=(
                "Run Bandit (Python-only) security scanner. "
                "Checks for common Python security issues: SQL injection, hardcoded passwords, "
                "shell injection, insecure crypto, etc. "
                "Returns informational message if no Python files found. "
                "**Error if bandit not installed** (no mock mode)."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "confidence": {"type": "string", "default": "MEDIUM", "enum": ["LOW", "MEDIUM", "HIGH"], "description": "Minimum confidence level"},
                    "severity": {"type": "string", "default": "LOW", "enum": ["LOW", "MEDIUM", "HIGH"], "description": "Minimum severity level"},
                    "exclude_dirs": {"type": "array", "items": {"type": "string"}, "description": "Directories to exclude (optional)"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_scan_gitleaks",
            description=(
                "Run Gitleaks secret scanner to detect hardcoded secrets (API keys, passwords, tokens). "
                "Scans staged/unstaged files by default. Set scan_history=true to scan full git history. "
                "**Redacts secret values** (shows first/last 4 chars only) for safety. "
                "**Error if gitleaks not installed** (no mock mode)."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "scan_history": {"type": "boolean", "default": False, "description": "Scan full git history (default: false = staged/unstaged only)"},
                    "max_findings": {"type": "integer", "default": 100, "description": "Max findings to return"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_list_files",
            description=(
                "List source files with priority scoring to guide LLM code review. "
                "Priority score (0-100) based on file type: auth/login (+30), database (+20), "
                "API routes (+15), config (+10), tests (-10). "
                "Use this to decide what to review first with sast_read_file."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "language": {"type": "string", "description": "Language filter (optional, e.g., 'python')"},
                    "pattern": {"type": "string", "description": "Glob pattern filter (optional, e.g., '*.py')"},
                    "sort_by": {"type": "string", "default": "priority", "enum": ["priority", "size", "name"], "description": "Sort order"},
                    "limit": {"type": "integer", "default": 100, "description": "Max files to return"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_read_file",
            description=(
                "Read source file content for LLM manual code review. "
                "Supports line range selection for large files. "
                "Default 500-line limit for token budget. "
                "**Validates no path traversal** (rejects `..` in path)."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "file_path": {"type": "string", "description": "Relative file path within repo (required)"},
                    "start_line": {"type": "integer", "description": "Start line number (optional, 1-indexed)"},
                    "end_line": {"type": "integer", "description": "End line number (optional)"},
                    "max_lines": {"type": "integer", "default": 500, "description": "Max lines to return (default: 500)"},
                },
                "required": ["file_path"],
            },
        ),
        Tool(
            name="sast_search_code",
            description=(
                "Regex search across codebase. "
                "Use for pattern-based searches (SQL queries, crypto usage, dangerous functions). "
                "Returns file/line/content matches. "
                "For semantic search (find code by meaning), use wm_recall(category='source_code') instead."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "pattern": {"type": "string", "description": "Regex pattern (required)"},
                    "file_types": {"type": "array", "items": {"type": "string"}, "description": "File extensions filter (e.g., ['.py', '.js'])"},
                    "max_results": {"type": "integer", "default": 50, "description": "Max results to return"},
                },
                "required": ["pattern"],
            },
        ),
        Tool(
            name="sast_record_finding",
            description=(
                "Record code-level security finding to world model + cards. "
                "Creates card(type='finding') with SAST metadata: source, file, line, code_snippet, cwe. "
                "Also creates wm_finding. "
                "Use after confirming vulnerability through code review."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "file_path": {"type": "string", "description": "Vulnerable file path (required)"},
                    "line": {"type": "integer", "description": "Line number (required)"},
                    "title": {"type": "string", "description": "Finding title (required)"},
                    "severity": {"type": "string", "enum": ["info", "low", "medium", "high", "critical"], "description": "Severity (required)"},
                    "vuln_class": {"type": "string", "description": "Vulnerability class (e.g., sqli_error, xss_reflected)"},
                    "description": {"type": "string", "description": "Detailed vulnerability description"},
                    "code_snippet": {"type": "string", "description": "Vulnerable code snippet (3-5 lines context)"},
                    "remediation": {"type": "string", "description": "Remediation guidance (optional)"},
                    "confidence": {"type": "number", "default": 0.8, "minimum": 0.0, "maximum": 1.0},
                    "cwe": {"type": "string", "description": "CWE ID (optional, e.g., 'CWE-89')"},
                    "cvss_vector": {"type": "string", "description": "CVSS:3.1 vector string (optional, e.g., CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H)"},
                    "attack_scenario": {"type": "string", "description": "How an attacker could exploit this code vulnerability (optional)"},
                    "affected_endpoints": {"type": "string", "description": "Comma-separated URLs mapping to this code path (optional)"},
                },
                "required": ["file_path", "line", "title", "severity"],
            },
        ),
        Tool(
            name="sast_correlate",
            description=(
                "Cross-reference SAST findings with DAST findings to confirm code issues dynamically. "
                "Correlates by: (a) vuln_class match, (b) endpoint↔file mapping heuristics, "
                "(c) parameter match. Creates wm_relationships(rel_type='confirms'). "
                "Returns correlations, SAST-only count, DAST-only count, confirmed count."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "auto_correlate": {"type": "boolean", "default": True, "description": "Automatically correlate all findings"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_get_progress",
            description=(
                "Show SAST review progress: scanners run, files reviewed, RAG index status, "
                "findings by source/severity, correlations. "
                "Use to verify coverage and track manual review completeness."
            ),
            inputSchema={
                "type": "object",
                "properties": {},
                "required": [],
            },
        ),
        Tool(
            name="sast_get_next_unverified",
            description=(
                "Get next unverified SAST finding with function-level code extract and local file path. "
                "Returns vulnerable function code (not whole file), local file path for Claude Code access, "
                "and investigation guide for tracing execution flow. "
                "Workflow: 1) Receive function code, 2) Use local_file_path/repo_local_root with Read/Grep "
                "to trace data flow, 3) Test dynamically with inject_payload/inject_batch, "
                "4) Call sast_mark_verified with result. "
                "Returns status='all_verified' when no unverified findings remain."
            ),
            inputSchema={
                "type": "object",
                "properties": {},
                "required": [],
            },
        ),
        Tool(
            name="sast_mark_verified",
            description=(
                "Mark SAST finding as verified after dynamic testing. "
                "If exploitable=true: creates wm_finding with severity and remediation. "
                "If exploitable=false and false_positive=false: creates info-level finding "
                "(real but not demonstrable). "
                "If false_positive=true: no finding created, just marks verified. "
                "Includes remediation code snippet for all real findings."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "finding_id": {"type": "string", "description": "wm_knowledge ID from sast_get_next_unverified (required)"},
                    "exploitable": {"type": "boolean", "description": "Was vulnerability exploitable dynamically? (required)"},
                    "false_positive": {"type": "boolean", "default": False, "description": "Is this a false positive? (default: false)"},
                    "severity_override": {"type": "string", "enum": ["info", "low", "medium", "high", "critical"], "description": "Override SAST scanner severity (optional)"},
                    "remediation": {"type": "string", "description": "Remediation code snippet (required for exploitable findings)"},
                    "dynamic_evidence": {"type": "object", "description": "Request/response from testing (optional for exploitable)"},
                    "notes": {"type": "string", "description": "Explanation of testing or why false positive (optional)"},
                },
                "required": ["finding_id", "exploitable"],
            },
        ),
    ]


# ========== Tool Handlers ==========

async def handle_sast_tool(name: str, arguments: dict, mcp_service) -> List[TextContent]:
    """Handle all SAST tool calls."""

    try:
        if name == "sast_clone_repo":
            return await _handle_sast_clone_repo(arguments, mcp_service)
        elif name == "sast_index_repo":
            return await _handle_sast_index_repo(arguments, mcp_service)
        elif name == "sast_scan_semgrep":
            return await _handle_sast_scan_semgrep(arguments, mcp_service)
        elif name == "sast_scan_bandit":
            return await _handle_sast_scan_bandit(arguments, mcp_service)
        elif name == "sast_scan_gitleaks":
            return await _handle_sast_scan_gitleaks(arguments, mcp_service)
        elif name == "sast_list_files":
            return await _handle_sast_list_files(arguments, mcp_service)
        elif name == "sast_read_file":
            return await _handle_sast_read_file(arguments, mcp_service)
        elif name == "sast_search_code":
            return await _handle_sast_search_code(arguments, mcp_service)
        elif name == "sast_record_finding":
            return await _handle_sast_record_finding(arguments, mcp_service)
        elif name == "sast_correlate":
            return await _handle_sast_correlate(arguments, mcp_service)
        elif name == "sast_get_progress":
            return await _handle_sast_get_progress(arguments, mcp_service)
        elif name == "sast_get_next_unverified":
            return await _handle_sast_get_next_unverified(arguments, mcp_service)
        elif name == "sast_mark_verified":
            return await _handle_sast_mark_verified(arguments, mcp_service)
        else:
            return _error_content(f"Unknown SAST tool: {name}")
    except Exception as e:
        import traceback
        return _error_content(f"Error in {name}: {str(e)}\n{traceback.format_exc()}")


# ========== Handler Implementations ==========

async def _handle_sast_clone_repo(args: dict, mcp_service) -> List[TextContent]:
    """Clone git repository into assessment-scoped directory."""
    repo_url = args.get("repo_url")

    # If repo_url not provided, try to read from assessment
    if not repo_url:
        repo_url = await _get_git_repo_url(mcp_service)
        if not repo_url:
            return _error_content("No repo_url provided and no git_repo_url set in assessment")

    branch = args.get("branch", "main")
    depth = args.get("depth", 0)
    auth_token_env = args.get("auth_token_env")

    runner = get_sast_runner()
    result = await runner.clone_repo(
        url=repo_url,
        assessment_id=mcp_service.current_assessment_id,
        branch=branch,
        depth=depth,
        auth_token_env=auth_token_env,
    )

    return _json_content(result)


async def _handle_sast_index_repo(args: dict, mcp_service) -> List[TextContent]:
    """Index source code into wm_knowledge for RAG."""
    languages = args.get("languages")
    exclude_patterns = args.get("exclude_patterns")

    # Get repo path from previous clone
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"

    # Find repo directory (should be only one)
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    # First line is repo_dir itself, second is the actual repo
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]  # Take first subdirectory

    # Get database
    db = await _get_db(mcp_service)
    indexer = get_code_indexer(db)

    # Index repository
    result = await indexer.index_repo(
        repo_path=repo_path,
        sast_runner=runner,
        languages=languages,
        exclude_patterns=exclude_patterns,
    )

    return _json_content(result)


async def _handle_sast_scan_semgrep(args: dict, mcp_service) -> List[TextContent]:
    """Run Semgrep scanner."""
    rulesets = args.get("rulesets", ["auto"])
    paths = args.get("paths")
    exclude_paths = args.get("exclude_paths")
    severity_filter = args.get("severity_filter")
    max_findings = args.get("max_findings", 200)

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]

    # Run Semgrep
    result = await runner.run_semgrep(
        repo_path=repo_path,
        rulesets=rulesets,
        paths=paths,
        exclude=exclude_paths,
        severity=severity_filter,
    )

    if not result["success"]:
        return _json_content(result)

    # Limit findings
    findings = result["findings"][:max_findings]
    result["findings"] = findings
    result["findings_count"] = len(findings)

    # Store ALL findings in wm_knowledge for verification workflow
    if findings:
        db = await _get_db(mcp_service)
        for finding in findings:
            try:
                # Store in knowledge base for sast_get_next_unverified
                await db.store_knowledge(
                    title=f"semgrep:{finding.get('rule_id', 'unknown')} in {finding.get('file', 'unknown')}:{finding.get('line', 0)}",
                    content=json.dumps(finding),
                    category="sast_scan_result",
                    source_tool="sast_semgrep",
                    metadata={
                        "verified": False,
                        "vuln_class": finding.get("vuln_class", "misconfig"),
                        "severity": finding.get("severity", "medium"),
                        "file": finding.get("file", ""),
                        "line": finding.get("line", 0),
                        "rule_id": finding.get("rule_id", ""),
                        "remediation": finding.get("remediation", ""),
                    },
                    tags=[finding.get("language", "unknown"), finding.get("vuln_class", "misconfig")],
                )
            except Exception:
                pass  # Continue on error

    return _json_content(result)


async def _handle_sast_scan_bandit(args: dict, mcp_service) -> List[TextContent]:
    """Run Bandit (Python-only) scanner."""
    confidence = args.get("confidence", "MEDIUM")
    severity = args.get("severity", "LOW")
    exclude_dirs = args.get("exclude_dirs")

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]

    # Check for Python files
    py_check = await runner._exec("find", [repo_path, "-name", "*.py", "-type", "f"], timeout=30)
    if not py_check["stdout"].strip():
        return _json_content({
            "success": True,
            "findings": [],
            "scan_time_ms": 0,
            "message": "No Python files found in repository. Bandit is Python-only.",
        })

    # Run Bandit
    result = await runner.run_bandit(
        repo_path=repo_path,
        confidence=confidence,
        severity=severity,
        exclude_dirs=exclude_dirs,
    )

    # Store findings in wm_knowledge for verification workflow
    if result.get("success") and result.get("findings"):
        db = await _get_db(mcp_service)
        for finding in result["findings"]:
            try:
                await db.store_knowledge(
                    title=f"bandit:{finding.get('test_id', 'unknown')} in {finding.get('file', 'unknown')}:{finding.get('line', 0)}",
                    content=json.dumps(finding),
                    category="sast_scan_result",
                    source_tool="sast_bandit",
                    metadata={
                        "verified": False,
                        "vuln_class": finding.get("vuln_class", "misconfig"),
                        "severity": finding.get("severity", "medium"),
                        "file": finding.get("file", ""),
                        "line": finding.get("line", 0),
                        "rule_id": finding.get("test_id", ""),
                        "remediation": finding.get("remediation", ""),
                    },
                    tags=["python", finding.get("vuln_class", "misconfig")],
                )
            except Exception:
                pass  # Continue on error

    return _json_content(result)


async def _handle_sast_scan_gitleaks(args: dict, mcp_service) -> List[TextContent]:
    """Run Gitleaks secret scanner."""
    scan_history = args.get("scan_history", False)
    max_findings = args.get("max_findings", 100)

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]

    # Run Gitleaks
    result = await runner.run_gitleaks(
        repo_path=repo_path,
        scan_history=scan_history,
    )

    if not result["success"]:
        return _json_content(result)

    # Limit findings
    findings = result["findings"][:max_findings]
    result["findings"] = findings
    result["findings_count"] = len(findings)

    # Store findings in wm_knowledge for verification workflow
    if findings:
        db = await _get_db(mcp_service)
        for finding in findings:
            try:
                await db.store_knowledge(
                    title=f"gitleaks:{finding.get('rule_id', 'unknown')} in {finding.get('file', 'unknown')}:{finding.get('line', 0)}",
                    content=json.dumps(finding),
                    category="sast_scan_result",
                    source_tool="sast_gitleaks",
                    metadata={
                        "verified": False,
                        "vuln_class": "info_disclosure",  # Gitleaks findings are secret leaks
                        "severity": "high",  # Secret leaks are always high severity
                        "file": finding.get("file", ""),
                        "line": finding.get("line", 0),
                        "rule_id": finding.get("rule_id", ""),
                        "remediation": "Remove hardcoded secrets, use environment variables or secret management system",
                    },
                    tags=["secrets", "info_disclosure"],
                )
            except Exception:
                pass  # Continue on error

    return _json_content(result)


async def _handle_sast_list_files(args: dict, mcp_service) -> List[TextContent]:
    """List source files with priority scoring."""
    language = args.get("language")
    pattern = args.get("pattern")
    sort_by = args.get("sort_by", "priority")
    limit = args.get("limit", 100)

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]

    # Build find command
    find_args = [repo_path, "-type", "f"]
    if pattern:
        find_args.extend(["-name", pattern])

    # Execute find
    find_result = await runner._exec("find", find_args, timeout=60)
    if find_result["exit_code"] != 0:
        return _error_content("Failed to list files")

    all_files = find_result["stdout"].splitlines()

    # Filter by language if specified
    if language:
        lang_exts = {
            "python": [".py"],
            "javascript": [".js", ".jsx"],
            "typescript": [".ts", ".tsx"],
            "java": [".java"],
            "go": [".go"],
            "ruby": [".rb"],
            "php": [".php"],
        }
        exts = lang_exts.get(language.lower(), [])
        all_files = [f for f in all_files if any(f.endswith(ext) for ext in exts)]

    # Build file list with metadata
    files = []
    languages_detected = set()

    for file_path in all_files:
        # Make path relative
        rel_path = file_path[len(repo_path):].lstrip("/") if file_path.startswith(repo_path) else file_path

        # Detect language
        lang = _detect_file_language(rel_path)
        if lang:
            languages_detected.add(lang)

        # Count lines
        wc_result = await runner._exec("wc", ["-l", file_path], timeout=5)
        line_count = 0
        if wc_result["exit_code"] == 0:
            match = re.search(r"^(\d+)", wc_result["stdout"])
            if match:
                line_count = int(match.group(1))

        # Get file size
        stat_result = await runner._exec("stat", ["-c", "%s", file_path], timeout=5)
        size_bytes = 0
        if stat_result["exit_code"] == 0:
            try:
                size_bytes = int(stat_result["stdout"].strip())
            except ValueError:
                pass

        # Calculate priority score
        priority_score, priority_reasons = _calculate_file_priority(rel_path)

        files.append({
            "path": rel_path,
            "language": lang or "unknown",
            "size_bytes": size_bytes,
            "line_count": line_count,
            "priority_score": priority_score,
            "priority_reasons": priority_reasons,
        })

    # Sort
    if sort_by == "priority":
        files.sort(key=lambda f: f["priority_score"], reverse=True)
    elif sort_by == "size":
        files.sort(key=lambda f: f["size_bytes"], reverse=True)
    elif sort_by == "name":
        files.sort(key=lambda f: f["path"])

    # Limit
    files = files[:limit]

    result = {
        "files": files,
        "total_files": len(all_files),
        "languages": sorted(list(languages_detected)),
    }

    return _json_content(result)


async def _handle_sast_read_file(args: dict, mcp_service) -> List[TextContent]:
    """Read source file for LLM review."""
    file_path = args["file_path"]
    start_line = args.get("start_line")
    end_line = args.get("end_line")
    max_lines = args.get("max_lines", 500)

    # Validate no path traversal
    if ".." in file_path:
        return _error_content("Path traversal not allowed (contains '..')")

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]
    full_path = f"{repo_path}/{file_path}"

    # Read file
    cat_result = await runner._exec("cat", [full_path], timeout=30)
    if cat_result["exit_code"] != 0:
        return _error_content(f"Failed to read file: {cat_result['stderr']}")

    content = cat_result["stdout"]
    lines = content.splitlines()
    total_lines = len(lines)

    # Apply line range
    if start_line is not None:
        start_idx = max(0, start_line - 1)  # 1-indexed -> 0-indexed
    else:
        start_idx = 0

    if end_line is not None:
        end_idx = min(total_lines, end_line)
    else:
        end_idx = min(total_lines, start_idx + max_lines)

    # Truncate if needed
    if (end_idx - start_idx) > max_lines:
        end_idx = start_idx + max_lines

    selected_lines = lines[start_idx:end_idx]
    selected_content = "\n".join(selected_lines)

    language = _detect_file_language(file_path)

    result = {
        "file_path": file_path,
        "language": language or "unknown",
        "content": selected_content,
        "start_line": start_idx + 1,
        "end_line": end_idx,
        "total_lines": total_lines,
        "truncated": end_idx < total_lines,
    }

    return _json_content(result)


async def _handle_sast_search_code(args: dict, mcp_service) -> List[TextContent]:
    """Regex search across codebase."""
    pattern = args["pattern"]
    file_types = args.get("file_types", [])
    max_results = args.get("max_results", 50)

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]

    # Build grep command
    grep_args = ["-rn", pattern, repo_path]
    if file_types:
        for ft in file_types:
            grep_args.extend(["--include", f"*{ft}"])

    # Execute grep
    grep_result = await runner._exec("grep", grep_args, timeout=60)

    # grep exit code 1 = no matches (not an error)
    if grep_result["exit_code"] not in [0, 1]:
        return _error_content("Grep search failed")

    if not grep_result["stdout"].strip():
        return _json_content({"matches": [], "total_matches": 0})

    # Parse results
    matches = []
    for line in grep_result["stdout"].splitlines()[:max_results]:
        # Format: /path/to/file:123:matched content
        parts = line.split(":", 2)
        if len(parts) >= 3:
            file_path = parts[0]
            # Make relative
            rel_path = file_path[len(repo_path):].lstrip("/") if file_path.startswith(repo_path) else file_path

            try:
                line_num = int(parts[1])
            except ValueError:
                continue

            content = parts[2]

            matches.append({
                "file": rel_path,
                "line": line_num,
                "content": content.strip(),
            })

    result = {
        "matches": matches,
        "total_matches": len(matches),
    }

    return _json_content(result)


async def _handle_sast_record_finding(args: dict, mcp_service) -> List[TextContent]:
    """Record code-level finding."""
    file_path = args["file_path"]
    line = args["line"]
    title = args["title"]
    severity = args["severity"]
    vuln_class = args.get("vuln_class")
    description = args.get("description", "")
    code_snippet = args.get("code_snippet", "")
    remediation = args.get("remediation")
    confidence = args.get("confidence", 0.8)
    cwe = args.get("cwe")
    cvss_vector = args.get("cvss_vector")
    attack_scenario = args.get("attack_scenario")
    affected_endpoints = args.get("affected_endpoints")

    # Auto-compute cvss_score from cvss_vector if provided
    cvss_score = None
    if cvss_vector:
        try:
            from lib.risk_engine import CVSSv31
            cvss_score = CVSSv31(cvss_vector).base_score()
        except Exception:
            pass

    # Build card description
    description_parts = []

    if vuln_class:
        description_parts.append(f"**Vulnerability Class:** {vuln_class}")

    description_parts.append(f"**File:** `{file_path}:{line}`")

    if cwe:
        description_parts.append(f"**CWE:** {cwe}")

    if code_snippet:
        description_parts.append(f"\n**Code:**\n```\n{code_snippet}\n```")

    if description:
        description_parts.append(f"\n{description}")

    if remediation:
        description_parts.append(f"\n**Remediation:**\n{remediation}")

    card_description = "\n\n".join(description_parts)

    # Create card via safe_add_card
    result = await mcp_service.safe_add_card(
        card_type="finding",
        title=title,
        description=card_description,
        severity=severity.upper() if isinstance(severity, str) else severity,
        status="confirmed",
        target_service=file_path,
        context=f"line={line}, vuln_class={vuln_class}" if vuln_class else f"line={line}",
        confidence=confidence,
        cvss_vector=cvss_vector,
        cvss_score=cvss_score,
        attack_scenario=attack_scenario,
        affected_endpoints=affected_endpoints,
        recommendation=remediation,
        evidence=json.dumps({
            "source": "sast_llm_review",
            "file": file_path,
            "line": line,
            "code_snippet": code_snippet,
            "cwe": cwe,
        }),
    )

    if not result.ok:
        return _error_content(f"Failed to create card: {result.reason}")

    card_id = result.data.get("id") if result.data else None

    response = {
        "finding_id": str(card_id) if card_id else None,
        "card_id": str(card_id) if card_id else None,
        "success": True,
    }

    return _json_content(response)


async def _handle_sast_correlate(args: dict, mcp_service) -> List[TextContent]:
    """Cross-reference SAST↔DAST findings."""
    auto_correlate = args.get("auto_correlate", True)

    db = await _get_db(mcp_service)

    # SAST from wm_knowledge (verified findings only), DAST from wm_findings
    sast_results = await db.query("knowledge", filters={"category": "sast_scan_result"}, limit=500)
    sast_findings = [r for r in sast_results if r.get("metadata", {}).get("verified", False)
                     and r.get("metadata", {}).get("exploitable", False)]
    dast_findings = await db.query("findings")

    correlations = []

    if auto_correlate:
        # Correlate by vuln_class
        for sast_f in sast_findings:
            sast_class = sast_f.get("metadata", {}).get("vuln_class")
            if not sast_class:
                continue

            for dast_f in dast_findings:
                dast_meta = dast_f.get("metadata", {}) or {}
                dast_class = dast_meta.get("vuln_type") or dast_meta.get("vuln_class")
                if dast_class == sast_class:
                    correlations.append({
                        "sast_finding": sast_f.get("id"),
                        "dast_finding": dast_f.get("id"),
                        "vuln_class": sast_class,
                        "confidence": 0.7,
                        "correlation_type": "vuln_class_match",
                    })

                    # Create relationship
                    try:
                        await db.add_relationship(
                            from_entity_type="finding",
                            from_entity_id=str(dast_f.get("id")),
                            to_entity_type="finding",
                            to_entity_id=str(sast_f.get("id")),
                            rel_type="confirms",
                            metadata={"correlation": "SAST↔DAST", "vuln_class": sast_class},
                        )
                    except Exception:
                        pass

    result = {
        "correlations": correlations,
        "total_sast_findings": len(sast_results),
        "verified_sast_findings": len(sast_findings),
        "sast_only_count": len(sast_findings) - len(correlations),
        "dast_only_count": len(dast_findings) - len(correlations),
        "confirmed_count": len(correlations),
    }

    return _json_content(result)


async def _handle_sast_get_progress(args: dict, mcp_service) -> List[TextContent]:
    """Show SAST review progress."""
    db = await _get_db(mcp_service)

    # Get repo info
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)

    repo_path = None
    if find_result["exit_code"] == 0:
        dirs = find_result["stdout"].strip().splitlines()
        if len(dirs) >= 2:
            repo_path = dirs[1]

    # Query wm_knowledge where SAST scan results actually live
    sast_results = await db.query("knowledge", filters={"category": "sast_scan_result"}, limit=500)

    # Count by source_tool (stored in metadata)
    findings_by_source = {}
    for r in sast_results:
        source = r.get("metadata", {}).get("source_tool", "unknown")
        findings_by_source[source] = findings_by_source.get(source, 0) + 1

    # Count by severity
    findings_by_severity = {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0}
    for r in sast_results:
        sev = r.get("metadata", {}).get("severity", "info").lower()
        if sev in findings_by_severity:
            findings_by_severity[sev] += 1

    # Count verified vs unverified
    verified_count = len([r for r in sast_results if r.get("metadata", {}).get("verified", False)])
    unverified_count = len(sast_results) - verified_count

    # Check RAG index
    knowledge_entries = await db.query("knowledge", filters={"category": "source_code"})
    files_indexed = len(knowledge_entries)

    # Check correlations
    relationships = await db.query("relationships", filters={"rel_type": "confirms"})
    correlations_count = len([r for r in relationships if "SAST↔DAST" in r.get("metadata", {}).get("correlation", "")])

    result = {
        "repo": repo_path or "Not cloned",
        "scanners": {
            "semgrep": findings_by_source.get("sast_semgrep", 0),
            "bandit": findings_by_source.get("sast_bandit", 0),
            "gitleaks": findings_by_source.get("sast_gitleaks", 0),
            "llm_review": findings_by_source.get("sast_llm_review", 0),
        },
        "rag_index": {
            "files_indexed": files_indexed,
            "searchable": files_indexed > 0,
        },
        "findings": {
            "total": len(sast_results),
            "verified": verified_count,
            "unverified": unverified_count,
            "by_source": findings_by_source,
            "by_severity": findings_by_severity,
        },
        "correlations": {
            "confirmed": correlations_count,
        },
    }

    return _json_content(result)


async def _handle_sast_get_next_unverified(args: dict, mcp_service) -> List[TextContent]:
    """Get next unverified SAST finding with function-level code extract."""
    db = await _get_db(mcp_service)

    # Query for unverified SAST scan results
    sast_results = await db.query("knowledge", filters={"category": "sast_scan_result"}, limit=500)
    unverified = [r for r in sast_results if not r.get("metadata", {}).get("verified", False)]

    if not unverified:
        # All findings verified
        verified_count = len([r for r in sast_results if r.get("metadata", {}).get("verified", False)])
        return _json_content({
            "status": "all_verified",
            "total_verified": verified_count,
            "message": "All SAST scan results have been verified",
        })

    # Get first unverified finding
    finding_entry = unverified[0]
    finding_data = json.loads(finding_entry.get("content", "{}"))
    metadata = finding_entry.get("metadata", {})

    # Extract details
    file_path = metadata.get("file", "")
    line_number = metadata.get("line", 0)
    vuln_class = metadata.get("vuln_class", "unknown")
    severity = metadata.get("severity", "medium")
    rule_id = metadata.get("rule_id", "unknown")
    remediation_hint = metadata.get("remediation", "")

    # Read source file
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)

    if find_result["exit_code"] != 0 or len(find_result["stdout"].strip().splitlines()) < 2:
        return _error_content("No cloned repository found. Cannot extract function code.")

    repo_path = find_result["stdout"].strip().splitlines()[1]
    repo_name = repo_path.split("/")[-1]
    full_file_path = f"{repo_path}/{file_path}"

    # Read file content
    cat_result = await runner._exec("cat", [full_file_path], timeout=30)
    if cat_result["exit_code"] != 0:
        return _error_content(f"Failed to read file: {file_path}")

    content = cat_result["stdout"]

    # Extract function containing the vulnerable line
    indexer = get_code_indexer(db)
    language = indexer._detect_language(file_path)
    function_extract = indexer.extract_function_at_line(content, language, line_number)

    # Compute local file paths for Claude Code access
    local_file_path = f"/mnt/d/testing_tool/AutoPentest/sast_repos/{mcp_service.current_assessment_id}/{repo_name}/{file_path}"
    repo_local_root = f"/mnt/d/testing_tool/AutoPentest/sast_repos/{mcp_service.current_assessment_id}/{repo_name}"

    # Generate investigation guide
    investigation_guide = (
        "1. Search for callers: Use Grep tool with function_name across repo_local_root\n"
        "2. Trace input: Find HTTP handler/route that calls this function (search for function_name in route files)\n"
        "3. Check sanitization: Look for input validation, middleware, WAF rules between input and this function\n"
        "4. Look for related code: Use Read tool on files in repo_local_root to understand context\n"
        "5. Test dynamically: Use inject_payload/inject_batch tools to test actual exploitation"
    )

    # Get title from finding data
    title = finding_data.get("title") or finding_data.get("message") or f"{rule_id} in {file_path}:{line_number}"

    return _json_content({
        "finding_id": str(finding_entry.get("id")),
        "title": title,
        "severity": severity,
        "vuln_class": vuln_class,
        "file": file_path,
        "line": line_number,
        "function_name": function_extract.get("function_name"),
        "function_code": function_extract.get("code"),
        "local_file_path": local_file_path,
        "repo_local_root": repo_local_root,
        "remediation_hint": remediation_hint,
        "investigation_guide": investigation_guide,
        "total_remaining": len(unverified),
    })


async def _handle_sast_mark_verified(args: dict, mcp_service) -> List[TextContent]:
    """Mark SAST finding as verified and record result."""
    finding_id = args["finding_id"]
    exploitable = args["exploitable"]
    false_positive = args.get("false_positive", False)
    severity_override = args.get("severity_override")
    remediation = args.get("remediation", "")
    dynamic_evidence = args.get("dynamic_evidence")
    notes = args.get("notes", "")

    db = await _get_db(mcp_service)

    # Get finding entry from knowledge
    try:
        knowledge_entries = await db.query("knowledge", filters={"id": finding_id}, limit=1)
        if not knowledge_entries:
            return _error_content(f"Finding {finding_id} not found in knowledge base")

        finding_entry = knowledge_entries[0]
        finding_data = json.loads(finding_entry.get("content", "{}"))
        metadata = finding_entry.get("metadata", {})
    except Exception as e:
        return _error_content(f"Failed to load finding: {str(e)}")

    # Update knowledge entry metadata
    await db.update_knowledge(
        knowledge_id=finding_id,
        metadata={"verified": True, "exploitable": exploitable, "false_positive": false_positive},
    )

    # Handle different verification outcomes
    if false_positive:
        # No finding created, just mark verified with notes
        return _json_content({
            "success": True,
            "action": "marked_false_positive",
            "finding_id": finding_id,
            "notes": notes,
        })

    # Determine final severity
    final_severity = severity_override or metadata.get("severity", "medium")
    if not exploitable:
        final_severity = "info"  # Real but not demonstrable -> info

    # Build description
    description_parts = []
    description_parts.append(finding_data.get("message", "SAST finding from code analysis"))

    if not exploitable:
        description_parts.append("\n\n**Note:** This vulnerability was confirmed in source code but could not be demonstrated dynamically through testing.")

    if dynamic_evidence:
        description_parts.append("\n\n**Dynamic Testing Evidence:**")
        description_parts.append(f"```\n{json.dumps(dynamic_evidence, indent=2)}\n```")

    if notes:
        description_parts.append(f"\n\n**Testing Notes:**\n{notes}")

    description = "\n".join(description_parts)

    # Create finding via sast_record_finding logic
    file_path = metadata.get("file", "")
    line = metadata.get("line", 0)
    vuln_class = metadata.get("vuln_class", "misconfig")
    title = finding_data.get("title") or finding_data.get("message") or f"SAST: {vuln_class} in {file_path}"

    # Build card description
    card_description_parts = []
    if vuln_class:
        card_description_parts.append(f"**Vulnerability Class:** {vuln_class}")
    card_description_parts.append(f"**File:** `{file_path}:{line}`")
    if finding_data.get("cwe"):
        card_description_parts.append(f"**CWE:** {finding_data['cwe']}")
    if finding_data.get("code_snippet"):
        card_description_parts.append(f"\n**Code:**\n```\n{finding_data['code_snippet']}\n```")
    card_description_parts.append(f"\n{description}")
    if remediation:
        card_description_parts.append(f"\n**Remediation:**\n{remediation}")

    card_description = "\n\n".join(card_description_parts)

    # Create card
    result = await mcp_service.safe_add_card(
        card_type="finding",
        title=title,
        description=card_description,
        severity=final_severity.upper(),
        status="confirmed",
        target_service=file_path,
        context=f"line={line}, vuln_class={vuln_class}",
        confidence=0.8 if exploitable else 0.6,
        evidence=json.dumps({
            "source": "sast_verification",
            "file": file_path,
            "line": line,
            "code_snippet": finding_data.get("code_snippet"),
            "cwe": finding_data.get("cwe"),
            "dynamic_evidence": dynamic_evidence,
        }),
    )

    if not result.ok:
        return _error_content(f"Failed to create card: {result.reason}")

    card_id = result.data.get("id") if result.data else None

    return _json_content({
        "success": True,
        "action": "finding_recorded" if exploitable else "informational_finding_recorded",
        "finding_id": str(card_id) if card_id else None,
        "card_id": str(card_id) if card_id else None,
        "severity": final_severity,
        "exploitable": exploitable,
    })


# ========== Helper Functions ==========

def _detect_file_language(file_path: str) -> Optional[str]:
    """Detect language from file extension."""
    ext_map = {
        ".py": "python",
        ".js": "javascript",
        ".ts": "typescript",
        ".jsx": "javascript",
        ".tsx": "typescript",
        ".java": "java",
        ".go": "go",
        ".rb": "ruby",
        ".php": "php",
        ".c": "c",
        ".cpp": "cpp",
        ".cs": "csharp",
        ".rs": "rust",
        ".swift": "swift",
        ".kt": "kotlin",
    }

    for ext, lang in ext_map.items():
        if file_path.endswith(ext):
            return lang

    return None


def _calculate_file_priority(file_path: str) -> tuple:
    """Calculate file priority score (0-100) and reasons.

    Returns: (score, reasons)
    """
    path_lower = file_path.lower()
    score = 50  # Base score
    reasons = []

    # Routes, controllers, handlers, views (+30)
    if any(keyword in path_lower for keyword in ["route", "controller", "handler", "view", "endpoint"]):
        score += 30
        reasons.append("Route/Controller")

    # Auth, login, session (+25)
    if any(keyword in path_lower for keyword in ["auth", "login", "session", "token", "password"]):
        score += 25
        reasons.append("Authentication")

    # Database, ORM, query, model (+20)
    if any(keyword in path_lower for keyword in ["database", "db", "orm", "query", "model"]):
        score += 20
        reasons.append("Database")

    # API definitions (+15)
    if any(keyword in path_lower for keyword in ["api", "graphql", "rest"]):
        score += 15
        reasons.append("API")

    # Config, settings (+10)
    if any(keyword in path_lower for keyword in ["config", "settings", "env"]):
        score += 10
        reasons.append("Config")

    # Test files (-10)
    if any(keyword in path_lower for keyword in ["test", "spec", "_test.", ".test."]):
        score -= 10
        reasons.append("Test file")

    # Generated, vendor (-20)
    if any(keyword in path_lower for keyword in ["generated", "vendor", "node_modules", ".min.", "bundle"]):
        score -= 20
        reasons.append("Generated/Vendor")

    return (max(0, min(100, score)), reasons)
