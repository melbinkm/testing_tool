"""
SAST Tools - Source Code Analysis Integration

18 tools for static application security testing:
1. sast_clone_repo - Clone git repository
2. sast_index_repo - Index source code into RAG (wm_knowledge)
3. sast_scan_semgrep - Run Semgrep scanner
4. sast_scan_bandit - Run Bandit (Python only)
5. sast_scan_gitleaks - Run Gitleaks secret detection
6. sast_list_files - List files with priority scoring
7. sast_read_file - Read source file for LLM review
8. sast_search_code - Regex search across codebase
9. sast_record_finding - Record code-level finding
10. sast_correlate - Cross-reference SAST↔DAST findings
11. sast_get_progress - Show SAST review progress
12. sast_get_next_unverified - Get next unverified SAST finding
13. sast_mark_verified - Mark SAST finding as verified
14. sast_trace_dataflow - Trace source→sanitization→sink data flows
15. sast_build_exploitation_queue - Build prioritized exploitation queue from SAST
16. sast_get_dast_targets - Map exploitation queue to DAST endpoints
17. sast_sink_hunt - Hunt for dangerous sinks with specialized Semgrep rules
18. sast_bind_endpoints - Bind code sinks to discovered endpoints
"""

import json
import os
import re
from typing import List, Dict, Any, Optional
from pathlib import Path

from mcp.types import Tool, TextContent

from lib.sast_runner import get_sast_runner
from lib.code_indexer import get_code_indexer
from lib.tool_helpers import _get_db, _json_content, _error_content


async def _get_git_repo_url(mcp_service) -> Optional[str]:
    """Get git_repo_url from current assessment."""
    import logging
    log = logging.getLogger("autopentest-mcp")
    try:
        response = await mcp_service.http_client.get(
            f"{mcp_service.backend_url}/assessments/{mcp_service.current_assessment_id}"
        )
        if response.status_code == 200:
            data = response.json()
            return data.get("git_repo_url")
        else:
            log.warning(f"Failed to fetch assessment {mcp_service.current_assessment_id}: HTTP {response.status_code}")
    except Exception as e:
        log.warning(f"Failed to fetch git_repo_url for assessment {mcp_service.current_assessment_id}: {e}")
    return None


# ========== Tool Definitions ==========

def get_sast_tools() -> List[Tool]:
    """Get SAST tool definitions."""
    return [
        Tool(
            name="sast_clone_repo",
            description=(
                "Clone git repository into Kali workspace for SAST analysis. "
                "Supports authentication via environment variable token. "
                "Per-assessment isolation: /workspace/repos/{assessment_id}/. "
                "If repo_url not provided, reads git_repo_url from assessment."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "repo_url": {"type": "string", "description": "Git repository URL (optional if assessment.git_repo_url is set)"},
                    "branch": {"type": "string", "default": "main", "description": "Branch to clone (default: main)"},
                    "depth": {"type": "integer", "default": 0, "description": "Clone depth (0 = full history)"},
                    "auth_token_env": {"type": "string", "description": "Environment variable containing auth token (optional)"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_index_repo",
            description=(
                "Index all source files into wm_knowledge for RAG-based semantic search. "
                "After indexing, use wm_recall(query='...', category='source_code') to find "
                "code sections by meaning, not just grep. Dramatically faster than iterating files manually. "
                "Generates embeddings from function signatures and imports. "
                "Excludes vendor/node_modules/build directories automatically."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "languages": {"type": "array", "items": {"type": "string"}, "description": "Language filter (optional, e.g., ['python', 'javascript'])"},
                    "exclude_patterns": {"type": "array", "items": {"type": "string"}, "description": "Additional exclude patterns (default: ['.git', 'node_modules', 'vendor', '__pycache__', '.venv'])"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_scan_semgrep",
            description=(
                "Run Semgrep static analyzer with configurable rulesets. "
                "Auto-persists high/critical findings to wm_findings + cards. "
                "Use 'auto' ruleset (default, recommended). Do NOT use p/owasp-top-10 or p/security-audit (removed from registry). "
                "Returns structured findings with file/line/code_snippet. "
                "**Error if semgrep not installed** (no mock mode)."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "rulesets": {"type": "array", "items": {"type": "string"}, "default": ["auto"], "description": "Semgrep rulesets (default: auto)"},
                    "paths": {"type": "array", "items": {"type": "string"}, "description": "Specific paths to scan (optional)"},
                    "exclude_paths": {"type": "array", "items": {"type": "string"}, "description": "Paths to exclude (optional)"},
                    "severity_filter": {"type": "string", "description": "Minimum severity: INFO, WARNING, ERROR (optional)"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_scan_bandit",
            description=(
                "Run Bandit (Python-only) security scanner. "
                "Checks for common Python security issues: SQL injection, hardcoded passwords, "
                "shell injection, insecure crypto, etc. "
                "Returns informational message if no Python files found. "
                "**Error if bandit not installed** (no mock mode)."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "confidence": {"type": "string", "default": "MEDIUM", "enum": ["LOW", "MEDIUM", "HIGH"], "description": "Minimum confidence level"},
                    "severity": {"type": "string", "default": "LOW", "enum": ["LOW", "MEDIUM", "HIGH"], "description": "Minimum severity level"},
                    "exclude_dirs": {"type": "array", "items": {"type": "string"}, "description": "Directories to exclude (optional)"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_scan_gitleaks",
            description=(
                "Run Gitleaks secret scanner to detect hardcoded secrets (API keys, passwords, tokens). "
                "Scans staged/unstaged files by default. Set scan_history=true to scan full git history. "
                "**Redacts secret values** (shows first/last 4 chars only) for safety. "
                "**Error if gitleaks not installed** (no mock mode)."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "scan_history": {"type": "boolean", "default": False, "description": "Scan full git history (default: false = staged/unstaged only)"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_list_files",
            description=(
                "List source files with priority scoring to guide LLM code review. "
                "Priority score (0-100) based on file type: auth/login (+30), database (+20), "
                "API routes (+15), config (+10), tests (-10). "
                "Use this to decide what to review first with sast_read_file."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "language": {"type": "string", "description": "Language filter (optional, e.g., 'python')"},
                    "pattern": {"type": "string", "description": "Glob pattern filter (optional, e.g., '*.py')"},
                    "sort_by": {"type": "string", "default": "priority", "enum": ["priority", "size", "name"], "description": "Sort order"},
                    "limit": {"type": "integer", "default": 100, "description": "Max files to return"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_read_file",
            description=(
                "Read source file content for LLM manual code review. "
                "Supports line range selection for large files. "
                "Default 500-line limit for token budget. "
                "**Validates no path traversal** (rejects `..` in path)."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "file_path": {"type": "string", "description": "Relative file path within repo (required)"},
                    "start_line": {"type": "integer", "description": "Start line number (optional, 1-indexed)"},
                    "end_line": {"type": "integer", "description": "End line number (optional)"},
                    "max_lines": {"type": "integer", "default": 500, "description": "Max lines to return (default: 500)"},
                },
                "required": ["file_path"],
            },
        ),
        Tool(
            name="sast_search_code",
            description=(
                "Regex search across codebase. "
                "Use for pattern-based searches (SQL queries, crypto usage, dangerous functions). "
                "Returns file/line/content matches. "
                "For semantic search (find code by meaning), use wm_recall(category='source_code') instead."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "pattern": {"type": "string", "description": "Regex pattern (required)"},
                    "file_types": {"type": "array", "items": {"type": "string"}, "description": "File extensions filter (e.g., ['.py', '.js'])"},
                    "max_results": {"type": "integer", "default": 50, "description": "Max results to return"},
                },
                "required": ["pattern"],
            },
        ),
        Tool(
            name="sast_record_finding",
            description=(
                "Record code-level security finding to world model + cards. "
                "Creates card(type='finding') with SAST metadata: source, file, line, code_snippet, cwe. "
                "Also creates wm_finding. "
                "Use after confirming vulnerability through code review."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "file_path": {"type": "string", "description": "Vulnerable file path (required)"},
                    "line": {"type": "integer", "description": "Line number (required)"},
                    "title": {"type": "string", "description": "Finding title (required)"},
                    "severity": {"type": "string", "enum": ["info", "low", "medium", "high", "critical"], "description": "Severity (required)"},
                    "vuln_class": {"type": "string", "description": "Vulnerability class (e.g., sqli_error, xss_reflected)"},
                    "description": {"type": "string", "description": "Detailed vulnerability description"},
                    "code_snippet": {"type": "string", "description": "Vulnerable code snippet (3-5 lines context)"},
                    "remediation": {"type": "string", "description": "Remediation guidance (optional)"},
                    "confidence": {"type": "number", "default": 0.8, "minimum": 0.0, "maximum": 1.0},
                    "cwe": {"type": "string", "description": "CWE ID (optional, e.g., 'CWE-89')"},
                    "cvss_vector": {"type": "string", "description": "CVSS:3.1 vector string (optional, e.g., CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H)"},
                    "attack_scenario": {"type": "string", "description": "How an attacker could exploit this code vulnerability (optional)"},
                    "affected_endpoints": {"type": "string", "description": "Comma-separated URLs mapping to this code path (optional)"},
                },
                "required": ["file_path", "line", "title", "severity"],
            },
        ),
        Tool(
            name="sast_correlate",
            description=(
                "Cross-reference SAST findings with DAST findings to confirm code issues dynamically. "
                "Correlates by: (a) vuln_class match, (b) endpoint↔file mapping heuristics, "
                "(c) parameter match. Creates wm_relationships(rel_type='confirms'). "
                "Returns correlations, SAST-only count, DAST-only count, confirmed count."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "auto_correlate": {"type": "boolean", "default": True, "description": "Automatically correlate all findings"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_get_progress",
            description=(
                "Show SAST review progress: scanners run, files reviewed, RAG index status, "
                "findings by source/severity, correlations. "
                "Use to verify coverage and track manual review completeness."
            ),
            inputSchema={
                "type": "object",
                "properties": {},
                "required": [],
            },
        ),
        Tool(
            name="sast_get_next_unverified",
            description=(
                "Get next unverified SAST finding with function-level code extract and local file path. "
                "Returns vulnerable function code (not whole file), local file path for Claude Code access, "
                "and investigation guide for tracing execution flow. "
                "Workflow: 1) Receive function code, 2) Use local_file_path/repo_local_root with Read/Grep "
                "to trace data flow, 3) Test dynamically with inject_payload/inject_batch, "
                "4) Call sast_mark_verified with result. "
                "Returns status='all_verified' when no unverified findings remain."
            ),
            inputSchema={
                "type": "object",
                "properties": {},
                "required": [],
            },
        ),
        Tool(
            name="sast_mark_verified",
            description=(
                "Mark SAST finding as verified after dynamic testing. "
                "If exploitable=true: creates wm_finding with severity and remediation. "
                "If exploitable=false and false_positive=false: creates info-level finding "
                "(real but not demonstrable). "
                "If false_positive=true: no finding created, just marks verified. "
                "Includes remediation code snippet for all real findings."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "finding_id": {"type": "string", "description": "wm_knowledge ID from sast_get_next_unverified (required)"},
                    "exploitable": {"type": "boolean", "description": "Was vulnerability exploitable dynamically? (required)"},
                    "false_positive": {"type": "boolean", "default": False, "description": "Is this a false positive? (default: false)"},
                    "severity_override": {"type": "string", "enum": ["info", "low", "medium", "high", "critical"], "description": "Override SAST scanner severity (optional)"},
                    "remediation": {"type": "string", "description": "Remediation code snippet (required for exploitable findings)"},
                    "dynamic_evidence": {"type": "object", "description": "Request/response from testing (optional for exploitable)"},
                    "notes": {"type": "string", "description": "Explanation of testing or why false positive (optional)"},
                },
                "required": ["finding_id", "exploitable"],
            },
        ),
        Tool(
            name="sast_trace_dataflow",
            description=(
                "Trace data flow from user-input sources through sanitization to dangerous sinks. "
                "Reads SAST findings from wm_knowledge (category='sast_scan_result'), runs heuristic "
                "source→sanitization→sink tracing, stores traces in wm_knowledge (category='dataflow_trace'). "
                "Returns trace count and summaries with verdicts (safe/vulnerable/needs_review). "
                "Use before sast_build_exploitation_queue to populate data flow context."
            ),
            inputSchema={
                "type": "object",
                "properties": {},
                "required": [],
            },
        ),
        Tool(
            name="sast_build_exploitation_queue",
            description=(
                "Build prioritized exploitation queue from SAST findings and data flow traces. "
                "Reads sast_scan_result and dataflow_trace from wm_knowledge, creates queue entries "
                "with slot types (injection/auth/privilege/data_exposure/business_logic), witness payloads, "
                "and confidence scores. Stores in wm_knowledge (category='exploitation_queue'). "
                "Returns queue summary: count by slot type, average confidence, top entries. "
                "Use after sast_trace_dataflow. Follow with sast_get_dast_targets to map to endpoints."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "min_confidence": {"type": "number", "default": 0.0, "minimum": 0.0, "maximum": 1.0,
                                       "description": "Minimum confidence threshold for queue entries (default: 0.0 = all)"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_get_dast_targets",
            description=(
                "Map exploitation queue entries to discovered endpoints for DAST testing. "
                "Reads exploitation queue from wm_knowledge and endpoints from world model, "
                "matches code sink paths to URL paths using route heuristics. "
                "Returns prioritized list of targets with endpoint, parameter, payload hint, "
                "confidence, and vuln type. Use to bridge SAST→DAST gap and focus dynamic testing "
                "on code-confirmed vulnerabilities. Pass results to coverage_init(sast_hints=...) "
                "to boost priority of SAST-informed coverage cells."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "min_confidence": {"type": "number", "default": 0.3, "minimum": 0.0, "maximum": 1.0,
                                       "description": "Minimum confidence threshold (default: 0.3)"},
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_sink_hunt",
            description=(
                "Run specialized Semgrep rules focused on dangerous sinks before full SAST analysis. "
                "Scans for: injection sinks (eval/exec/subprocess), SQL sinks (execute/query/raw), "
                "SSRF sinks (requests/urllib/httpx), deserialization sinks (pickle/yaml/marshal). "
                "Returns structured sink map with file, line, sink_type, function, risk_level. "
                "Stores results in wm_knowledge (category='sink_map'). "
                "Use before sast_trace_dataflow for targeted data flow tracing."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "sink_types": {
                        "type": "array",
                        "items": {"type": "string", "enum": ["injection", "sql", "ssrf", "deserialization"]},
                        "description": "Sink types to hunt for (default: all)",
                    },
                },
                "required": [],
            },
        ),
        Tool(
            name="sast_bind_endpoints",
            description=(
                "Bind code-level sinks to discovered endpoints using route pattern matching. "
                "Reads sink map from wm_knowledge and endpoints from world model, "
                "matches route decorators (e.g., @app.route('/api/users')) to endpoint paths. "
                "Updates wm_endpoints with source_file and source_line bindings. "
                "Returns count of bound endpoints. Use after sast_sink_hunt and endpoint discovery."
            ),
            inputSchema={
                "type": "object",
                "properties": {},
                "required": [],
            },
        ),
    ]


# ========== Tool Handlers ==========

async def handle_sast_tool(name: str, arguments: dict, mcp_service) -> List[TextContent]:
    """Handle all SAST tool calls."""

    try:
        if name == "sast_clone_repo":
            return await _handle_sast_clone_repo(arguments, mcp_service)
        elif name == "sast_index_repo":
            return await _handle_sast_index_repo(arguments, mcp_service)
        elif name == "sast_scan_semgrep":
            return await _handle_sast_scan_semgrep(arguments, mcp_service)
        elif name == "sast_scan_bandit":
            return await _handle_sast_scan_bandit(arguments, mcp_service)
        elif name == "sast_scan_gitleaks":
            return await _handle_sast_scan_gitleaks(arguments, mcp_service)
        elif name == "sast_list_files":
            return await _handle_sast_list_files(arguments, mcp_service)
        elif name == "sast_read_file":
            return await _handle_sast_read_file(arguments, mcp_service)
        elif name == "sast_search_code":
            return await _handle_sast_search_code(arguments, mcp_service)
        elif name == "sast_record_finding":
            return await _handle_sast_record_finding(arguments, mcp_service)
        elif name == "sast_correlate":
            return await _handle_sast_correlate(arguments, mcp_service)
        elif name == "sast_get_progress":
            return await _handle_sast_get_progress(arguments, mcp_service)
        elif name == "sast_get_next_unverified":
            return await _handle_sast_get_next_unverified(arguments, mcp_service)
        elif name == "sast_mark_verified":
            return await _handle_sast_mark_verified(arguments, mcp_service)
        elif name == "sast_trace_dataflow":
            return await _handle_sast_trace_dataflow(arguments, mcp_service)
        elif name == "sast_build_exploitation_queue":
            return await _handle_sast_build_exploitation_queue(arguments, mcp_service)
        elif name == "sast_get_dast_targets":
            return await _handle_sast_get_dast_targets(arguments, mcp_service)
        elif name == "sast_sink_hunt":
            return await _handle_sast_sink_hunt(arguments, mcp_service)
        elif name == "sast_bind_endpoints":
            return await _handle_sast_bind_endpoints(arguments, mcp_service)
        else:
            return _error_content(f"Unknown SAST tool: {name}")
    except Exception as e:
        import traceback
        return _error_content(f"Error in {name}: {str(e)}\n{traceback.format_exc()}")


# ========== Handler Implementations ==========

async def _handle_sast_clone_repo(args: dict, mcp_service) -> List[TextContent]:
    """Clone git repository into assessment-scoped directory."""
    repo_url = args.get("repo_url")

    # If repo_url not provided, try to read from assessment
    if not repo_url:
        repo_url = await _get_git_repo_url(mcp_service)
        if not repo_url:
            return _error_content("No repo_url provided and no git_repo_url set in assessment")

    branch = args.get("branch", "main")
    depth = args.get("depth", 1)
    auth_token_env = args.get("auth_token_env")

    runner = get_sast_runner()
    result = await runner.clone_repo(
        url=repo_url,
        assessment_id=mcp_service.current_assessment_id,
        branch=branch,
        depth=depth,
        auth_token_env=auth_token_env,
    )

    return _json_content(result)


async def _handle_sast_index_repo(args: dict, mcp_service) -> List[TextContent]:
    """Index source code into wm_knowledge for RAG."""
    languages = args.get("languages")
    exclude_patterns = args.get("exclude_patterns")

    # Get repo path from previous clone
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"

    # Find repo directory (should be only one)
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    # First line is repo_dir itself, second is the actual repo
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]  # Take first subdirectory

    # Get database
    db = await _get_db(mcp_service)
    indexer = get_code_indexer(db)

    # Index repository
    result = await indexer.index_repo(
        repo_path=repo_path,
        sast_runner=runner,
        languages=languages,
        exclude_patterns=exclude_patterns,
    )

    return _json_content(result)


async def _handle_sast_scan_semgrep(args: dict, mcp_service) -> List[TextContent]:
    """Run Semgrep scanner."""
    rulesets = args.get("rulesets", ["auto"])
    paths = args.get("paths")
    exclude_paths = args.get("exclude_paths")
    severity_filter = args.get("severity_filter")

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]

    # Run Semgrep
    result = await runner.run_semgrep(
        repo_path=repo_path,
        rulesets=rulesets,
        paths=paths,
        exclude=exclude_paths,
        severity=severity_filter,
    )

    if not result["success"]:
        return _json_content(result)

    findings = result["findings"]
    result["findings_count"] = len(findings)

    # Store ALL findings in wm_knowledge for verification workflow
    if findings:
        db = await _get_db(mcp_service)
        for finding in findings:
            try:
                # Store in knowledge base for sast_get_next_unverified
                await db.store_knowledge(
                    title=f"semgrep:{finding.get('rule_id', 'unknown')} in {finding.get('file', 'unknown')}:{finding.get('line', 0)}",
                    content=json.dumps(finding),
                    category="sast_scan_result",
                    source_tool="sast_semgrep",
                    metadata={
                        "verified": False,
                        "vuln_class": finding.get("vuln_class", "misconfig"),
                        "severity": finding.get("severity", "medium"),
                        "file": finding.get("file", ""),
                        "line": finding.get("line", 0),
                        "rule_id": finding.get("rule_id", ""),
                        "remediation": finding.get("remediation", ""),
                    },
                    tags=[finding.get("language", "unknown"), finding.get("vuln_class", "misconfig")],
                )
            except Exception:
                pass  # Continue on error

    return _json_content(result)


async def _handle_sast_scan_bandit(args: dict, mcp_service) -> List[TextContent]:
    """Run Bandit (Python-only) scanner."""
    confidence = args.get("confidence", "MEDIUM")
    severity = args.get("severity", "LOW")
    exclude_dirs = args.get("exclude_dirs")

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]

    # Check for Python files
    py_check = await runner._exec("find", [repo_path, "-name", "*.py", "-type", "f"], timeout=30)
    if not py_check["stdout"].strip():
        return _json_content({
            "success": True,
            "findings": [],
            "scan_time_ms": 0,
            "message": "No Python files found in repository. Bandit is Python-only.",
        })

    # Run Bandit
    result = await runner.run_bandit(
        repo_path=repo_path,
        confidence=confidence,
        severity=severity,
        exclude_dirs=exclude_dirs,
    )

    # Store findings in wm_knowledge for verification workflow
    if result.get("success") and result.get("findings"):
        db = await _get_db(mcp_service)
        for finding in result["findings"]:
            try:
                await db.store_knowledge(
                    title=f"bandit:{finding.get('test_id', 'unknown')} in {finding.get('file', 'unknown')}:{finding.get('line', 0)}",
                    content=json.dumps(finding),
                    category="sast_scan_result",
                    source_tool="sast_bandit",
                    metadata={
                        "verified": False,
                        "vuln_class": finding.get("vuln_class", "misconfig"),
                        "severity": finding.get("severity", "medium"),
                        "file": finding.get("file", ""),
                        "line": finding.get("line", 0),
                        "rule_id": finding.get("test_id", ""),
                        "remediation": finding.get("remediation", ""),
                    },
                    tags=["python", finding.get("vuln_class", "misconfig")],
                )
            except Exception:
                pass  # Continue on error

    return _json_content(result)


async def _handle_sast_scan_gitleaks(args: dict, mcp_service) -> List[TextContent]:
    """Run Gitleaks secret scanner."""
    scan_history = args.get("scan_history", False)

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]

    # Run Gitleaks
    result = await runner.run_gitleaks(
        repo_path=repo_path,
        scan_history=scan_history,
    )

    if not result["success"]:
        return _json_content(result)

    findings = result["findings"]
    result["findings_count"] = len(findings)

    # Store findings in wm_knowledge for verification workflow
    if findings:
        db = await _get_db(mcp_service)
        for finding in findings:
            try:
                await db.store_knowledge(
                    title=f"gitleaks:{finding.get('rule_id', 'unknown')} in {finding.get('file', 'unknown')}:{finding.get('line', 0)}",
                    content=json.dumps(finding),
                    category="sast_scan_result",
                    source_tool="sast_gitleaks",
                    metadata={
                        "verified": False,
                        "vuln_class": "info_disclosure",  # Gitleaks findings are secret leaks
                        "severity": "high",  # Secret leaks are always high severity
                        "file": finding.get("file", ""),
                        "line": finding.get("line", 0),
                        "rule_id": finding.get("rule_id", ""),
                        "remediation": "Remove hardcoded secrets, use environment variables or secret management system",
                    },
                    tags=["secrets", "info_disclosure"],
                )
            except Exception:
                pass  # Continue on error

    return _json_content(result)


async def _handle_sast_list_files(args: dict, mcp_service) -> List[TextContent]:
    """List source files with priority scoring."""
    language = args.get("language")
    pattern = args.get("pattern")
    sort_by = args.get("sort_by", "priority")
    limit = args.get("limit", 100)

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]

    # Build find command
    find_args = [repo_path, "-type", "f"]
    if pattern:
        find_args.extend(["-name", pattern])

    # Execute find
    find_result = await runner._exec("find", find_args, timeout=60)
    if find_result["exit_code"] != 0:
        return _error_content("Failed to list files")

    all_files = find_result["stdout"].splitlines()

    # Filter by language if specified
    if language:
        lang_exts = {
            "python": [".py"],
            "javascript": [".js", ".jsx"],
            "typescript": [".ts", ".tsx"],
            "java": [".java"],
            "go": [".go"],
            "ruby": [".rb"],
            "php": [".php"],
        }
        exts = lang_exts.get(language.lower(), [])
        all_files = [f for f in all_files if any(f.endswith(ext) for ext in exts)]

    # Build file list with metadata
    files = []
    languages_detected = set()

    for file_path in all_files:
        # Make path relative
        rel_path = file_path[len(repo_path):].lstrip("/") if file_path.startswith(repo_path) else file_path

        # Detect language
        lang = _detect_file_language(rel_path)
        if lang:
            languages_detected.add(lang)

        # Count lines
        wc_result = await runner._exec("wc", ["-l", file_path], timeout=5)
        line_count = 0
        if wc_result["exit_code"] == 0:
            match = re.search(r"^(\d+)", wc_result["stdout"])
            if match:
                line_count = int(match.group(1))

        # Get file size
        stat_result = await runner._exec("stat", ["-c", "%s", file_path], timeout=5)
        size_bytes = 0
        if stat_result["exit_code"] == 0:
            try:
                size_bytes = int(stat_result["stdout"].strip())
            except ValueError:
                pass

        # Calculate priority score
        priority_score, priority_reasons = _calculate_file_priority(rel_path)

        files.append({
            "path": rel_path,
            "language": lang or "unknown",
            "size_bytes": size_bytes,
            "line_count": line_count,
            "priority_score": priority_score,
            "priority_reasons": priority_reasons,
        })

    # Sort
    if sort_by == "priority":
        files.sort(key=lambda f: f["priority_score"], reverse=True)
    elif sort_by == "size":
        files.sort(key=lambda f: f["size_bytes"], reverse=True)
    elif sort_by == "name":
        files.sort(key=lambda f: f["path"])

    # Limit
    files = files[:limit]

    result = {
        "files": files,
        "total_files": len(all_files),
        "languages": sorted(list(languages_detected)),
    }

    return _json_content(result)


async def _handle_sast_read_file(args: dict, mcp_service) -> List[TextContent]:
    """Read source file for LLM review."""
    file_path = args["file_path"]
    start_line = args.get("start_line")
    end_line = args.get("end_line")
    max_lines = args.get("max_lines", 500)

    # Validate no path traversal
    if ".." in file_path:
        return _error_content("Path traversal not allowed (contains '..')")

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]
    full_path = f"{repo_path}/{file_path}"

    # Read file
    cat_result = await runner._exec("cat", [full_path], timeout=30)
    if cat_result["exit_code"] != 0:
        return _error_content(f"Failed to read file: {cat_result['stderr']}")

    content = cat_result["stdout"]
    lines = content.splitlines()
    total_lines = len(lines)

    # Apply line range
    if start_line is not None:
        start_idx = max(0, start_line - 1)  # 1-indexed -> 0-indexed
    else:
        start_idx = 0

    if end_line is not None:
        end_idx = min(total_lines, end_line)
    else:
        end_idx = min(total_lines, start_idx + max_lines)

    # Truncate if needed
    if (end_idx - start_idx) > max_lines:
        end_idx = start_idx + max_lines

    selected_lines = lines[start_idx:end_idx]
    selected_content = "\n".join(selected_lines)

    language = _detect_file_language(file_path)

    result = {
        "file_path": file_path,
        "language": language or "unknown",
        "content": selected_content,
        "start_line": start_idx + 1,
        "end_line": end_idx,
        "total_lines": total_lines,
        "truncated": end_idx < total_lines,
    }

    return _json_content(result)


async def _handle_sast_search_code(args: dict, mcp_service) -> List[TextContent]:
    """Regex search across codebase."""
    pattern = args["pattern"]
    file_types = args.get("file_types", [])
    max_results = args.get("max_results", 50)

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    dirs = find_result["stdout"].strip().splitlines()
    if len(dirs) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = dirs[1]

    # Build grep command
    grep_args = ["-rn", pattern, repo_path]
    if file_types:
        for ft in file_types:
            grep_args.extend(["--include", f"*{ft}"])

    # Execute grep
    grep_result = await runner._exec("grep", grep_args, timeout=60)

    # grep exit code 1 = no matches (not an error)
    if grep_result["exit_code"] not in [0, 1]:
        return _error_content("Grep search failed")

    if not grep_result["stdout"].strip():
        return _json_content({"matches": [], "total_matches": 0})

    # Parse results
    matches = []
    for line in grep_result["stdout"].splitlines()[:max_results]:
        # Format: /path/to/file:123:matched content
        parts = line.split(":", 2)
        if len(parts) >= 3:
            file_path = parts[0]
            # Make relative
            rel_path = file_path[len(repo_path):].lstrip("/") if file_path.startswith(repo_path) else file_path

            try:
                line_num = int(parts[1])
            except ValueError:
                continue

            content = parts[2]

            matches.append({
                "file": rel_path,
                "line": line_num,
                "content": content.strip(),
            })

    result = {
        "matches": matches,
        "total_matches": len(matches),
    }

    return _json_content(result)


async def _handle_sast_record_finding(args: dict, mcp_service) -> List[TextContent]:
    """Record code-level finding."""
    file_path = args["file_path"]
    line = args["line"]
    title = args["title"]
    severity = args["severity"]
    vuln_class = args.get("vuln_class")
    description = args.get("description", "")
    code_snippet = args.get("code_snippet", "")
    remediation = args.get("remediation")
    confidence = args.get("confidence", 0.8)
    cwe = args.get("cwe")
    cvss_vector = args.get("cvss_vector")
    attack_scenario = args.get("attack_scenario")
    affected_endpoints = args.get("affected_endpoints")

    # Auto-compute cvss_score from cvss_vector if provided
    cvss_score = None
    if cvss_vector:
        try:
            from lib.risk_engine import CVSSv31
            cvss_score = CVSSv31(cvss_vector).base_score()
        except Exception:
            pass

    # Build card description
    description_parts = []

    if vuln_class:
        description_parts.append(f"**Vulnerability Class:** {vuln_class}")

    description_parts.append(f"**File:** `{file_path}:{line}`")

    if cwe:
        description_parts.append(f"**CWE:** {cwe}")

    if code_snippet:
        description_parts.append(f"\n**Code:**\n```\n{code_snippet}\n```")

    if description:
        description_parts.append(f"\n{description}")

    if remediation:
        description_parts.append(f"\n**Remediation:**\n{remediation}")

    card_description = "\n\n".join(description_parts)

    # Create card via safe_add_card
    result = await mcp_service.safe_add_card(
        card_type="finding",
        title=title,
        description=card_description,
        severity=severity.upper() if isinstance(severity, str) else severity,
        status="confirmed",
        target_service=file_path,
        context=f"line={line}, vuln_class={vuln_class}" if vuln_class else f"line={line}",
        confidence=confidence,
        cvss_vector=cvss_vector,
        cvss_score=cvss_score,
        attack_scenario=attack_scenario,
        affected_endpoints=affected_endpoints,
        recommendation=remediation,
        evidence=json.dumps({
            "source": "sast_llm_review",
            "file": file_path,
            "line": line,
            "code_snippet": code_snippet,
            "cwe": cwe,
        }),
    )

    if not result.ok:
        return _error_content(f"Failed to create card: {result.reason}")

    card_id = result.data.get("id") if result.data else None

    response = {
        "finding_id": str(card_id) if card_id else None,
        "card_id": str(card_id) if card_id else None,
        "success": True,
    }

    return _json_content(response)


async def _handle_sast_correlate(args: dict, mcp_service) -> List[TextContent]:
    """Cross-reference SAST↔DAST findings."""
    auto_correlate = args.get("auto_correlate", True)

    db = await _get_db(mcp_service)

    # SAST from wm_knowledge (verified findings only), DAST from wm_findings
    sast_results = await db.query("knowledge", filters={"category": "sast_scan_result"}, limit=500)
    sast_findings = [r for r in sast_results if r.get("metadata", {}).get("verified", False)
                     and r.get("metadata", {}).get("exploitable", False)]
    dast_findings = await db.query("findings")

    correlations = []

    if auto_correlate:
        # Correlate by vuln_class
        for sast_f in sast_findings:
            sast_class = sast_f.get("metadata", {}).get("vuln_class")
            if not sast_class:
                continue

            for dast_f in dast_findings:
                dast_meta = dast_f.get("metadata", {}) or {}
                dast_class = dast_meta.get("vuln_type") or dast_meta.get("vuln_class")
                if dast_class == sast_class:
                    correlations.append({
                        "sast_finding": sast_f.get("id"),
                        "dast_finding": dast_f.get("id"),
                        "vuln_class": sast_class,
                        "confidence": 0.7,
                        "correlation_type": "vuln_class_match",
                    })

                    # Create relationship
                    try:
                        await db.add_relationship(
                            source_type="finding",
                            source_id=str(dast_f.get("id")),
                            target_type="finding",
                            target_id=str(sast_f.get("id")),
                            rel_type="confirms",
                            metadata={"correlation": "SAST↔DAST", "vuln_class": sast_class},
                        )
                    except Exception:
                        pass

    result = {
        "correlations": correlations,
        "total_sast_findings": len(sast_results),
        "verified_sast_findings": len(sast_findings),
        "sast_only_count": len(sast_findings) - len(correlations),
        "dast_only_count": len(dast_findings) - len(correlations),
        "confirmed_count": len(correlations),
    }

    return _json_content(result)


async def _handle_sast_get_progress(args: dict, mcp_service) -> List[TextContent]:
    """Show SAST review progress."""
    db = await _get_db(mcp_service)

    # Get repo info
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)

    repo_path = None
    if find_result["exit_code"] == 0:
        dirs = find_result["stdout"].strip().splitlines()
        if len(dirs) >= 2:
            repo_path = dirs[1]

    # Query wm_knowledge where SAST scan results actually live
    sast_results = await db.query("knowledge", filters={"category": "sast_scan_result"}, limit=500)

    # Count by source_tool (stored in metadata)
    findings_by_source = {}
    for r in sast_results:
        source = r.get("metadata", {}).get("source_tool", "unknown")
        findings_by_source[source] = findings_by_source.get(source, 0) + 1

    # Count by severity
    findings_by_severity = {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0}
    for r in sast_results:
        sev = r.get("metadata", {}).get("severity", "info").lower()
        if sev in findings_by_severity:
            findings_by_severity[sev] += 1

    # Count verified vs unverified
    verified_count = len([r for r in sast_results if r.get("metadata", {}).get("verified", False)])
    unverified_count = len(sast_results) - verified_count

    # Check RAG index
    knowledge_entries = await db.query("knowledge", filters={"category": "source_code"})
    files_indexed = len(knowledge_entries)

    # Check correlations
    relationships = await db.query("relationships", filters={"rel_type": "confirms"})
    correlations_count = len([r for r in relationships if "SAST↔DAST" in r.get("metadata", {}).get("correlation", "")])

    result = {
        "repo": repo_path or "Not cloned",
        "scanners": {
            "semgrep": findings_by_source.get("sast_semgrep", 0),
            "bandit": findings_by_source.get("sast_bandit", 0),
            "gitleaks": findings_by_source.get("sast_gitleaks", 0),
            "llm_review": findings_by_source.get("sast_llm_review", 0),
        },
        "rag_index": {
            "files_indexed": files_indexed,
            "searchable": files_indexed > 0,
        },
        "findings": {
            "total": len(sast_results),
            "verified": verified_count,
            "unverified": unverified_count,
            "by_source": findings_by_source,
            "by_severity": findings_by_severity,
        },
        "correlations": {
            "confirmed": correlations_count,
        },
    }

    return _json_content(result)


async def _handle_sast_get_next_unverified(args: dict, mcp_service) -> List[TextContent]:
    """Get next unverified SAST finding with function-level code extract."""
    db = await _get_db(mcp_service)

    # Query for unverified SAST scan results
    sast_results = await db.query("knowledge", filters={"category": "sast_scan_result"}, limit=500)
    unverified = [r for r in sast_results if not r.get("metadata", {}).get("verified", False)]

    if not unverified:
        # All findings verified
        verified_count = len([r for r in sast_results if r.get("metadata", {}).get("verified", False)])
        return _json_content({
            "status": "all_verified",
            "total_verified": verified_count,
            "message": "All SAST scan results have been verified",
        })

    # Get first unverified finding
    finding_entry = unverified[0]
    finding_data = json.loads(finding_entry.get("content", "{}"))
    metadata = finding_entry.get("metadata", {})

    # Extract details
    file_path = metadata.get("file", "")
    line_number = metadata.get("line", 0)
    vuln_class = metadata.get("vuln_class", "unknown")
    severity = metadata.get("severity", "medium")
    rule_id = metadata.get("rule_id", "unknown")
    remediation_hint = metadata.get("remediation", "")

    # Read source file
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)

    if find_result["exit_code"] != 0 or len(find_result["stdout"].strip().splitlines()) < 2:
        return _error_content("No cloned repository found. Cannot extract function code.")

    repo_path = find_result["stdout"].strip().splitlines()[1]
    repo_name = repo_path.split("/")[-1]
    full_file_path = f"{repo_path}/{file_path}"

    # Read file content
    cat_result = await runner._exec("cat", [full_file_path], timeout=30)
    if cat_result["exit_code"] != 0:
        return _error_content(f"Failed to read file: {file_path}")

    content = cat_result["stdout"]

    # Extract function containing the vulnerable line
    indexer = get_code_indexer(db)
    language = indexer._detect_language(file_path)
    function_extract = indexer.extract_function_at_line(content, language, line_number)

    # Compute local file paths for Claude Code access
    local_file_path = f"{mcp_service.sast_repos_dir}/{mcp_service.current_assessment_id}/{repo_name}/{file_path}"
    repo_local_root = f"{mcp_service.sast_repos_dir}/{mcp_service.current_assessment_id}/{repo_name}"

    # Generate investigation guide
    investigation_guide = (
        "1. Search for callers: Use Grep tool with function_name across repo_local_root\n"
        "2. Trace input: Find HTTP handler/route that calls this function (search for function_name in route files)\n"
        "3. Check sanitization: Look for input validation, middleware, WAF rules between input and this function\n"
        "4. Look for related code: Use Read tool on files in repo_local_root to understand context\n"
        "5. Test dynamically: Use inject_payload/inject_batch tools to test actual exploitation"
    )

    # Get title from finding data
    title = finding_data.get("title") or finding_data.get("message") or f"{rule_id} in {file_path}:{line_number}"

    return _json_content({
        "finding_id": str(finding_entry.get("id")),
        "title": title,
        "severity": severity,
        "vuln_class": vuln_class,
        "file": file_path,
        "line": line_number,
        "function_name": function_extract.get("function_name"),
        "function_code": function_extract.get("code"),
        "local_file_path": local_file_path,
        "repo_local_root": repo_local_root,
        "remediation_hint": remediation_hint,
        "investigation_guide": investigation_guide,
        "total_remaining": len(unverified),
    })


async def _handle_sast_mark_verified(args: dict, mcp_service) -> List[TextContent]:
    """Mark SAST finding as verified and record result."""
    finding_id = args["finding_id"]
    exploitable = args["exploitable"]
    false_positive = args.get("false_positive", False)
    severity_override = args.get("severity_override")
    remediation = args.get("remediation", "")
    dynamic_evidence = args.get("dynamic_evidence")
    notes = args.get("notes", "")

    db = await _get_db(mcp_service)

    # Get finding entry from knowledge
    try:
        knowledge_entries = await db.query("knowledge", filters={"id": finding_id}, limit=1)
        if not knowledge_entries:
            return _error_content(f"Finding {finding_id} not found in knowledge base")

        finding_entry = knowledge_entries[0]
        finding_data = json.loads(finding_entry.get("content", "{}"))
        metadata = finding_entry.get("metadata", {})
    except Exception as e:
        return _error_content(f"Failed to load finding: {str(e)}")

    # Update knowledge entry metadata
    await db.update_knowledge(
        knowledge_id=finding_id,
        metadata={"verified": True, "exploitable": exploitable, "false_positive": false_positive},
    )

    # Handle different verification outcomes
    if false_positive:
        # No finding created, just mark verified with notes
        return _json_content({
            "success": True,
            "action": "marked_false_positive",
            "finding_id": finding_id,
            "notes": notes,
        })

    # Determine final severity
    final_severity = severity_override or metadata.get("severity", "medium")
    if not exploitable:
        final_severity = "info"  # Real but not demonstrable -> info

    # Build description
    description_parts = []
    description_parts.append(finding_data.get("message", "SAST finding from code analysis"))

    if not exploitable:
        description_parts.append("\n\n**Note:** This vulnerability was confirmed in source code but could not be demonstrated dynamically through testing.")

    if dynamic_evidence:
        description_parts.append("\n\n**Dynamic Testing Evidence:**")
        description_parts.append(f"```\n{json.dumps(dynamic_evidence, indent=2)}\n```")

    if notes:
        description_parts.append(f"\n\n**Testing Notes:**\n{notes}")

    description = "\n".join(description_parts)

    # Create finding via sast_record_finding logic
    file_path = metadata.get("file", "")
    line = metadata.get("line", 0)
    vuln_class = metadata.get("vuln_class", "misconfig")
    title = finding_data.get("title") or finding_data.get("message") or f"SAST: {vuln_class} in {file_path}"

    # Build card description
    card_description_parts = []
    if vuln_class:
        card_description_parts.append(f"**Vulnerability Class:** {vuln_class}")
    card_description_parts.append(f"**File:** `{file_path}:{line}`")
    if finding_data.get("cwe"):
        card_description_parts.append(f"**CWE:** {finding_data['cwe']}")
    if finding_data.get("code_snippet"):
        card_description_parts.append(f"\n**Code:**\n```\n{finding_data['code_snippet']}\n```")
    card_description_parts.append(f"\n{description}")
    if remediation:
        card_description_parts.append(f"\n**Remediation:**\n{remediation}")

    card_description = "\n\n".join(card_description_parts)

    # Create card
    result = await mcp_service.safe_add_card(
        card_type="finding",
        title=title,
        description=card_description,
        severity=final_severity.upper(),
        status="confirmed",
        target_service=file_path,
        context=f"line={line}, vuln_class={vuln_class}",
        confidence=0.8 if exploitable else 0.6,
        evidence=json.dumps({
            "source": "sast_verification",
            "file": file_path,
            "line": line,
            "code_snippet": finding_data.get("code_snippet"),
            "cwe": finding_data.get("cwe"),
            "dynamic_evidence": dynamic_evidence,
        }),
    )

    if not result.ok:
        return _error_content(f"Failed to create card: {result.reason}")

    card_id = result.data.get("id") if result.data else None

    return _json_content({
        "success": True,
        "action": "finding_recorded" if exploitable else "informational_finding_recorded",
        "finding_id": str(card_id) if card_id else None,
        "card_id": str(card_id) if card_id else None,
        "severity": final_severity,
        "exploitable": exploitable,
    })


# ========== Dataflow / Exploitation Queue / DAST Targets ==========

async def _handle_sast_trace_dataflow(args: dict, mcp_service) -> List[TextContent]:
    """Trace data flow from sources through sanitization to sinks."""
    db = await _get_db(mcp_service)

    # Read SAST findings from wm_knowledge
    sast_results = await db.query("knowledge", filters={"category": "sast_scan_result"}, limit=500)

    if not sast_results:
        return _json_content({
            "success": True,
            "traces": 0,
            "message": "No SAST findings found. Run sast_scan_semgrep or sast_scan_bandit first.",
        })

    # Run dataflow tracing
    from lib.dataflow_tracer import trace_dataflow
    traces = trace_dataflow(sast_results)

    # Store traces in wm_knowledge
    stored = 0
    for trace in traces:
        trace_dict = trace.to_dict()
        try:
            await db.store_knowledge(
                title=f"dataflow:{trace_dict.get('verdict', 'unknown')}:{trace_dict.get('trace_id', '')[:8]}",
                content=json.dumps(trace_dict),
                category="dataflow_trace",
                source_tool="sast_trace_dataflow",
                metadata={
                    "verdict": trace_dict.get("verdict", "needs_review"),
                    "confidence": trace_dict.get("confidence", 0.5),
                    "source_file": trace_dict.get("source", {}).get("file", "") if trace_dict.get("source") else "",
                    "sink_function": trace_dict.get("sink", {}).get("function", "") if trace_dict.get("sink") else "",
                },
                tags=["dataflow", trace_dict.get("verdict", "unknown")],
            )
            stored += 1
        except Exception:
            pass

    # Build summary
    by_verdict = {}
    for t in traces:
        v = t.verdict.value
        by_verdict[v] = by_verdict.get(v, 0) + 1

    return _json_content({
        "success": True,
        "traces": len(traces),
        "stored": stored,
        "by_verdict": by_verdict,
        "avg_confidence": round(sum(t.confidence for t in traces) / len(traces), 2) if traces else 0.0,
        "message": f"Traced {len(traces)} data flows from {len(sast_results)} SAST findings.",
    })


async def _handle_sast_build_exploitation_queue(args: dict, mcp_service) -> List[TextContent]:
    """Build exploitation queue from SAST findings and data flow traces."""
    min_confidence = args.get("min_confidence", 0.0)

    db = await _get_db(mcp_service)

    # Read SAST findings
    sast_results = await db.query("knowledge", filters={"category": "sast_scan_result"}, limit=500)

    # Read dataflow traces
    dataflow_results = await db.query("knowledge", filters={"category": "dataflow_trace"}, limit=500)

    if not sast_results:
        return _json_content({
            "success": True,
            "queue_size": 0,
            "message": "No SAST findings found. Run sast_scan_semgrep first, then sast_trace_dataflow.",
        })

    # Build exploitation queue
    from lib.exploitation_queue import build_queue_from_sast
    queue = build_queue_from_sast(sast_results, dataflow_results)

    # Apply confidence filter
    if min_confidence > 0.0:
        queue.entries = queue.filter_by_confidence(min_confidence)

    queue_dict = queue.to_dict()

    # Store queue in wm_knowledge
    try:
        await db.store_knowledge(
            title=f"exploitation_queue:{len(queue.entries)}_entries",
            content=json.dumps(queue_dict),
            category="exploitation_queue",
            source_tool="sast_build_exploitation_queue",
            metadata={
                "total_entries": queue_dict["total_entries"],
                "avg_confidence": queue_dict["avg_confidence"],
                "by_slot_type": queue_dict["by_slot_type"],
            },
            tags=["exploitation", "queue"],
        )
    except Exception:
        pass

    # Top entries for response
    top_entries = [e.to_dict() for e in queue.entries[:10]]

    return _json_content({
        "success": True,
        "queue_size": len(queue.entries),
        "by_slot_type": queue_dict["by_slot_type"],
        "avg_confidence": queue_dict["avg_confidence"],
        "top_entries": top_entries,
        "message": f"Built exploitation queue with {len(queue.entries)} entries from {len(sast_results)} SAST findings.",
    })


async def _handle_sast_get_dast_targets(args: dict, mcp_service) -> List[TextContent]:
    """Map exploitation queue entries to discovered endpoints for DAST testing."""
    min_confidence = args.get("min_confidence", 0.3)

    db = await _get_db(mcp_service)

    # Read exploitation queue entries
    queue_results = await db.query("knowledge", filters={"category": "exploitation_queue"}, limit=10)

    if not queue_results:
        return _json_content({
            "success": True,
            "targets": [],
            "message": "No exploitation queue found. Run sast_build_exploitation_queue first.",
        })

    # Parse queue entries from latest queue
    latest_queue = queue_results[0]
    queue_data = json.loads(latest_queue.get("content", "{}"))
    entries = queue_data.get("entries", [])

    # Read discovered endpoints
    endpoints = await db.query(table="endpoints", limit=1000)

    if not endpoints:
        return _json_content({
            "success": True,
            "targets": [],
            "queue_entries": len(entries),
            "message": "No endpoints in world model. Run crawler_start or openapi_parse first.",
        })

    # Match queue entries to endpoints
    targets = []
    for entry in entries:
        if entry.get("confidence", 0) < min_confidence:
            continue

        # Extract path hints from source (file:line format)
        source = entry.get("source", "")
        sink = entry.get("sink", "")

        # Try to match to endpoints using path similarity
        matched_endpoints = _match_entry_to_endpoints(entry, endpoints)

        for ep in matched_endpoints:
            targets.append({
                "endpoint_id": ep.get("id"),
                "url": ep.get("path", ep.get("url", "")),
                "method": ep.get("method", "GET"),
                "parameter": _guess_parameter(entry, ep),
                "payload_hint": entry.get("witness_payload", ""),
                "vuln_type": entry.get("vuln_type", ""),
                "slot_type": entry.get("slot_type", ""),
                "confidence": entry.get("confidence", 0.5),
                "source_file": source,
                "sink_function": sink,
            })

    # Sort by confidence
    targets.sort(key=lambda t: t["confidence"], reverse=True)

    return _json_content({
        "success": True,
        "targets": targets[:50],
        "total_matched": len(targets),
        "queue_entries": len(entries),
        "endpoints_checked": len(endpoints),
        "message": f"Matched {len(targets)} DAST targets from {len(entries)} queue entries across {len(endpoints)} endpoints.",
    })


def _match_entry_to_endpoints(entry: dict, endpoints: list) -> list:
    """Match an exploitation queue entry to endpoints using heuristics."""
    matched = []
    vuln_type = entry.get("vuln_type", "").lower()
    source = entry.get("source", "").lower()
    sink = entry.get("sink", "").lower()

    for ep in endpoints:
        ep_path = (ep.get("path") or ep.get("url") or "").lower()
        ep_method = (ep.get("method") or "GET").upper()

        # Heuristic 1: Route path segments match file path segments
        # e.g., source "routes/users.py:45" matches endpoint "/api/users"
        source_parts = set()
        for part in re.split(r'[/\\.:_\-]', source):
            if len(part) > 2 and part not in ("py", "js", "ts", "rb", "go", "php", "java"):
                source_parts.add(part)

        path_parts = set()
        for part in re.split(r'[/\\.:_\-]', ep_path):
            if len(part) > 2:
                path_parts.add(part)

        overlap = source_parts & path_parts
        if overlap:
            matched.append(ep)
            continue

        # Heuristic 2: Sink function name matches route handler
        if sink and any(sink_part in ep_path for sink_part in re.split(r'[/\\.:_\-]', sink) if len(sink_part) > 2):
            matched.append(ep)
            continue

        # Heuristic 3: Vuln type + endpoint characteristics
        if vuln_type in ("sqli", "sql_injection") and ep_method in ("POST", "PUT", "PATCH"):
            params = ep.get("parameters") or []
            if params:
                matched.append(ep)
                continue
        elif vuln_type == "xss" and ep_method == "GET":
            params = ep.get("parameters") or []
            if params:
                matched.append(ep)
                continue

    return matched


def _guess_parameter(entry: dict, endpoint: dict) -> str:
    """Guess which parameter to target based on entry and endpoint."""
    params = endpoint.get("parameters") or []
    if not params:
        return ""

    vuln_type = entry.get("vuln_type", "").lower()

    # Preference ordering for different vuln types
    preferences = {
        "sqli": ["id", "user_id", "name", "search", "query", "filter", "sort"],
        "sql_injection": ["id", "user_id", "name", "search", "query", "filter", "sort"],
        "xss": ["name", "search", "query", "comment", "message", "input", "q"],
        "command_injection": ["cmd", "command", "exec", "file", "path", "filename"],
        "path_traversal": ["file", "path", "filename", "dir", "template", "page"],
        "ssrf": ["url", "uri", "link", "redirect", "callback", "webhook"],
    }

    preferred = preferences.get(vuln_type, [])

    for pref in preferred:
        for param in params:
            param_name = param.get("name", "") if isinstance(param, dict) else str(param)
            if pref in param_name.lower():
                return param_name

    # Return first parameter as fallback
    if params:
        first = params[0]
        return first.get("name", str(first)) if isinstance(first, dict) else str(first)

    return ""


# ========== Sink Hunt + Endpoint Binding ==========

# Sink type -> rule file mapping
_SINK_RULE_FILES = {
    "injection": "injection_sinks.yaml",
    "sql": "sql_sinks.yaml",
    "ssrf": "ssrf_sinks.yaml",
    "deserialization": "deserialization_sinks.yaml",
}

# Built-in sink patterns for heuristic mode (when semgrep not available)
_BUILTIN_SINK_PATTERNS = {
    "injection": [
        (r'\beval\s*\(', "eval", "critical"),
        (r'\bexec\s*\(', "exec", "critical"),
        (r'\bos\.system\s*\(', "os.system", "critical"),
        (r'\bos\.popen\s*\(', "os.popen", "critical"),
        (r'\bsubprocess\.\w+\s*\(', "subprocess", "critical"),
        (r'\bchild_process\.exec', "child_process.exec", "critical"),
    ],
    "sql": [
        (r'\bcursor\.execute\s*\(', "cursor.execute", "high"),
        (r'\.execute\s*\(', "execute", "high"),
        (r'\.query\s*\(', "query", "high"),
        (r'\.raw\s*\(', "raw", "high"),
        (r'\bknex\.raw\s*\(', "knex.raw", "high"),
    ],
    "ssrf": [
        (r'\brequests\.\w+\s*\(', "requests", "high"),
        (r'\burllib\.request\.urlopen\s*\(', "urllib.urlopen", "high"),
        (r'\bhttpx\.\w+\s*\(', "httpx", "high"),
        (r'\bfetch\s*\(', "fetch", "medium"),
        (r'\baxios\.\w+\s*\(', "axios", "high"),
    ],
    "deserialization": [
        (r'\bpickle\.loads?\s*\(', "pickle.load", "critical"),
        (r'\byaml\.load\s*\(', "yaml.load", "critical"),
        (r'\bmarshal\.loads?\s*\(', "marshal.load", "critical"),
        (r'\bshelve\.open\s*\(', "shelve.open", "high"),
    ],
}


async def _handle_sast_sink_hunt(args: dict, mcp_service) -> List[TextContent]:
    """Hunt for dangerous sinks using Semgrep rules or heuristic fallback."""
    sink_types = args.get("sink_types") or list(_SINK_RULE_FILES.keys())

    db = await _get_db(mcp_service)

    # Get repo path
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0 or len(find_result["stdout"].strip().splitlines()) < 2:
        return _error_content("No cloned repository found. Call sast_clone_repo first.")

    repo_path = find_result["stdout"].strip().splitlines()[1]

    # Try Semgrep-based sink hunt first
    sinks = []
    semgrep_available = await _check_semgrep_available(runner)

    if semgrep_available:
        rules_dir = os.path.join(os.path.dirname(__file__), "lib", "semgrep_rules")
        for sink_type in sink_types:
            rule_file = _SINK_RULE_FILES.get(sink_type)
            if not rule_file:
                continue
            rule_path = os.path.join(rules_dir, rule_file)
            if not os.path.exists(rule_path):
                continue

            result = await runner.run_semgrep(
                repo_path=repo_path,
                rulesets=[rule_path],
            )
            if result.get("success") and result.get("findings"):
                for f in result["findings"]:
                    sinks.append({
                        "file": f.get("file", ""),
                        "line": f.get("line", 0),
                        "sink_type": sink_type,
                        "function": f.get("rule_id", "").replace("dangerous-", "").replace("raw-", ""),
                        "message": f.get("message", ""),
                        "risk_level": f.get("metadata", {}).get("risk_level", "high"),
                        "source": "semgrep",
                    })
    else:
        # Heuristic fallback: grep for sink patterns
        sinks = await _heuristic_sink_hunt(runner, repo_path, sink_types)

    # Store in wm_knowledge
    if sinks:
        try:
            await db.store_knowledge(
                title=f"sink_map:{len(sinks)}_sinks",
                content=json.dumps({"sinks": sinks}),
                category="other",
                source_tool="sast_sink_hunt",
                metadata={
                    "total_sinks": len(sinks),
                    "by_type": _count_by_key(sinks, "sink_type"),
                    "by_risk": _count_by_key(sinks, "risk_level"),
                },
                tags=["sink_map"] + sink_types,
            )
        except Exception:
            pass

    # Summary
    by_type = _count_by_key(sinks, "sink_type")
    by_risk = _count_by_key(sinks, "risk_level")

    return _json_content({
        "success": True,
        "total_sinks": len(sinks),
        "by_type": by_type,
        "by_risk": by_risk,
        "sinks": sinks[:50],  # Limit response size
        "detection_method": "semgrep" if semgrep_available else "heuristic",
        "message": f"Found {len(sinks)} dangerous sinks across {len(by_type)} categories.",
    })


async def _check_semgrep_available(runner) -> bool:
    """Check if semgrep is available."""
    result = await runner._exec("which", ["semgrep"], timeout=5)
    return result["exit_code"] == 0


async def _heuristic_sink_hunt(runner, repo_path: str, sink_types: list) -> list:
    """Fallback: use grep to find sink patterns."""
    sinks = []
    for sink_type in sink_types:
        patterns = _BUILTIN_SINK_PATTERNS.get(sink_type, [])
        for pattern, func_name, risk_level in patterns:
            result = await runner._exec(
                "grep", ["-rnE", pattern, repo_path,
                         "--include=*.py", "--include=*.js", "--include=*.ts",
                         "--include=*.java", "--include=*.rb", "--include=*.php"],
                timeout=30,
            )
            if result["exit_code"] == 0 and result["stdout"].strip():
                for line in result["stdout"].strip().splitlines()[:20]:
                    parts = line.split(":", 2)
                    if len(parts) >= 3:
                        file_path = parts[0].replace(repo_path + "/", "")
                        try:
                            line_no = int(parts[1])
                        except ValueError:
                            line_no = 0
                        sinks.append({
                            "file": file_path,
                            "line": line_no,
                            "sink_type": sink_type,
                            "function": func_name,
                            "message": parts[2].strip()[:200],
                            "risk_level": risk_level,
                            "source": "heuristic",
                        })
    return sinks


def _count_by_key(items: list, key: str) -> dict:
    """Count items by a key value."""
    counts = {}
    for item in items:
        v = item.get(key, "unknown")
        counts[v] = counts.get(v, 0) + 1
    return counts


async def _handle_sast_bind_endpoints(args: dict, mcp_service) -> List[TextContent]:
    """Bind code-level sinks to discovered endpoints using route pattern matching."""
    db = await _get_db(mcp_service)

    # Read sink map from wm_knowledge
    sink_results = await db.query("knowledge", filters={"source_tool": "sast_sink_hunt"}, limit=10)
    if not sink_results:
        return _json_content({
            "success": True,
            "bound": 0,
            "message": "No sink map found. Run sast_sink_hunt first.",
        })

    # Parse sinks from latest result
    latest_sink_map = sink_results[0]
    sink_data = json.loads(latest_sink_map.get("content", "{}"))
    sinks = sink_data.get("sinks", [])

    # Read endpoints
    endpoints = await db.query(table="endpoints", limit=1000)
    if not endpoints:
        return _json_content({
            "success": True,
            "bound": 0,
            "sinks_found": len(sinks),
            "message": "No endpoints in world model. Run crawler_start or openapi_parse first.",
        })

    # Read source code to find route decorators
    runner = get_sast_runner()
    repo_dir = f"{runner.repo_base_dir}/{mcp_service.current_assessment_id}"
    find_result = await runner._exec("find", [repo_dir, "-maxdepth", "1", "-type", "d"], timeout=10)
    if find_result["exit_code"] != 0 or len(find_result["stdout"].strip().splitlines()) < 2:
        return _error_content("No cloned repository found.")

    repo_path = find_result["stdout"].strip().splitlines()[1]

    # Find route definitions in source code
    route_bindings = await _find_route_bindings(runner, repo_path)

    # Match sinks -> routes -> endpoints
    bound = 0
    bindings = []
    for ep in endpoints:
        ep_path = ep.get("path", "")
        ep_method = (ep.get("method") or "GET").upper()

        # Check if any route binding matches this endpoint
        for binding in route_bindings:
            if _routes_match(binding["route"], ep_path) and \
               (not binding.get("method") or binding["method"].upper() == ep_method):
                # Found matching route -> bind source_file/source_line
                source_file = binding["file"]
                source_line = binding["line"]

                # Update endpoint metadata with source binding
                ep_meta = ep.get("metadata", {}) or {}
                ep_meta["source_file"] = source_file
                ep_meta["source_line"] = source_line

                # Check if any sinks are in the same file near the route
                nearby_sinks = [
                    s for s in sinks
                    if s["file"] == source_file
                    and abs(s["line"] - source_line) < 50
                ]
                if nearby_sinks:
                    ep_meta["nearby_sinks"] = [
                        {"function": s["function"], "line": s["line"],
                         "sink_type": s["sink_type"], "risk_level": s["risk_level"]}
                        for s in nearby_sinks
                    ]

                try:
                    await db.update_endpoint(
                        endpoint_id=ep["id"],
                        metadata=ep_meta,
                    )
                    bound += 1
                    bindings.append({
                        "endpoint": ep_path,
                        "method": ep_method,
                        "source_file": source_file,
                        "source_line": source_line,
                        "nearby_sinks": len(nearby_sinks),
                    })
                except Exception:
                    pass
                break  # Only first matching route

    return _json_content({
        "success": True,
        "bound": bound,
        "total_endpoints": len(endpoints),
        "total_sinks": len(sinks),
        "route_definitions": len(route_bindings),
        "bindings": bindings[:30],
        "message": f"Bound {bound} of {len(endpoints)} endpoints to source code.",
    })


async def _find_route_bindings(runner, repo_path: str) -> list:
    """Find route definitions in source code using grep."""
    bindings = []

    # Python: @app.route('/path'), @blueprint.route('/path'), @router.get('/path')
    py_patterns = [
        r"@\w+\.(route|get|post|put|delete|patch)\s*\(['\"]([^'\"]+)['\"]",
    ]
    for pattern in py_patterns:
        result = await runner._exec(
            "grep", ["-rnE", pattern, repo_path,
                     "--include=*.py"],
            timeout=30,
        )
        if result["exit_code"] == 0 and result["stdout"].strip():
            for line in result["stdout"].strip().splitlines()[:100]:
                parts = line.split(":", 2)
                if len(parts) >= 3:
                    file_path = parts[0].replace(repo_path + "/", "")
                    try:
                        line_no = int(parts[1])
                    except ValueError:
                        line_no = 0
                    # Extract route path and method
                    import re as _re
                    match = _re.search(r'\.(route|get|post|put|delete|patch)\s*\([\'"]([^\'"]+)[\'"]', parts[2])
                    if match:
                        method_str = match.group(1)
                        route = match.group(2)
                        method = method_str.upper() if method_str != "route" else None
                        bindings.append({
                            "file": file_path,
                            "line": line_no,
                            "route": route,
                            "method": method,
                        })

    # JavaScript/TypeScript: app.get('/path', ...), router.post('/path', ...)
    js_patterns = [
        r"\.(get|post|put|delete|patch|all)\s*\(['\"]([^'\"]+)['\"]",
    ]
    for pattern in js_patterns:
        result = await runner._exec(
            "grep", ["-rnE", pattern, repo_path,
                     "--include=*.js", "--include=*.ts"],
            timeout=30,
        )
        if result["exit_code"] == 0 and result["stdout"].strip():
            for line in result["stdout"].strip().splitlines()[:100]:
                parts = line.split(":", 2)
                if len(parts) >= 3:
                    file_path = parts[0].replace(repo_path + "/", "")
                    try:
                        line_no = int(parts[1])
                    except ValueError:
                        line_no = 0
                    import re as _re
                    match = _re.search(r'\.(get|post|put|delete|patch|all)\s*\([\'"]([^\'"]+)[\'"]', parts[2])
                    if match:
                        method_str = match.group(1)
                        route = match.group(2)
                        method = method_str.upper() if method_str != "all" else None
                        bindings.append({
                            "file": file_path,
                            "line": line_no,
                            "route": route,
                            "method": method,
                        })

    return bindings


def _routes_match(source_route: str, endpoint_path: str) -> bool:
    """Check if a source code route matches an endpoint path."""
    # Normalize both
    sr = source_route.rstrip("/").lower()
    ep = endpoint_path.rstrip("/").lower()

    if sr == ep:
        return True

    # Handle route parameters: /users/<id> matches /users/{id}
    # Convert <param> and {param} to wildcards
    sr_normalized = re.sub(r'[<{]\w+[>}]', '*', sr)
    ep_normalized = re.sub(r'[<{]\w+[>}]', '*', ep)

    if sr_normalized == ep_normalized:
        return True

    # Check if one is a prefix of the other (api prefix handling)
    if ep.endswith(sr) or sr.endswith(ep):
        return True

    return False


# ========== Helper Functions ==========

def _detect_file_language(file_path: str) -> Optional[str]:
    """Detect language from file extension."""
    ext_map = {
        ".py": "python",
        ".js": "javascript",
        ".ts": "typescript",
        ".jsx": "javascript",
        ".tsx": "typescript",
        ".java": "java",
        ".go": "go",
        ".rb": "ruby",
        ".php": "php",
        ".c": "c",
        ".cpp": "cpp",
        ".cs": "csharp",
        ".rs": "rust",
        ".swift": "swift",
        ".kt": "kotlin",
    }

    for ext, lang in ext_map.items():
        if file_path.endswith(ext):
            return lang

    return None


def _calculate_file_priority(file_path: str) -> tuple:
    """Calculate file priority score (0-100) and reasons.

    Returns: (score, reasons)
    """
    path_lower = file_path.lower()
    score = 50  # Base score
    reasons = []

    # Routes, controllers, handlers, views (+30)
    if any(keyword in path_lower for keyword in ["route", "controller", "handler", "view", "endpoint"]):
        score += 30
        reasons.append("Route/Controller")

    # Auth, login, session (+25)
    if any(keyword in path_lower for keyword in ["auth", "login", "session", "token", "password"]):
        score += 25
        reasons.append("Authentication")

    # Database, ORM, query, model (+20)
    if any(keyword in path_lower for keyword in ["database", "db", "orm", "query", "model"]):
        score += 20
        reasons.append("Database")

    # API definitions (+15)
    if any(keyword in path_lower for keyword in ["api", "graphql", "rest"]):
        score += 15
        reasons.append("API")

    # Config, settings (+10)
    if any(keyword in path_lower for keyword in ["config", "settings", "env"]):
        score += 10
        reasons.append("Config")

    # Test files (-10)
    if any(keyword in path_lower for keyword in ["test", "spec", "_test.", ".test."]):
        score -= 10
        reasons.append("Test file")

    # Generated, vendor (-20)
    if any(keyword in path_lower for keyword in ["generated", "vendor", "node_modules", ".min.", "bundle"]):
        score -= 20
        reasons.append("Generated/Vendor")

    return (max(0, min(100, score)), reasons)
