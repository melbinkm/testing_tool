"""
MCP Resources - Resource definitions and handlers
"""
import json
from typing import List
from mcp.types import Resource


def get_resources() -> List[Resource]:
    """List available resources"""
    return [
        Resource(
            uri="kali://status",
            name="Current Status",
            description="Current assessment status and container info",
            mimeType="application/json"
        ),
        Resource(
            uri="kali://containers",
            name="Kali Containers",
            description="List of all Kali pentesting containers",
            mimeType="application/json"
        ),
        Resource(
            uri="autopentest://error-recovery",
            name="Error Recovery Patterns",
            description="Common errors and step-by-step recovery strategies for autonomous error handling",
            mimeType="application/json"
        ),
        Resource(
            uri="autopentest://attack-patterns",
            name="Attack Chaining Patterns",
            description="Proven multi-tool attack sequences for complex exploitation chains",
            mimeType="application/json"
        ),
        Resource(
            uri="autopentest://budget-optimization",
            name="Budget Management Strategies",
            description="Smart strategies for managing request budget and scope constraints",
            mimeType="application/json"
        ),
        Resource(
            uri="autopentest://workflow-guide",
            name="Phase Workflow Guide",
            description="Dynamic workflow recommendations based on current assessment phase and state",
            mimeType="application/json"
        ),
        Resource(
            uri="autopentest://tool-dependencies",
            name="Tool Dependencies & Chains",
            description="Tool sequencing rules, prerequisites, and common multi-tool workflows",
            mimeType="application/json"
        ),
        Resource(
            uri="autopentest://tool-metadata",
            name="Tool Metadata Registry",
            description="Machine-readable tool categories, dependencies, risk levels, and phase mappings for all 136 tools",
            mimeType="application/json"
        ),
        Resource(
            uri="autopentest://pentest-workflow",
            name="LLM-in-the-Loop Pentest Workflow",
            description="Step-by-step workflow for manual security testing using the 8 pentest tools",
            mimeType="application/json"
        ),
        Resource(
            uri="autopentest://sast-workflow",
            name="SAST Workflow Guide",
            description="Step-by-step workflow for static application security testing using the 13 SAST tools",
            mimeType="application/json"
        ),
        Resource(
            uri="autopentest://autonomous-runbook",
            name="Autonomous Pentest Runbook",
            description="Complete 6-phase runbook with step sequences, gate checks, and approval points for autonomous execution",
            mimeType="application/json"
        ),
        Resource(
            uri="autopentest://code-audit-workflow",
            name="Code Audit Workflow Guide",
            description="Step-by-step workflow for LLM-powered full code audit using the 4 code audit tools",
            mimeType="application/json"
        )
    ]


async def handle_resource_read(uri: str, mcp_service) -> str:
    """Read resource content"""
    await mcp_service.initialize()

    if uri == "kali://status":
        status_info = {
            "current_assessment": mcp_service.current_assessment_name,
            "assessment_id": mcp_service.current_assessment_id,
            "current_container": mcp_service.current_container,
            "current_target": mcp_service.current_target,
            "containers_available": len(mcp_service.containers_cache),
            "recent_commands": len(mcp_service.command_history),
            "tool_cache_size": len(mcp_service.tool_cache)
        }
        return json.dumps(status_info, indent=2)

    elif uri == "kali://containers":
        containers = await mcp_service.discover_containers()
        return json.dumps(containers, indent=2)

    elif uri == "autopentest://error-recovery":
        return json.dumps(_get_error_recovery_patterns(), indent=2)

    elif uri == "autopentest://attack-patterns":
        return json.dumps(_get_attack_patterns(), indent=2)

    elif uri == "autopentest://budget-optimization":
        return json.dumps(_get_budget_optimization(), indent=2)

    elif uri == "autopentest://workflow-guide":
        return json.dumps(await _generate_workflow_guide(mcp_service), indent=2)

    elif uri == "autopentest://tool-dependencies":
        return json.dumps(_get_tool_dependencies(), indent=2)

    elif uri == "autopentest://tool-metadata":
        from lib.tool_metadata import TOOL_METADATA, TOOL_CATEGORIES, PHASE_TOOLS
        return json.dumps({
            "tool_metadata": TOOL_METADATA,
            "categories": TOOL_CATEGORIES,
            "phase_tools": PHASE_TOOLS,
            "summary": {
                "total_tools": len(TOOL_METADATA),
                "by_category": {cat: len(tools) for cat, tools in TOOL_CATEGORIES.items()},
                "by_phase": {phase: len(tools) for phase, tools in PHASE_TOOLS.items()},
                "high_risk_tools": len([t for t, m in TOOL_METADATA.items() if m.get("risk_level") == "high_risk"]),
                "high_budget_tools": len([t for t, m in TOOL_METADATA.items() if m.get("budget_impact") == "high"])
            }
        }, indent=2)

    elif uri == "autopentest://pentest-workflow":
        return json.dumps(_get_pentest_workflow(), indent=2)

    elif uri == "autopentest://sast-workflow":
        return json.dumps(_get_sast_workflow(), indent=2)

    elif uri == "autopentest://autonomous-runbook":
        return json.dumps(_get_autonomous_runbook(), indent=2)

    elif uri == "autopentest://code-audit-workflow":
        return json.dumps(_get_code_audit_workflow(), indent=2)

    else:
        raise ValueError(f"Unknown resource: {uri}")


def _get_error_recovery_patterns() -> dict:
    """Common errors and recovery strategies for autonomous error handling."""
    return {
        "common_errors": {
            "scope_violation": {
                "symptoms": ["out of scope", "target not in allowlist", "scope validation failed"],
                "recovery": [
                    "1. Call scope_get_allowlist() to see all allowed targets, domains, and IP ranges",
                    "2. Verify target matches allowlist patterns (exact domain, wildcard subdomain, or IP range)",
                    "3. If legitimately out of scope: document as observation and skip testing",
                    "4. If should be in scope: inform user and ask them to update allowlist configuration",
                    "5. Never bypass scope validation - fail closed for safety"
                ],
                "prevention": "Always call scope_validate_target() before ANY request to a target"
            },
            "budget_exhausted": {
                "symptoms": ["budget exceeded", "rate limit reached", "too many requests"],
                "recovery": [
                    "1. Call scope_check_budget() to see remaining requests and per-target limits",
                    "2. If >50 requests remain: prioritize high-value tests (coverage cells with priority>70)",
                    "3. If 20-50 requests: only test confirmed vulnerabilities and skip broad scanning",
                    "4. If <20 requests: complete evidence collection and generate report",
                    "5. Use http_send_batch() for parallel requests to optimize budget",
                    "6. Skip expensive tools: nuclei_scan_template, fuzz_endpoint, crawler_start"
                ],
                "prevention": "Call scope_check_budget() every 20-30 tool calls, especially before high-cost operations"
            },
            "empty_crawl_results": {
                "symptoms": ["0 pages crawled", "no endpoints found", "crawler returned empty"],
                "recovery": [
                    "1. Check if JavaScript rendering needed: use browser_navigate() + browser_discover_forms()",
                    "2. Try authenticated crawl: credentials_add() then crawler_start(identity_id=...)",
                    "3. Use directory discovery: execute('gobuster dir ...') or scan(type='gobuster')",
                    "4. Parse API specs if available: openapi_parse() + openapi_list_endpoints()",
                    "5. Manual endpoint addition: wm_add_endpoint() for known paths"
                ],
                "prevention": "Check if target uses JavaScript-heavy SPA or requires authentication before crawling"
            },
            "nuclei_no_findings": {
                "symptoms": ["0 vulnerabilities found", "all nuclei templates returned clean"],
                "recovery": [
                    "1. This is EXPECTED for secure targets - not an error",
                    "2. Proceed to manual testing: fuzz_parameter(), endpoint_execute_plan()",
                    "3. Focus on business logic: sequence_workflow_bypass(), auth_diff_test()",
                    "4. Test authorization: auth_diff_test() with multiple identities",
                    "5. Custom injection tests: http_send() with crafted payloads"
                ],
                "note": "Nuclei finds known CVEs and misconfigurations, not custom application logic flaws"
            },
            "validation_failure": {
                "symptoms": ["validate_repro failed", "cannot reproduce", "inconsistent results"],
                "recovery": [
                    "1. Check if timing-dependent: retry validate_repro() 2-3 times",
                    "2. Verify credentials still valid: test authentication separately",
                    "3. Check if WAF detected and blocked: look for 403/429 status codes",
                    "4. Verify exact request parameters: ensure payload not URL-encoded incorrectly",
                    "5. If consistently fails (3+ attempts): downgrade to 'observation' not 'finding'",
                    "6. Document unreliability in notes even if eventually succeeds"
                ],
                "prevention": "Run validate_repro() immediately after discovering potential finding, before state changes"
            },
            "tool_not_available": {
                "symptoms": ["tool not available", "command not found", "tool_help returned empty"],
                "recovery": [
                    "1. Check tool name spelling and case sensitivity",
                    "2. Use tool_help() to verify tool exists in Kali container",
                    "3. Try alternative tools: gobuster instead of ffuf, subfinder instead of amass",
                    "4. Use execute() with explicit path: /usr/bin/tool instead of tool",
                    "5. Install tool if missing: execute('apt-get update && apt-get install -y tool')"
                ],
                "prevention": "Call tool_help() before first use of unfamiliar tools"
            },
            "timeout": {
                "symptoms": ["timeout exceeded", "operation timed out", "no response"],
                "recovery": [
                    "1. For scan timeouts: reduce scope (fewer ports, specific targets only)",
                    "2. For directory scan timeouts: use smaller wordlist (wordlist='common')",
                    "3. For http_send timeouts: increase timeout parameter or check if target is down",
                    "4. For execute timeouts: break command into smaller steps or run manually",
                    "5. Verify target is responsive: simple GET request before complex scan"
                ],
                "prevention": "Start with quick scans (nmap_quick, common wordlist) before comprehensive scans"
            },
            "authentication_required": {
                "symptoms": ["401 Unauthorized", "403 Forbidden", "login required", "authentication needed"],
                "recovery": [
                    "1. Check scope_get_identities() for available credentials",
                    "2. If credentials exist: use identity_id parameter in crawler_start() or http_send()",
                    "3. If no credentials: test authentication bypass first (auth_diff_test, http_send with header manipulation)",
                    "4. Document accessible vs. authenticated attack surface separately",
                    "5. Re-run discovery after obtaining credentials: crawler_start(identity_id=...) then coverage_discover()"
                ],
                "note": "Always test both unauthenticated and authenticated attack surfaces separately"
            }
        },
        "general_principles": [
            "Read error messages carefully - they often suggest the fix",
            "Check budget and scope before retrying expensive operations",
            "Try alternative approaches when primary tool fails",
            "Document failures as observations even if they don't lead to findings",
            "Don't retry the exact same operation more than 3 times",
            "Consult tool descriptions for known failure modes and workarounds"
        ]
    }


def _get_attack_patterns() -> dict:
    """Proven multi-tool attack sequences for complex exploitation chains."""
    return {
        "patterns": {
            "auth_bypass_exploitation": {
                "description": "Full exploitation chain after discovering authentication bypass vulnerability",
                "use_when": "You discovered a way to bypass authentication (missing auth check, JWT manipulation, session fixation, etc.)",
                "sequence": [
                    {
                        "phase": "Confirm",
                        "tools": ["validate_repro", "validate_negative_control", "validate_promote"],
                        "note": "Ensure bypass is reproducible and not a false positive"
                    },
                    {
                        "phase": "Document",
                        "tools": ["evidence_bundle", "evidence_add_artifact", "poc_generate"],
                        "note": "Capture evidence before state changes"
                    },
                    {
                        "phase": "Store Credentials",
                        "tools": ["credentials_add"],
                        "note": "Store the bypass method as a usable identity for follow-up testing"
                    },
                    {
                        "phase": "Re-Crawl",
                        "tools": ["crawler_start"],
                        "params": {"identity_id": "<bypass_cred_id>"},
                        "note": "Re-crawl application with bypassed/elevated access to discover authenticated endpoints"
                    },
                    {
                        "phase": "Extend Coverage",
                        "tools": ["coverage_discover", "coverage_refresh"],
                        "note": "Detect newly accessible endpoints and add them to coverage matrix"
                    },
                    {
                        "phase": "Test New Surface",
                        "tools": ["coverage_next", "endpoint_execute_plan"],
                        "note": "Test the newly discovered authenticated endpoints for additional vulnerabilities"
                    },
                    {
                        "phase": "Document Chain",
                        "tools": ["wm_add_relationship"],
                        "params": {"rel_type": "leads_to"},
                        "note": "Link auth bypass finding to secondary findings discovered via elevated access"
                    }
                ],
                "expected_outcome": "Complete mapping of authenticated attack surface and potential secondary vulnerabilities"
            },
            "idor_to_data_exfil": {
                "description": "Chain IDOR discovery to comprehensive data enumeration and exfiltration",
                "use_when": "You found an endpoint where changing user ID/resource ID grants unauthorized access to other users' data",
                "sequence": [
                    {
                        "phase": "Discover",
                        "tools": ["auth_diff_test"],
                        "params": {"url": "/api/users/{id}", "identity_ids": ["user_a", "user_b"]},
                        "note": "Confirm different users can access each other's resources by changing ID parameter"
                    },
                    {
                        "phase": "Validate Ownership",
                        "tools": ["sequence_data_ownership"],
                        "params": {"url_template": "/api/users/{id}", "resource_ids": [1, 2, 3, 5, 10, 100]},
                        "note": "Test sample IDs to confirm pattern and identify accessible ID range"
                    },
                    {
                        "phase": "Enumerate IDs",
                        "tools": ["http_send_batch"],
                        "note": "Batch enumerate all accessible IDs (respect budget limits - prioritize sample over exhaustive)",
                        "caution": "Check budget first: scope_check_budget(). For large ID spaces, enumerate sample (every 10th, 100th)"
                    },
                    {
                        "phase": "Evidence Collection",
                        "tools": ["evidence_bundle", "evidence_add_artifact"],
                        "note": "Capture request/response examples showing unauthorized access to multiple users' data"
                    },
                    {
                        "phase": "Risk Assessment",
                        "tools": ["risk_score"],
                        "params": {"data_classification": "confidential", "confidence": 0.9},
                        "note": "Calculate business impact based on data sensitivity"
                    }
                ],
                "expected_outcome": "Proof of IDOR vulnerability with evidence of data exfiltration and business impact assessment"
            },
            "sqli_to_rce": {
                "description": "Escalate SQL injection to remote code execution through database features",
                "use_when": "You confirmed SQL injection in a parameter and want to maximize impact",
                "sequence": [
                    {
                        "phase": "Detect",
                        "tools": ["fuzz_parameter"],
                        "params": {"payload_types": ["sqli_db"], "db_type": "mysql"},
                        "note": "Identify SQL injection with database-specific payloads"
                    },
                    {
                        "phase": "Validate",
                        "tools": ["validate_repro", "validate_negative_control"],
                        "note": "Confirm injection is real and not a false positive from error message variations"
                    },
                    {
                        "phase": "Promote",
                        "tools": ["validate_promote", "evidence_bundle"],
                        "note": "Mark as confirmed finding and start evidence collection"
                    },
                    {
                        "phase": "Advanced Exploitation",
                        "tools": ["execute"],
                        "params": {"command": "sqlmap -u 'URL' -p 'param' --batch --risk=3 --level=5 --threads=5"},
                        "note": "Use sqlmap for advanced enumeration: db version, users, tables, data extraction"
                    },
                    {
                        "phase": "Privilege Escalation",
                        "tools": ["execute"],
                        "params": {"command": "sqlmap ... --is-dba --privileges"},
                        "note": "Check if database user has DBA privileges for file operations"
                    },
                    {
                        "phase": "File Operations",
                        "tools": ["execute"],
                        "params": {"command": "sqlmap ... --file-write=shell.php --file-dest=/var/www/html/shell.php"},
                        "note": "Attempt file write (PHP webshell) if DBA. ONLY if explicitly authorized in scope."
                    },
                    {
                        "phase": "OS Command Execution",
                        "tools": ["execute"],
                        "params": {"command": "sqlmap ... --os-shell"},
                        "note": "Attempt command execution via xp_cmdshell (MSSQL) or sys_exec (MySQL). HIGH RISK - requires explicit authorization."
                    },
                    {
                        "phase": "Documentation",
                        "tools": ["wm_store", "poc_generate", "evidence_add_artifact"],
                        "note": "Store full sqlmap output, generate PoC, and package evidence"
                    }
                ],
                "expected_outcome": "Comprehensive SQLi documentation from detection through potential RCE",
                "warnings": [
                    "OS command execution and file write operations are HIGH RISK",
                    "Verify scope explicitly permits exploitation beyond detection",
                    "Stop at database enumeration if RCE attempts are out of scope"
                ]
            },
            "xss_to_session_hijack": {
                "description": "Escalate XSS discovery to session hijacking and account takeover demonstration",
                "use_when": "You discovered XSS and want to demonstrate maximum business impact",
                "sequence": [
                    {
                        "phase": "Detect",
                        "tools": ["browser_test_xss", "fuzz_parameter"],
                        "params": {"payload_types": ["injection", "format"]},
                        "note": "Identify XSS vulnerability in parameter with JavaScript execution"
                    },
                    {
                        "phase": "Confirm Execution",
                        "tools": ["browser_navigate", "browser_eval"],
                        "params": {"expression": "document.cookie"},
                        "note": "Load page in real browser and verify JavaScript executes in user context"
                    },
                    {
                        "phase": "Test Cookie Exfiltration",
                        "tools": ["browser_eval"],
                        "params": {"expression": "document.cookie"},
                        "note": "Check if session cookies are accessible (httpOnly flag not set)"
                    },
                    {
                        "phase": "Visual Evidence",
                        "tools": ["browser_screenshot"],
                        "note": "Capture screenshot of alert() or modified DOM as visual proof"
                    },
                    {
                        "phase": "Validate",
                        "tools": ["validate_repro", "validate_negative_control"],
                        "note": "Confirm XSS is reproducible and not filtered by WAF"
                    },
                    {
                        "phase": "PoC Development",
                        "tools": ["poc_generate"],
                        "note": "Generate PoC showing session cookie exfiltration payload (DO NOT actually exfiltrate)"
                    },
                    {
                        "phase": "Evidence Collection",
                        "tools": ["evidence_bundle", "evidence_add_artifact"],
                        "note": "Package screenshot, PoC, request/response, and cookie accessibility proof"
                    }
                ],
                "expected_outcome": "Demonstrated XSS with proof of session cookie accessibility for account takeover scenario",
                "warnings": [
                    "DO NOT actually exfiltrate session cookies to external server",
                    "Stop at proof-of-concept - demonstrate capability without causing harm"
                ]
            },
            "subdomain_takeover": {
                "description": "Discover and validate subdomain takeover vulnerability through full reconnaissance chain",
                "use_when": "Performing domain reconnaissance and discovering dangling DNS records",
                "sequence": [
                    {
                        "phase": "Discovery",
                        "tools": ["subdomain_enum"],
                        "note": "Discover all subdomains via passive reconnaissance"
                    },
                    {
                        "phase": "Validation",
                        "tools": ["scope_validate_target"],
                        "note": "Verify each subdomain is in scope before testing"
                    },
                    {
                        "phase": "DNS Analysis",
                        "tools": ["execute"],
                        "params": {"command": "dig +short subdomain.example.com"},
                        "note": "Check DNS records for each subdomain to identify CNAME records"
                    },
                    {
                        "phase": "HTTP Testing",
                        "tools": ["http_send"],
                        "note": "Attempt to access each subdomain and look for service-specific takeover signatures"
                    },
                    {
                        "phase": "Service Identification",
                        "tools": ["tech_detection"],
                        "note": "Identify hosting platform (GitHub Pages, AWS S3, Heroku, etc.) for dangling CNAMEs"
                    },
                    {
                        "phase": "Takeover Validation",
                        "note": "Document dangling CNAME but DO NOT claim subdomain - only demonstrate vulnerability exists"
                    },
                    {
                        "phase": "Documentation",
                        "tools": ["add_card", "evidence_bundle"],
                        "note": "Document finding with DNS records, HTTP responses, and service identification"
                    }
                ],
                "expected_outcome": "Subdomain takeover vulnerability documented without actual takeover",
                "warnings": [
                    "DO NOT claim or takeover subdomains - only document vulnerability",
                    "Subdomain takeover can have legal implications - document, don't exploit"
                ]
            }
        },
        "general_workflow": {
            "description": "Standard multi-tool workflow for comprehensive endpoint testing",
            "sequence": [
                "1. Discover (recon_pipeline_run, crawler_start, openapi_parse)",
                "2. Map (coverage_init, wm_add_endpoint, wm_add_hypothesis)",
                "3. Prioritize (coverage_next with priority>70 first)",
                "4. Test (endpoint_execute_plan, fuzz_parameter, auth_diff_test)",
                "5. Validate (validate_repro, validate_negative_control, validate_promote)",
                "6. Document (evidence_bundle, poc_generate, evidence_export)",
                "7. Report (evidence_generate_report only when user requests)"
            ]
        }
    }


def _get_budget_optimization() -> dict:
    """Smart strategies for managing request budget and scope constraints."""
    return {
        "budget_check_frequency": "Call scope_check_budget() every 20-30 tool calls, especially before high-cost operations",
        "optimization_strategies": {
            "high_budget": {
                "threshold": ">500 requests remaining",
                "strategy": "Comprehensive discovery - use all reconnaissance and scanning tools",
                "recommended_tools": [
                    "recon_pipeline_run (full 6-stage pipeline)",
                    "crawler_start (with max_pages=200)",
                    "nuclei_scan_template (with multiple tags: cve, misconfig, default-logins)",
                    "scan (type='nmap_full' for comprehensive port scan)",
                    "fuzz_endpoint (test all parameters with full payload sets)"
                ],
                "rationale": "Early reconnaissance phases need broad discovery to identify complete attack surface. Invest budget in comprehensive coverage now to find all testing targets."
            },
            "medium_budget": {
                "threshold": "100-500 requests remaining",
                "strategy": "Targeted testing - prioritize high-value endpoints and known vulnerability classes",
                "recommended_tools": [
                    "coverage_next (with limit=20, prioritize cells with priority>70)",
                    "endpoint_probe (gather context before full testing)",
                    "endpoint_execute_plan (with use_library=true for curated payloads)",
                    "auth_diff_test (focus on authorization issues)",
                    "http_send_batch (batch parallel requests for efficiency)"
                ],
                "tools_to_avoid": [
                    "recon_pipeline_run (too expensive for remaining budget)",
                    "nuclei_scan_template with broad tags (hundreds of templates)",
                    "fuzz_endpoint (high request volume per endpoint)",
                    "scan with type='nmap_full' (use nmap_quick instead)"
                ],
                "rationale": "Focus on likely vulnerabilities and high-priority targets. Use budget efficiently with batching and targeted testing."
            },
            "low_budget": {
                "threshold": "20-100 requests remaining",
                "strategy": "Validation and evidence collection only - stop new discovery",
                "recommended_tools": [
                    "validate_repro (confirm existing findings)",
                    "validate_negative_control (rule out false positives)",
                    "validate_promote (graduate validated findings)",
                    "evidence_bundle (create evidence packages)",
                    "evidence_add_artifact (collect proof)",
                    "http_send (surgical single requests for critical tests)"
                ],
                "tools_to_avoid": [
                    "ANY scanning tools (scan, nuclei_scan_template)",
                    "crawler_start (even small crawls use 50+ requests)",
                    "fuzz_parameter or fuzz_endpoint",
                    "coverage_next (stop new testing, focus on validation)"
                ],
                "rationale": "Preserve budget for confirming discovered findings. Stop exploration, focus on quality over quantity."
            },
            "critical_budget": {
                "threshold": "<20 requests remaining",
                "strategy": "Documentation and reporting only - stop all testing",
                "recommended_tools": [
                    "evidence_export (export findings as SARIF/JSON/ZIP)",
                    "poc_generate (create PoC scripts for findings)",
                    "evidence_generate_report (if user requests report)",
                    "wm_query (retrieve findings and observations)",
                    "list_cards (review all findings)",
                    "risk_assess (calculate risk scores)"
                ],
                "stop_testing": "No more requests to targets - only local documentation and reporting operations",
                "rationale": "Budget exhausted. Complete assessment documentation and prepare deliverables."
            }
        },
        "budget_optimization_techniques": {
            "batching": {
                "description": "Use http_send_batch() instead of multiple http_send() calls",
                "example": "Instead of 10x http_send(), use 1x http_send_batch([...10 requests...])",
                "savings": "Shares rate limiting overhead across requests, more efficient budget usage"
            },
            "probing": {
                "description": "Use endpoint_probe() before endpoint_execute_plan() to gather context",
                "example": "endpoint_probe() returns reflected params, tech indicators, error patterns - use this to craft targeted payloads instead of brute-force testing",
                "savings": "Reduces unnecessary payload attempts by targeting only relevant vulnerability classes"
            },
            "library_payloads": {
                "description": "Set use_library=true in endpoint_execute_plan() to use curated payload sets",
                "example": "Library payloads are pre-selected high-quality payloads (20-50 per vuln class) instead of LLM-generated exhaustive sets (hundreds)",
                "savings": "Reduces payload count by 80-90% while maintaining detection effectiveness"
            },
            "coverage_prioritization": {
                "description": "Use coverage_next() priority scores to test high-value cells first",
                "example": "coverage_next(limit=10) returns cells sorted by priority (endpoint risk × vuln class severity × parameter exposure)",
                "savings": "Test 20% of coverage matrix to find 80% of vulnerabilities (Pareto principle)"
            },
            "incremental_scanning": {
                "description": "Start with quick scans, only escalate to comprehensive if findings warrant it",
                "example": "scan(type='nmap_quick') first (~100 requests), only run scan(type='nmap_full') if interesting services found",
                "savings": "Avoid wasting budget on comprehensive scans of uninteresting targets"
            },
            "smart_wordlists": {
                "description": "Use smaller wordlists for directory discovery",
                "example": "wordlist='common' (~4500 paths) instead of wordlist='large' (~220000 paths)",
                "savings": "Common wordlist finds 90% of directories using 2% of requests"
            },
            "knowledge_reuse": {
                "description": "Check wm_recall() before re-scanning to see if data already exists",
                "example": "Before running scan(), call wm_recall(category='scan_output', target='example.com') to see if already scanned",
                "savings": "Avoid redundant scans when resuming assessment or testing similar targets"
            }
        },
        "budget_monitoring": {
            "check_before": [
                "recon_pipeline_run (500-2000 requests)",
                "nuclei_scan_template (100-1000 requests per target)",
                "fuzz_endpoint (200-500 requests per endpoint)",
                "crawler_start (50-200 requests per site)",
                "scan with type='nmap_full' or directory discovery (100-1000 requests)"
            ],
            "safe_anytime": [
                "wm_* tools (world model operations - local only)",
                "list_* tools (listing operations - local only)",
                "scope_* tools (scope operations - local only)",
                "evidence_* tools (evidence operations - local only)",
                "coverage_* tools except coverage_next execution (most are local planning)",
                "openapi_parse (local parsing - no requests)"
            ],
            "budget_recovery": "Budget does NOT recover during assessment - it's a fixed allocation. Plan budget usage across entire assessment lifecycle, not per-phase."
        },
        "phase_budget_allocation": {
            "recommended_distribution": {
                "Phase 1 Recon": "35% of budget - broad discovery to identify all targets",
                "Phase 2 Mapping": "20% of budget - enumerate endpoints and build coverage matrix",
                "Phase 3 SAST": "5% of budget - static code analysis and verification",
                "Phase 4 Assessment": "25% of budget - test vulnerabilities with targeted payloads",
                "Phase 5 Validation": "10% of budget - confirm findings and collect evidence",
                "Phase 6 Reporting": "5% of budget - evidence collection and report generation"
            },
            "note": "Adjust based on assessment type - API testing needs more mapping, network testing needs more recon. SAST phase budget is low since it's mostly local analysis."
        }
    }


async def _generate_workflow_guide(mcp_service) -> dict:
    """Generate dynamic workflow guidance based on current assessment phase and state."""
    # Try to detect current phase from mcp_service
    current_phase = 1
    phase_name = "Reconnaissance"

    # Get current state if assessment loaded
    if mcp_service and hasattr(mcp_service, "current_assessment_id") and mcp_service.current_assessment_id:
        # Query world model for current state indicators
        try:
            from lib.world_model_db import get_world_model_db
            db = await get_world_model_db(mcp_service.current_assessment_id)

            # Check what's been done to infer phase
            assets = await db.query("assets", limit=1)
            endpoints = await db.query("endpoints", limit=1)
            findings = await db.query("findings", limit=1)

            if findings:
                current_phase = 4
                phase_name = "Exploitation & Validation"
            elif endpoints and len(endpoints) > 5:
                current_phase = 3
                phase_name = "Vulnerability Assessment"
            elif endpoints:
                current_phase = 2
                phase_name = "Mapping & Enumeration"
            elif assets:
                current_phase = 1
                phase_name = "Reconnaissance"
        except Exception:
            pass  # Fall back to defaults

    # Phase-specific guidance
    phase_guides = {
        1: {
            "current_phase": 1,
            "phase_name": "Reconnaissance",
            "recommended_tools": [
                {"tool": "recon_pipeline_run", "priority": "high", "reason": "Automated 6-stage recon (subdomain → nmap → tech → SSL → directory → nuclei)"},
                {"tool": "scope_validate_target", "priority": "critical", "reason": "MUST validate every target before testing"},
                {"tool": "scope_check_budget", "priority": "high", "reason": "Check budget before expensive scans"},
                {"tool": "subdomain_enum", "priority": "high", "reason": "Discover all subdomains for complete attack surface"},
                {"tool": "scan", "priority": "high", "reason": "Port/service discovery on all in-scope targets"},
                {"tool": "tech_detection", "priority": "medium", "reason": "Identify tech stack for targeted nuclei templates"},
                {"tool": "ssl_analysis", "priority": "low", "reason": "Extract SANs for additional domains"}
            ],
            "next_actions": [
                "1. Validate scope: scope_get_allowlist() to see all allowed targets",
                "2. Check budget: scope_check_budget() to plan reconnaissance depth",
                "3. Run recon pipeline OR manual recon:",
                "   - AUTO: recon_pipeline_run(target_domain='example.com')",
                "   - MANUAL: subdomain_enum → scan(type='nmap_quick') → tech_detection → ssl_analysis",
                "4. Record discoveries: add_recon_data() for all findings",
                "5. Add assets to world model: wm_add_asset() for each target",
                "6. Store scan output: wm_store() for later recall"
            ],
            "common_mistakes": [
                "❌ Forgetting to call scope_validate_target() before first scan",
                "❌ Running nmap_full on all hosts (budget killer) - use nmap_quick first",
                "❌ Not checking for existing data with wm_recall() before re-scanning",
                "❌ Skipping tech_detection - needed for nuclei template filtering later",
                "❌ Running nuclei_scan_template in recon phase - save for Phase 3"
            ],
            "success_criteria": [
                "✓ All in-scope domains/IPs discovered and added as assets",
                "✓ All open ports and services identified",
                "✓ Technology stack documented for each service",
                "✓ Recon data stored in world model for query"
            ]
        },
        2: {
            "current_phase": 2,
            "phase_name": "Mapping & Enumeration",
            "recommended_tools": [
                {"tool": "openapi_parse", "priority": "high", "reason": "Parse API specs if available (fastest way to map APIs)"},
                {"tool": "crawler_start", "priority": "high", "reason": "Discover all pages/endpoints with authenticated or unauthenticated crawl"},
                {"tool": "browser_discover_forms", "priority": "high", "reason": "Find all HTML forms and input vectors"},
                {"tool": "sast_clone_repo", "priority": "high", "reason": "Clone git repo for static code analysis (if git_repo_url set)"},
                {"tool": "sast_index_repo", "priority": "high", "reason": "Index source code for RAG semantic search"},
                {"tool": "sast_scan_semgrep", "priority": "high", "reason": "Multi-language SAST scan with 200+ rules"},
                {"tool": "sast_scan_bandit", "priority": "medium", "reason": "Python-specific security scan (skip if not Python)"},
                {"tool": "sast_scan_gitleaks", "priority": "high", "reason": "Detect hardcoded secrets, API keys, passwords"},
                {"tool": "coverage_init", "priority": "medium", "reason": "Build coverage matrix after endpoints discovered"},
                {"tool": "wm_add_endpoint", "priority": "medium", "reason": "Manually add endpoints not found by automated discovery"},
                {"tool": "wm_add_hypothesis", "priority": "medium", "reason": "Document testable security hypotheses for each attack vector"}
            ],
            "next_actions": [
                "1. Parse API specs if available: openapi_parse() then openapi_list_endpoints()",
                "2. Crawl web applications:",
                "   - Unauthenticated: crawler_start(start_url='https://target.com')",
                "   - Authenticated: credentials_add() → crawler_start(identity_id=...)",
                "3. Discover forms: browser_navigate() → browser_discover_forms()",
                "4. If git_repo_url set, run SAST pipeline:",
                "   a. sast_clone_repo() - clone source code",
                "   b. sast_index_repo() - index for code search",
                "   c. sast_scan_semgrep() - multi-language SAST",
                "   d. sast_scan_bandit() - Python security (skip if not Python)",
                "   e. sast_scan_gitleaks() - secret detection",
                "5. Build coverage matrix: coverage_init(base_url='https://target.com')",
                "6. Create hypotheses: wm_add_hypothesis() for each identified attack vector",
                "7. Map assets in world model: wm_add_endpoint() for each discovered endpoint"
            ],
            "common_mistakes": [
                "❌ Not running authenticated crawl - misses privileged endpoints",
                "❌ Calling coverage_init() before endpoints discovered (empty matrix)",
                "❌ Skipping openapi_parse even when /swagger.json exists",
                "❌ Not using coverage_discover() after authenticated crawl to extend matrix",
                "❌ Skipping SAST when git_repo_url is set - misses code-level vulnerabilities",
                "❌ Creating findings instead of hypotheses - hypotheses come BEFORE testing"
            ],
            "success_criteria": [
                "✓ All endpoints documented in world model (wm_query table='endpoints')",
                "✓ Coverage matrix initialized with endpoint × vuln_class cells",
                "✓ Hypotheses created for major attack vectors (auth bypass, IDOR, SQLi, XSS)",
                "✓ Forms and input vectors catalogued",
                "✓ If git_repo_url set: SAST scanners run (semgrep + gitleaks at minimum)"
            ]
        },
        3: {
            "current_phase": 3,
            "phase_name": "SAST Code Review",
            "recommended_tools": [
                {"tool": "sast_get_next_unverified", "priority": "critical", "reason": "Get next unverified SAST finding with function code for review"},
                {"tool": "sast_mark_verified", "priority": "critical", "reason": "Mark SAST finding as exploitable, informational, or false positive"},
                {"tool": "code_audit_enumerate", "priority": "critical", "reason": "MANDATORY: Queue all functions for security review (gate blocks without this)"},
                {"tool": "code_audit_get_next", "priority": "critical", "reason": "Get next unreviewed function with code and investigation guide"},
                {"tool": "code_audit_mark_reviewed", "priority": "critical", "reason": "Record verdict: safe/suspicious/vulnerable (vulnerable auto-creates finding)"},
                {"tool": "code_audit_progress", "priority": "high", "reason": "Track code audit completion percentage (gate requires 100%)"},
                {"tool": "sast_read_file", "priority": "high", "reason": "Read source files to understand code context around findings"},
                {"tool": "sast_search_code", "priority": "high", "reason": "Regex search across codebase to trace data flow"},
                {"tool": "sast_list_files", "priority": "medium", "reason": "List source files with priority scoring for manual review"},
                {"tool": "inject_payload", "priority": "medium", "reason": "Dynamically test suspicious code paths found during review"},
                {"tool": "inject_batch", "priority": "medium", "reason": "Batch test suspected injection points found in source code"},
                {"tool": "record_finding", "priority": "high", "reason": "Document code-level vulnerabilities found during review"}
            ],
            "next_actions": [
                "STEP 1: VERIFY SAST SCANNER FINDINGS (if scans ran in Phase 2):",
                "1. sast_get_next_unverified() - get next unverified SAST finding with function code",
                "2. Read the code: sast_read_file() to understand the vulnerable function and data flow",
                "3. Test dynamically: inject_payload/inject_batch to attempt exploitation",
                "4. sast_mark_verified(finding_id, exploitable=True/False, false_positive=True/False)",
                "5. Repeat until sast_get_next_unverified returns status='all_verified'",
                "",
                "STEP 2: FULL CODE AUDIT — MANDATORY when source code is indexed:",
                "⚠️ The phase gate BLOCKS advancement without code audit. Do NOT skip this step.",
                "1. code_audit_enumerate(risk_filter='high', max_functions=50) - queue functions for review",
                "   - Extracts ALL functions, scores risk (auth/sql/eval/crypto keywords → higher score)",
                "   - Auto-marks functions already covered by SAST findings as reviewed",
                "2. code_audit_get_next(risk_tier='critical') - get highest-risk unreviewed function",
                "   - Returns: function_code, investigation_guide, local_file_path, risk_score",
                "3. LLM reviews code using the 5-step investigation guide:",
                "   a. Input Validation - Are all inputs validated? Type/length/format checks?",
                "   b. SQL Injection - Parameterized queries? String concatenation in SQL?",
                "   c. Auth/Authz - Permissions checked? Hardcoded credentials?",
                "   d. Code Injection - eval()/exec()/subprocess with user input?",
                "   e. Data Exposure - Sensitive data logged? Verbose error messages?",
                "4. code_audit_mark_reviewed(queue_item_id, verdict, vuln_class, severity)",
                "   - safe: no issues, marks reviewed",
                "   - suspicious: needs dynamic testing (follow up with inject_payload/inject_batch)",
                "   - vulnerable: auto-creates finding card",
                "5. Repeat steps 2-4 until code_audit_get_next returns 'All functions reviewed!'",
                "6. code_audit_progress() - verify 100% completion before advancing",
                "",
                "STEP 3: DYNAMIC TESTING OF SUSPICIOUS FUNCTIONS:",
                "For functions marked 'suspicious' in Step 2:",
                "1. Use inject_payload/inject_batch to test the code path dynamically",
                "2. Update verdict to vulnerable or safe via code_audit_mark_reviewed"
            ],
            "common_mistakes": [
                "❌ ONLY verifying SAST tool findings without doing function-by-function code review",
                "❌ Skipping code_audit_enumerate - phase gate BLOCKS without it when source is indexed",
                "❌ Not reviewing ALL queued functions - code_audit_pct must reach 100%",
                "❌ Marking functions as 'safe' without reading the code - use the investigation guide",
                "❌ Not dynamically testing 'suspicious' verdicts - follow up with inject_payload/inject_batch",
                "❌ Skipping SAST verification - unverified SAST findings have no confidence rating",
                "❌ Rushing through code review - spend 2-5 minutes per function for critical/high risk"
            ],
            "success_criteria": [
                "✓ All SAST findings verified (sast_verified_pct = 100%)",
                "✓ code_audit_enumerate called (functions queued for review)",
                "✓ All queued functions reviewed (code_audit_pct = 100%)",
                "✓ Suspicious functions dynamically tested",
                "✓ All code-level vulnerabilities documented with record_finding",
                "✓ code_audit_progress shows 100% completion"
            ]
        },
        4: {
            "current_phase": 4,
            "phase_name": "Exploitation & Validation",
            "recommended_tools": [
                {"tool": "validate_repro", "priority": "critical", "reason": "Confirm findings are reproducible (3x minimum)"},
                {"tool": "validate_negative_control", "priority": "high", "reason": "Rule out false positives with benign input"},
                {"tool": "validate_cross_identity", "priority": "high", "reason": "Test authorization vulns across users"},
                {"tool": "validate_promote", "priority": "high", "reason": "Promote validated findings to confirmed"},
                {"tool": "sast_correlate", "priority": "medium", "reason": "Cross-reference SAST and DAST findings for confidence boost"},
                {"tool": "evidence_bundle", "priority": "high", "reason": "Package evidence for each confirmed finding"},
                {"tool": "poc_generate", "priority": "medium", "reason": "Generate reproduction scripts"},
                {"tool": "browser_screenshot", "priority": "low", "reason": "Visual evidence for XSS/visual bugs"}
            ],
            "next_actions": [
                "1. List all findings: list_cards(card_type='finding', status='potential')",
                "2. For each finding:",
                "   a. validate_repro(finding_id) - run 3+ times",
                "   b. validate_negative_control(finding_id) - test benign input",
                "   c. For authz bugs: validate_cross_identity(finding_id, identities=[...])",
                "   d. If all pass: validate_promote(finding_id)",
                "3. If SAST ran: sast_correlate() - match code-level findings with dynamic findings",
                "4. Create evidence bundles: evidence_bundle(finding_id)",
                "5. Add artifacts: evidence_add_artifact(bundle_id, type='request'|'response'|'screenshot', ...)",
                "6. Generate PoCs: poc_generate(finding_id)",
                "7. Calculate risk: risk_score(finding_id, cvss_vector='...')",
                "8. Export evidence: evidence_export(bundle_id, format='zip'|'json'|'sarif')"
            ],
            "common_mistakes": [
                "❌ Skipping validate_negative_control - leads to false positive findings",
                "❌ Not testing reproducibility enough (1-2x insufficient, need 3+ successes)",
                "❌ Promoting findings before validation complete",
                "❌ Forgetting to create evidence bundles before state changes",
                "❌ Not testing findings across multiple user identities (misses scope)"
            ],
            "success_criteria": [
                "✓ All potential findings validated or rejected",
                "✓ Confirmed findings have evidence bundles with artifacts",
                "✓ PoC scripts generated for reproducibility",
                "✓ Risk scores calculated for prioritization"
            ]
        }
    }

    return phase_guides.get(current_phase, phase_guides[1])


def _get_tool_dependencies() -> dict:
    """Tool sequencing rules and common multi-tool workflows."""
    return {
        "tool_chains": {
            "authenticated_recon": {
                "description": "Discover authenticated attack surface after obtaining credentials",
                "steps": [
                    {"step": 1, "tool": "credentials_add", "description": "Store discovered credentials with auto-generated placeholder"},
                    {"step": 2, "tool": "crawler_start", "params": {"identity_id": "<cred_id>"}, "description": "Crawl with authentication to discover privileged endpoints"},
                    {"step": 3, "tool": "coverage_discover", "params": {"since": "<crawl_timestamp>"}, "description": "Detect new endpoints and auto-extend coverage matrix"},
                    {"step": 4, "tool": "coverage_next", "params": {"limit": 20}, "description": "Get prioritized tests for newly discovered endpoints"}
                ]
            },
            "api_testing": {
                "description": "Systematic API testing from spec to complete coverage",
                "steps": [
                    {"step": 1, "tool": "http_send", "params": {"method": "GET", "url": "https://api.example.com/swagger.json"}, "description": "Fetch API specification"},
                    {"step": 2, "tool": "openapi_parse", "params": {"content": "<spec_content>"}, "description": "Parse OpenAPI specification"},
                    {"step": 3, "tool": "openapi_list_endpoints", "params": {"spec_id": "<id>"}, "description": "List all API endpoints"},
                    {"step": 4, "tool": "coverage_init", "params": {"base_url": "https://api.example.com"}, "description": "Build endpoint × vuln_class matrix"},
                    {"step": 5, "tool": "coverage_next", "params": {"limit": 10}, "description": "Get next priority tests"},
                    {"step": 6, "tool": "endpoint_execute_plan", "description": "Execute tests with exact tool_call from coverage_next"}
                ]
            },
            "finding_validation": {
                "description": "Complete validation and evidence collection for a potential finding",
                "steps": [
                    {"step": 1, "tool": "validate_repro", "params": {"finding_id": "<id>", "count": 3}, "description": "Reproduce 3x to confirm consistency"},
                    {"step": 2, "tool": "validate_negative_control", "params": {"finding_id": "<id>"}, "description": "Test with benign input to rule out false positive"},
                    {"step": 3, "tool": "validate_cross_identity", "params": {"finding_id": "<id>", "identities": [...]}, "description": "Test across user contexts if authorization-related"},
                    {"step": 4, "tool": "validate_promote", "params": {"finding_id": "<id>"}, "description": "Promote to confirmed status if validation passes"},
                    {"step": 5, "tool": "evidence_bundle", "params": {"finding_id": "<id>"}, "description": "Create evidence package"},
                    {"step": 6, "tool": "evidence_add_artifact", "description": "Add request/response/screenshot artifacts"},
                    {"step": 7, "tool": "poc_generate", "params": {"finding_id": "<id>"}, "description": "Generate curl + Python PoC scripts"}
                ]
            },
            "web_app_discovery": {
                "description": "Complete web application mapping from URL to full endpoint list",
                "steps": [
                    {"step": 1, "tool": "scope_validate_target", "params": {"target": "https://example.com"}, "description": "Verify target is in scope"},
                    {"step": 2, "tool": "browser_navigate", "params": {"url": "https://example.com"}, "description": "Load application in browser"},
                    {"step": 3, "tool": "browser_discover_forms", "description": "Find all HTML forms and input vectors"},
                    {"step": 4, "tool": "crawler_start", "params": {"start_url": "https://example.com", "max_pages": 200}, "description": "Crawl all pages"},
                    {"step": 5, "tool": "wm_query", "params": {"table": "endpoints"}, "description": "Query discovered endpoints from world model"},
                    {"step": 6, "tool": "coverage_init", "params": {"base_url": "https://example.com"}, "description": "Build coverage matrix"}
                ]
            },
            "sast_analysis": {
                "description": "Complete source code security analysis from clone to verification",
                "steps": [
                    {"step": 1, "tool": "sast_clone_repo", "description": "Clone git repository (reads git_repo_url from assessment)"},
                    {"step": 2, "tool": "sast_index_repo", "description": "Index source code for RAG semantic search"},
                    {"step": 3, "tool": "sast_scan_semgrep", "description": "Run Semgrep multi-language SAST scanner"},
                    {"step": 4, "tool": "sast_scan_bandit", "description": "Run Bandit Python security scanner (Python repos only)"},
                    {"step": 5, "tool": "sast_scan_gitleaks", "description": "Run Gitleaks secret detection scanner"},
                    {"step": 6, "tool": "sast_get_next_unverified", "description": "Get next unverified finding with function code extract"},
                    {"step": 7, "tool": "sast_mark_verified", "params": {"exploitable": True}, "description": "Mark finding verified after code review + dynamic testing"},
                    {"step": 8, "tool": "sast_correlate", "description": "Cross-reference SAST and DAST findings"}
                ]
            }
        },
        "prerequisites": {
            "ANY_TOOL_TO_TARGET": ["scope_validate_target"],
            "crawler_start": ["scope_validate_target"],
            "coverage_init": ["wm_query(table='endpoints') shows endpoints"],
            "endpoint_execute_plan": ["endpoint_probe", "coverage_init"],
            "validate_repro": ["add_card(card_type='finding')"],
            "nuclei_scan_template": ["scope_validate_target", "scope_check_budget"],
            "fuzz_parameter": ["scope_validate_target", "scope_check_budget"],
            "evidence_bundle": ["validate_promote"],
            "poc_generate": ["validate_promote"],
            "coverage_mark": ["coverage_next returned this cell_id"],
            "browser_discover_forms": ["browser_navigate"],
            "browser_test_xss": ["browser_navigate"],
            "openapi_list_endpoints": ["openapi_parse"],
            "openapi_get_endpoint": ["openapi_parse"],
            "openapi_get_schemas": ["openapi_parse"],
            "sast_index_repo": ["sast_clone_repo"],
            "sast_scan_semgrep": ["sast_clone_repo"],
            "sast_scan_bandit": ["sast_clone_repo"],
            "sast_scan_gitleaks": ["sast_clone_repo"],
            "sast_get_next_unverified": ["sast_scan_semgrep OR sast_scan_bandit OR sast_scan_gitleaks"],
            "sast_mark_verified": ["sast_get_next_unverified"],
            "sast_correlate": ["sast_mark_verified"]
        },
        "common_sequences": {
            "before_any_request": [
                "scope_validate_target(url) - MANDATORY",
                "scope_record_request() - for rate limiting",
                "scope_check_budget() - periodically"
            ],
            "discovering_new_endpoints": [
                "crawler_start() OR openapi_parse() OR execute('gobuster')",
                "coverage_discover(since=<timestamp>)",
                "coverage_next() - get new tests"
            ],
            "testing_an_endpoint": [
                "endpoint_probe() - gather context",
                "endpoint_execute_plan() - run tests",
                "coverage_mark() - record result"
            ],
            "confirming_a_finding": [
                "validate_repro() - reproduce 3x",
                "validate_negative_control() - rule out FP",
                "validate_promote() - confirm",
                "evidence_bundle() - package"
            ],
            "sast_workflow": [
                "sast_clone_repo() - clone source code (Phase 2)",
                "sast_index_repo() - index for search",
                "sast_scan_semgrep() + sast_scan_bandit() + sast_scan_gitleaks() - run scanners",
                "sast_get_next_unverified() → review code → test → sast_mark_verified() - verify each (Phase 3)",
                "sast_correlate() - match SAST ↔ DAST (Phase 4)"
            ]
        },
        "anti_patterns": [
            "❌ Calling coverage_next() but not calling coverage_mark() after testing",
            "❌ Running nuclei/fuzz without scope_validate_target() first",
            "❌ Creating findings before validation (should be observations first)",
            "❌ Using scan() when dedicated tools exist (use fuzz_parameter not execute('sqlmap'))",
            "❌ Not checking wm_recall() before re-running expensive scans",
            "❌ Calling endpoint_execute_plan() without endpoint_probe() first",
            "❌ Using execute() for HTTP requests (use http_send for budget tracking)"
        ]
    }


def _get_pentest_workflow() -> dict:
    """LLM-in-the-loop pentest workflow using the 8 new pentest tools."""
    return {
        "overview": "Manual security testing workflow where the LLM sees raw HTTP traffic, selects payloads, and controls the testing flow. Unlike automated tools that return pre-digested summaries, these tools expose complete request/response data for LLM reasoning.",
        "workflow": [
            {
                "step": 1,
                "tool": "discover_attack_surface",
                "description": "See what endpoints, parameters, and findings are known",
                "example": "discover_attack_surface()",
                "what_to_look_for": [
                    "Total endpoints discovered",
                    "Untested endpoints",
                    "Known parameters on each endpoint",
                    "Existing findings count",
                    "Assets in scope"
                ]
            },
            {
                "step": 2,
                "tool": "recon_endpoint",
                "description": "Send baseline request to each endpoint, examine full response",
                "example": "recon_endpoint(url='https://api.example.com/users', method='GET')",
                "what_to_look_for": [
                    "Response status and error messages",
                    "Reflected parameters in body",
                    "JSON structure or HTML templates",
                    "Technology signatures (framework errors, server headers)",
                    "Authentication requirements (401/403)",
                    "Rate limiting headers"
                ]
            },
            {
                "step": 3,
                "tool": "analyze_headers",
                "description": "Check security headers and cookie configuration",
                "example": "analyze_headers(url='https://example.com')",
                "what_to_look_for": [
                    "Missing security headers (CSP, HSTS, X-Frame-Options)",
                    "Cookies without HttpOnly or Secure flags",
                    "Permissive CORS (Access-Control-Allow-Origin: *)",
                    "Server version disclosure",
                    "Weak cache headers (Cache-Control: public on sensitive endpoints)"
                ]
            },
            {
                "step": 4,
                "tool": "get_test_payloads",
                "description": "Get payloads for suspected vulnerability classes",
                "example": "get_test_payloads(vuln_class='sqli_error', limit=10)",
                "decision_tree": {
                    "sql_error_in_response": ["sqli_error", "sqli_union", "sqli_blind_time"],
                    "template_rendering_detected": ["ssti"],
                    "parameter_reflected_in_html": ["xss_reflected", "xss_dom"],
                    "json_api_with_ids": ["idor", "nosql_injection", "mass_assignment"],
                    "file_upload_found": ["path_traversal", "command_injection"],
                    "external_url_parameter": ["ssrf", "open_redirect"],
                    "xml_content_type": ["xml_injection"],
                    "jwt_authentication": ["jwt_manipulation"]
                },
                "available_classes": [
                    "sqli_error", "sqli_blind_boolean", "sqli_blind_time", "sqli_union",
                    "xss_reflected", "xss_stored", "xss_dom",
                    "ssti", "ssrf", "path_traversal", "command_injection",
                    "ldap_injection", "xml_injection", "header_injection", "nosql_injection",
                    "idor", "auth_bypass", "jwt_manipulation", "cors_misconfig", "open_redirect",
                    "mass_assignment", "param_pollution", "rate_limit_bypass"
                ]
            },
            {
                "step": 5,
                "tool": "inject_batch",
                "description": "Sweep hardcoded payloads against parameter, compare to baseline",
                "example": "inject_batch(url='https://api.example.com/search', parameter='q', location='query', payloads=[...], include_baseline=true)",
                "what_to_look_for": [
                    "Status code changes (500 errors = likely SQLi/SSTI)",
                    "Response body length variance (significant diff = injection success)",
                    "Timing anomalies (>2x baseline = blind time-based injection)",
                    "Payloads reflected in response",
                    "Error messages revealing injection success"
                ],
                "best_practices": [
                    "Always include baseline for comparison",
                    "Start with small payload set (10-25) for quick triage",
                    "Cap max_batch_size to respect budget",
                    "Look for outliers in summary stats first, then examine individual responses"
                ]
            },
            {
                "step": 6,
                "tool": "inject_payload",
                "description": "Craft and send targeted payloads based on observed behavior",
                "example": "inject_payload(url='https://api.example.com/users', parameter='id', location='query', payload=\"1' OR '1'='1\")",
                "use_cases": [
                    "Test custom payloads crafted for specific framework/DB",
                    "Follow up on promising batch results with refined payloads",
                    "Test context-specific injection (JSON vs URL-encoded vs path)",
                    "Verify payload reflection and encoding behavior",
                    "Check for WAF bypass with obfuscated payloads"
                ],
                "injection_locations": {
                    "query": "URL query parameters (?id=PAYLOAD)",
                    "body": "JSON body {\"param\": \"PAYLOAD\"} or form data",
                    "path": "URL path segments /users/{id} where {id}=PAYLOAD",
                    "header": "Custom headers (X-Forwarded-For: PAYLOAD)",
                    "cookie": "Cookie values (session=PAYLOAD)"
                }
            },
            {
                "step": 7,
                "tool": "record_finding",
                "description": "Record confirmed vulnerabilities with evidence",
                "example": "record_finding(title='SQL Injection in /api/users', severity='high', url='https://api.example.com/users', vuln_class='sqli_error', parameter='id', evidence={...})",
                "when_to_use": [
                    "After confirming vulnerability through multiple payloads",
                    "When you have clear evidence (error message, reflected payload, status change)",
                    "When vulnerability is reproducible (not intermittent)",
                    "When you understand the root cause and impact"
                ],
                "evidence_structure": {
                    "request": "Full request that triggered vulnerability",
                    "response": "Full response showing vulnerability behavior",
                    "payload": "Exact payload that worked",
                    "description": "Explanation of how to reproduce and why it's vulnerable"
                },
                "severity_guidelines": {
                    "critical": "RCE, SQL injection with data exfil, auth bypass to admin",
                    "high": "XSS in admin panel, IDOR on sensitive data, SSRF to internal network",
                    "medium": "XSS in user context, CSRF, info disclosure",
                    "low": "Verbose errors, missing security headers",
                    "info": "Observations, potential misconfigurations"
                }
            },
            {
                "step": 8,
                "tool": "get_test_progress",
                "description": "Verify coverage completeness and testing status",
                "example": "get_test_progress()",
                "use_to_answer": [
                    "How many endpoints have I tested?",
                    "What's my coverage percentage?",
                    "How many findings by severity?",
                    "How much budget remains?",
                    "Which vuln classes are untested?",
                    "Which endpoints need more testing?"
                ],
                "success_criteria": [
                    "Coverage >80% for comprehensive assessment",
                    "All high-risk endpoints tested",
                    "Budget usage proportional to phase (40% recon, 25% mapping, 25% testing, 10% validation)",
                    "Findings validated and documented"
                ]
            }
        ],
        "complete_example": {
            "scenario": "Testing a REST API endpoint for SQL injection",
            "sequence": [
                {
                    "step": "1. Discover",
                    "call": "discover_attack_surface()",
                    "observe": "Found /api/users?id=1 endpoint with 'id' parameter"
                },
                {
                    "step": "2. Baseline",
                    "call": "recon_endpoint(url='https://api.example.com/users?id=1', method='GET')",
                    "observe": "Returns 200 OK with user JSON. Response time 145ms."
                },
                {
                    "step": "3. Get Payloads",
                    "call": "get_test_payloads(vuln_class='sqli_error', limit=20)",
                    "observe": "Retrieved 20 error-based SQL injection payloads"
                },
                {
                    "step": "4. Batch Test",
                    "call": "inject_batch(url='https://api.example.com/users', parameter='id', location='query', payloads=[...20 payloads...], include_baseline=true)",
                    "observe": "15 payloads → 200 OK, 3 payloads → 500 error, 2 payloads → timing anomaly (2.3x baseline). Status changes detected!"
                },
                {
                    "step": "5. Targeted Test",
                    "call": "inject_payload(url='https://api.example.com/users', parameter='id', location='query', payload=\"1' OR '1'='1\")",
                    "observe": "500 Internal Server Error. Response body contains: 'You have an error in your SQL syntax'. Confirmed SQLi!"
                },
                {
                    "step": "6. Record",
                    "call": "record_finding(title='SQL Injection in /api/users?id parameter', severity='high', url='https://api.example.com/users', vuln_class='sqli_error', parameter='id', evidence={request: {...}, response: {...}, payload: \"1' OR '1'='1\", description: 'Error-based SQLi confirmed via MySQL error message'})",
                    "observe": "Finding recorded with card_id and finding_id"
                },
                {
                    "step": "7. Progress",
                    "call": "get_test_progress()",
                    "observe": "1 endpoint tested, 1 high finding, 248 requests remaining"
                }
            ]
        },
        "tips": [
            "Pentest tools return raw HTTP data for YOUR reasoning. Use include_analysis=true ONLY for quick triage — default is false to avoid anchoring bias.",
            "Run analyze_headers first for automated security header checks, then reason independently from raw responses in other tools.",
            "Use recon_endpoint first to understand normal behavior before injecting",
            "Start with inject_batch for broad sweep, then inject_payload for targeted tests",
            "Always compare injected responses to baseline (status, length, timing)",
            "Look for error messages, SQL syntax errors, stack traces in responses — YOU analyze, not the tool",
            "Check if payloads are reflected HTML-encoded (XSS mitigation)",
            "Test all 5 injection locations (query, body, path, header, cookie)",
            "Record findings with complete evidence while memory is fresh",
            "Monitor budget with get_test_progress to avoid exhaustion"
        ],
        "vs_automated_tools": {
            "fuzz_endpoint": {
                "automated": "Sends payloads, returns vuln class verdicts (sqli: true/false)",
                "llm_pentest": "LLM sees every response, reasons about error patterns, crafts adaptive payloads"
            },
            "endpoint_probe": {
                "automated": "Returns 500-char response snippet",
                "llm_pentest": "recon_endpoint returns 8KB+ full response for complete analysis"
            },
            "testing_next": {
                "automated": "Returns pre-determined test plan with tool calls to execute",
                "llm_pentest": "LLM controls flow: sees response → decides next payload → injects → observes → repeats"
            }
        },
        "when_to_use_this_workflow": [
            "When automated testing (fuzz_endpoint, endpoint_execute_plan) returns false negatives",
            "When you need to understand WHY a test failed or succeeded",
            "When testing complex business logic that requires adaptive payloads",
            "When you want to craft context-specific exploits (not hardcoded payloads)",
            "When debugging an unclear vulnerability signal",
            "When the LLM needs to make decisions based on application behavior"
        ],
        "when_to_use_automated_tools": [
            "Phase 1-2 (Recon/Mapping): Use recon_pipeline_run, crawler_start for broad discovery",
            "Phase 3 (Testing): Use testing_build_matrix → testing_next for comprehensive coverage",
            "When you need to test 100+ endpoints quickly (batch automation)",
            "When vulnerability patterns are well-known (Nuclei templates, standard SQLi/XSS)",
            "When budget is limited (automated tools are more efficient)"
        ],
        "simple_vulnerability_checklist": {
            "description": "MANDATORY simple tests that must be performed in Phase 3 before advancing to Phase 4. These catch easy wins that automated scanning often misses. Assessment 4 found 24 findings; Assessments 10/11 found only 8 each because these basic tests were skipped.",
            "requirement": "Complete at least 6 of 8 tests before Phase 4. These are low-hanging fruit that yield high-impact findings.",
            "tests": [
                {
                    "name": "Security Headers",
                    "tool": "analyze_headers",
                    "priority": "CRITICAL",
                    "effort": "1-2 minutes",
                    "how": "Call analyze_headers on main URL and 2-3 key endpoints. Check response for missing headers.",
                    "what_to_check": [
                        "Content-Security-Policy (CSP) header present?",
                        "HTTP Strict-Transport-Security (HSTS) header present?",
                        "X-Content-Type-Options: nosniff present?",
                        "X-Frame-Options present?",
                        "Cookies have HttpOnly, Secure, and SameSite flags?"
                    ],
                    "record_if": "Missing CSP, HSTS, or insecure cookie flags. Severity: medium (missing CSP/HSTS), low (other headers).",
                    "typical_findings": "Missing CSP, missing HSTS, cookies without HttpOnly/Secure flags"
                },
                {
                    "name": "Weak Password Policy",
                    "tool": "inject_payload",
                    "priority": "HIGH",
                    "effort": "2-3 minutes",
                    "how": "If registration or password-change endpoint exists, try setting passwords: '123456', 'password', 'a', 'test'. Check if accepted.",
                    "what_to_check": [
                        "Are passwords shorter than 8 chars accepted?",
                        "Are common passwords ('password', '123456') accepted?",
                        "Is there a minimum complexity requirement (uppercase, numbers, symbols)?",
                        "Does the API return specific errors for weak passwords?"
                    ],
                    "record_if": "Passwords shorter than 8 chars OR common passwords accepted. Severity: medium.",
                    "typical_findings": "Weak password policy allowing '123456' or single-character passwords"
                },
                {
                    "name": "Username Enumeration",
                    "tool": "inject_batch",
                    "priority": "HIGH",
                    "effort": "3-5 minutes",
                    "how": "Send login requests with known-valid username (if available) and known-invalid username (e.g., 'nonexistent_user_xyz'). Use inject_batch with 2 requests. Compare response status, body length, and timing.",
                    "what_to_check": [
                        "Do valid and invalid usernames return different status codes (200 vs 404)?",
                        "Do valid and invalid usernames return different response bodies ('User not found' vs 'Invalid password')?",
                        "Do valid and invalid usernames have different response times (>100ms difference)?",
                        "Does the API reveal whether a username exists before checking the password?"
                    ],
                    "record_if": "Different responses for valid vs invalid usernames (status, body content, or timing). Severity: medium.",
                    "typical_findings": "Login endpoint returns 'User not found' for invalid users and 'Invalid password' for valid users"
                },
                {
                    "name": "CSRF Protection",
                    "tool": "recon_endpoint",
                    "priority": "MEDIUM",
                    "effort": "2-3 minutes",
                    "how": "For state-changing endpoints (POST/PUT/DELETE), send request WITHOUT any CSRF token or anti-CSRF header. Check if request succeeds.",
                    "what_to_check": [
                        "Do POST/PUT/DELETE endpoints require a CSRF token?",
                        "Do forms include hidden CSRF token fields?",
                        "Does the API use SameSite cookies as CSRF protection?",
                        "Can you perform state-changing actions without CSRF protection?"
                    ],
                    "record_if": "State-changing requests succeed without CSRF token. Severity: medium. NOTE: Exchange analyzer will auto-detect missing CSRF tokens in forms.",
                    "typical_findings": "POST /api/users endpoint accepts requests without CSRF token"
                },
                {
                    "name": "Default Credentials",
                    "tool": "inject_batch",
                    "priority": "HIGH",
                    "effort": "2-3 minutes",
                    "how": "Try default credential pairs on login endpoint: admin:admin, admin:password, admin:123456, test:test, root:root. Use inject_batch with 5 requests.",
                    "what_to_check": [
                        "Do any default credential pairs return 200 OK or a session token?",
                        "Are there error messages revealing valid usernames ('Invalid password for admin')?",
                        "Is there an admin or test account with default password?"
                    ],
                    "record_if": "Any default credential pair grants access (200 OK with session/token). Severity: critical.",
                    "typical_findings": "Admin account with password 'admin' grants full access"
                },
                {
                    "name": "Input Validation",
                    "tool": "inject_batch",
                    "priority": "MEDIUM",
                    "effort": "3-5 minutes",
                    "how": "Send boundary values to API parameters: empty string '', very long string (5000 chars), special chars '<>\"&', negative numbers -1, zero, null, undefined. Use inject_batch with 6-8 payloads.",
                    "what_to_check": [
                        "Does the API accept invalid input (negative IDs, empty required fields)?",
                        "Does the API return 500 errors or stack traces for invalid input?",
                        "Are there verbose error messages revealing internal logic?",
                        "Does the API validate data types (string vs int, email format)?"
                    ],
                    "record_if": "API accepts invalid input OR returns 500 errors with stack traces. Severity: low (weak validation), medium (error disclosure).",
                    "typical_findings": "API accepts negative user IDs, returns 500 error with stack trace for malformed input"
                },
                {
                    "name": "Rate Limiting",
                    "tool": "inject_batch",
                    "priority": "MEDIUM",
                    "effort": "2-3 minutes",
                    "how": "Send 20-30 rapid identical requests to login endpoint or sensitive endpoint. Use inject_batch with 25 requests. Check if any rate limiting applied (429 status, lockout, delay).",
                    "what_to_check": [
                        "Does the API return 429 Too Many Requests after N attempts?",
                        "Does response time increase after repeated requests (throttling)?",
                        "Is there account lockout after N failed login attempts?",
                        "Are there rate-limiting headers (X-RateLimit-Remaining)?"
                    ],
                    "record_if": "No rate limiting after 20+ rapid requests on sensitive endpoint. Severity: medium.",
                    "typical_findings": "Login endpoint accepts 1000+ login attempts with no rate limiting or lockout"
                },
                {
                    "name": "Error Handling",
                    "tool": "recon_endpoint",
                    "priority": "MEDIUM",
                    "effort": "2-3 minutes",
                    "how": "Request non-existent URLs (404), use invalid HTTP methods (OPTIONS on restricted endpoint), send malformed JSON. Check for stack traces, debug info, or detailed error messages.",
                    "what_to_check": [
                        "Do 404 pages reveal framework/version info (Django debug page, Spring error)?",
                        "Do 500 errors include stack traces with file paths?",
                        "Do error messages reveal internal logic ('Database connection failed at line 42 in auth.py')?",
                        "Is debug mode enabled in production (verbose errors)?"
                    ],
                    "record_if": "Stack traces, debug info, or detailed error messages exposed. Severity: medium (stack traces), low (verbose errors). NOTE: Exchange analyzer will auto-detect stack traces.",
                    "typical_findings": "404 page shows Django debug page with installed apps, 500 error shows Python traceback"
                }
            ],
            "completion_criteria": {
                "minimum_tests": 6,
                "recommended_tests": 8,
                "expected_findings": "3-8 findings from simple tests alone",
                "phase_gate": "Before advancing to Phase 4, verify at least 6 of 8 tests completed and findings recorded"
            },
            "why_these_matter": "Assessment 4 (manual testing) found weak passwords, username enumeration, missing headers, no rate limiting, and verbose errors. Assessments 10/11 (automated only) missed these because they require simple manual checks that automated tools skip. These tests take 15-20 minutes total and often yield 30-50% of total findings.",
            "integration_with_automated_testing": "Run these simple tests FIRST in Phase 3, then use testing_build_matrix for comprehensive coverage. Simple tests are high-yield and help you understand the application before deep testing."
        }
    }


def _get_sast_workflow() -> dict:
    """SAST (Static Application Security Testing) workflow guide.

    10-step workflow for source code analysis using 11 SAST tools.
    """
    return {
        "title": "SAST Workflow - Static Application Security Testing",
        "description": "Comprehensive source code analysis workflow combining automated scanners (Semgrep, Bandit, Gitleaks) with LLM-driven code review and RAG-based semantic search",
        "prerequisites": [
            "Assessment created with git_repo_url field set",
            "SAST tools installed in Kali container (semgrep, bandit, gitleaks)",
            "Shared volume mounted: /workspace/repos"
        ],
        "workflow_steps": [
            {
                "step": 1,
                "tool": "sast_clone_repo",
                "phase": "mapping",
                "description": "Clone git repository into assessment-scoped workspace",
                "required_params": ["repo_url (optional if assessment.git_repo_url set)"],
                "optional_params": ["branch (default: main)", "depth (default: 0 = full history)", "auth_token_env"],
                "returns": "repo_path, commit_sha, file_count, languages_detected, total_size_kb",
                "what_happens": "Clones repo to /workspace/repos/{assessment_id}/, detects languages, counts files",
                "error_handling": "Returns error if git not found (no mock mode). Rebuild Kali: docker compose build kali",
                "example": {
                    "call": "sast_clone_repo(branch='develop', depth=1)",
                    "use_case": "Shallow clone of develop branch for faster analysis"
                }
            },
            {
                "step": 2,
                "tool": "sast_index_repo",
                "phase": "mapping",
                "description": "Index all source files into wm_knowledge for RAG-based semantic search",
                "required_params": [],
                "optional_params": ["languages (filter by language)", "exclude_patterns (additional excludes)"],
                "returns": "files_indexed, total_size_kb, chunks_created, languages",
                "what_happens": "Reads all source files, generates embeddings, stores in wm_knowledge(category='source_code'). After indexing, use wm_recall(query='...', category='source_code') to find code by meaning.",
                "excludes_automatically": [".git", "node_modules", "vendor", "__pycache__", ".venv", "build", "dist"],
                "why_rag": "Enables semantic search: wm_recall(query='SQL query builder') finds database code across entire codebase in one query. Dramatically faster than iterating files manually.",
                "example": {
                    "call": "sast_index_repo(languages=['python', 'javascript'])",
                    "use_case": "Index only Python and JavaScript files for targeted review"
                }
            },
            {
                "step": 3,
                "tool": "sast_scan_semgrep",
                "phase": "assessment",
                "description": "Run Semgrep static analyzer with configurable rulesets",
                "required_params": [],
                "optional_params": ["rulesets (default: ['auto'])", "paths", "exclude_paths", "severity_filter", "max_findings (default: 200)"],
                "returns": "findings_count, findings (with file/line/code_snippet), languages_scanned, scan_time_ms",
                "what_happens": "Runs Semgrep scanner via Docker exec. Auto-persists high/critical findings to wm_findings + cards.",
                "recommended_rulesets": ["auto (recommended)", "p/owasp-top-10", "p/security-audit", "p/ci"],
                "finding_format": "source, rule_id, title, severity, file, line, code_snippet, vuln_class, cwe, remediation",
                "error_handling": "Returns error if semgrep not installed (no mock mode)",
                "example": {
                    "call": "sast_scan_semgrep(rulesets=['p/owasp-top-10'], severity_filter='ERROR')",
                    "use_case": "Scan for OWASP Top 10 issues with high confidence"
                }
            },
            {
                "step": 4,
                "tool": "sast_scan_bandit",
                "phase": "assessment",
                "description": "Run Bandit (Python-only) security scanner",
                "required_params": [],
                "optional_params": ["confidence (default: MEDIUM)", "severity (default: LOW)", "exclude_dirs"],
                "returns": "findings_count, findings, scan_time_ms",
                "what_happens": "Scans Python code for SQL injection, hardcoded passwords, shell injection, insecure crypto, etc. Returns informational message if no Python files found.",
                "python_only": "Gracefully skips if no .py files detected",
                "example": {
                    "call": "sast_scan_bandit(confidence='HIGH', severity='MEDIUM')",
                    "use_case": "High-confidence Python security issues only"
                }
            },
            {
                "step": 5,
                "tool": "sast_scan_gitleaks",
                "phase": "assessment",
                "description": "Run Gitleaks secret scanner to detect hardcoded secrets",
                "required_params": [],
                "optional_params": ["scan_history (default: false)", "max_findings (default: 100)"],
                "returns": "findings_count, findings (with redacted secrets), scan_time_ms",
                "what_happens": "Scans for API keys, passwords, tokens. Redacts secret values (shows first/last 4 chars only). Default: staged/unstaged only. Set scan_history=true for full git history.",
                "secret_redaction": "For safety, secret values are redacted. Example: 'sk_live_abcd...wxyz'",
                "example": {
                    "call": "sast_scan_gitleaks(scan_history=true)",
                    "use_case": "Scan full git history for accidentally committed secrets"
                }
            },
            {
                "step": 6,
                "tool": "sast_list_files",
                "phase": "assessment",
                "description": "List source files with priority scoring to guide LLM code review",
                "required_params": [],
                "optional_params": ["language", "pattern (glob)", "sort_by (default: priority)", "limit (default: 100)"],
                "returns": "files (with path, language, size, line_count, priority_score, priority_reasons), total_files, languages",
                "what_happens": "Lists files sorted by priority score (0-100). High-priority: auth/login (+30), database (+20), API routes (+15), config (+10). Low-priority: tests (-10), generated/vendor (-20).",
                "use_this_to": "Decide what to review first with sast_read_file. Start with priority_score >= 70.",
                "example": {
                    "call": "sast_list_files(language='python', sort_by='priority', limit=20)",
                    "use_case": "Get top 20 priority Python files for manual review"
                }
            },
            {
                "step": 7,
                "tool": "sast_read_file",
                "phase": "assessment",
                "description": "Read source file content for LLM manual code review",
                "required_params": ["file_path (relative within repo)"],
                "optional_params": ["start_line", "end_line", "max_lines (default: 500)"],
                "returns": "file_path, language, content, start_line, end_line, total_lines, truncated",
                "what_happens": "Reads file via Docker exec. Default 500-line limit for token budget. Validates no path traversal (rejects '..').",
                "review_workflow": "Use sast_list_files to get priority files, then sast_read_file to review them. Look for: SQL injection, XSS, hardcoded secrets, weak crypto, insecure deserialization.",
                "example": {
                    "call": "sast_read_file(file_path='app/views/auth.py', max_lines=300)",
                    "use_case": "Review authentication logic (300 lines max)"
                }
            },
            {
                "step": 8,
                "tool": "sast_search_code",
                "phase": "assessment",
                "description": "Regex search across codebase for pattern-based discovery",
                "required_params": ["pattern (regex)"],
                "optional_params": ["file_types (e.g., ['.py', '.js'])", "max_results (default: 50)"],
                "returns": "matches (with file, line, content), total_matches",
                "what_happens": "Runs grep -rn across repo. Use for pattern-based searches (SQL queries, crypto usage, dangerous functions).",
                "when_to_use": "Pattern searches: 'eval(', 'exec(', 'SELECT.*FROM', 'password\\s*=', 'api_key'. For semantic search (find by meaning), use wm_recall(category='source_code') instead.",
                "example": {
                    "call": "sast_search_code(pattern='eval\\\\(', file_types=['.py', '.js'])",
                    "use_case": "Find dangerous eval() calls in Python/JavaScript"
                }
            },
            {
                "step": 9,
                "tool": "sast_record_finding",
                "phase": "assessment",
                "description": "Record code-level security finding to world model + cards",
                "required_params": ["file_path", "line", "title", "severity"],
                "optional_params": ["vuln_class", "description", "code_snippet", "remediation", "confidence (default: 0.8)", "cwe"],
                "returns": "finding_id, card_id, success",
                "what_happens": "Creates card(type='finding') with SAST metadata: source='sast_llm_review', file, line, code_snippet, cwe. Also creates wm_finding.",
                "when_to_call": "After confirming vulnerability through code review. Don't record every scanner finding - only confirmed issues.",
                "example": {
                    "call": "sast_record_finding(file_path='auth.py', line=42, title='SQL Injection in login', severity='high', vuln_class='sqli_error', code_snippet='query = \"SELECT * FROM users WHERE username='\" + username + \"'\"', cwe='CWE-89')",
                    "use_case": "Record confirmed SQL injection vulnerability"
                }
            },
            {
                "step": 10,
                "tool": "sast_correlate",
                "phase": "exploitation",
                "description": "Cross-reference SAST findings with DAST findings to confirm code issues dynamically",
                "required_params": [],
                "optional_params": ["auto_correlate (default: true)"],
                "returns": "correlations, sast_only_count, dast_only_count, confirmed_count",
                "what_happens": "Queries all findings, splits by source (sast_* vs others). Correlates by: (a) vuln_class match, (b) endpoint↔file mapping, (c) parameter match. Creates wm_relationships(rel_type='confirms').",
                "why_correlate": "SAST-only findings may be false positives (dead code, unreachable paths). DAST confirmation proves real-world exploitability.",
                "verification_workflow": "For SAST-only findings: use inject_payload/inject_batch to dynamically test the code path and confirm exploitability.",
                "example": {
                    "call": "sast_correlate(auto_correlate=true)",
                    "use_case": "Cross-reference all SAST and DAST findings automatically"
                }
            },
            {
                "step": 11,
                "tool": "sast_get_progress",
                "phase": "reporting",
                "description": "Show SAST review progress and coverage",
                "required_params": [],
                "optional_params": [],
                "returns": "repo, scanners (semgrep/bandit/gitleaks counts), rag_index (files_indexed), findings (by_source, by_severity), correlations",
                "what_happens": "Queries world model for SAST findings, scanner results, RAG index status, and correlations.",
                "use_to_verify": "Coverage completeness: files indexed, scanners run, manual review progress, correlation status",
                "example": {
                    "call": "sast_get_progress()",
                    "use_case": "Verify SAST coverage before advancing to next phase"
                }
            }
        ],
        "rag_semantic_search": {
            "description": "After sast_index_repo, use wm_recall for semantic code search",
            "why": "Finds code by meaning, not just grep patterns. Dramatically faster than manual file iteration.",
            "examples": [
                {
                    "query": "authentication bypass check",
                    "call": "wm_recall(query='authentication bypass check', category='source_code')",
                    "returns": "auth.py, login_handler.py ranked by semantic similarity"
                },
                {
                    "query": "SQL query builder",
                    "call": "wm_recall(query='SQL query builder', category='source_code', tags=['python'])",
                    "returns": "Database access files in Python"
                },
                {
                    "query": "file upload handler",
                    "call": "wm_recall(query='file upload handler', category='source_code')",
                    "returns": "Upload processing code"
                }
            ],
            "when_to_use": "Finding specific code patterns across entire codebase. Alternative to sast_list_files + sast_read_file one-by-one."
        },
        "phase_integration": {
            "phase_2_mapping": [
                "sast_clone_repo - Clone target repository",
                "sast_index_repo - Index code into RAG for semantic search"
            ],
            "phase_3_assessment": [
                "sast_scan_semgrep - Run automated Semgrep scanner",
                "sast_scan_bandit - Run Bandit (Python only)",
                "sast_scan_gitleaks - Scan for hardcoded secrets",
                "sast_list_files - Get priority file list",
                "sast_read_file + sast_record_finding - LLM manual code review",
                "wm_recall(category='source_code') - Semantic search for patterns",
                "sast_search_code - Regex pattern search"
            ],
            "phase_4_exploitation": [
                "sast_correlate - Cross-reference SAST↔DAST findings",
                "inject_payload/inject_batch - Dynamically verify SAST-only findings"
            ],
            "phase_5_reporting": [
                "sast_get_progress - Verify coverage",
                "evidence_generate_report - SAST findings in SARIF with physicalLocation"
            ]
        },
        "vuln_class_mapping": {
            "description": "How SAST scanners map to AutoPentest vuln classes",
            "semgrep_rules": {
                "*.injection.sql*": "sqli_error",
                "*.security.audit.xss*": "xss_reflected",
                "*.injection.command*": "command_injection",
                "*.ssti*": "ssti",
                "*.ssrf*": "ssrf",
                "*.path-traversal*": "path_traversal",
                "*.crypto.*": "tls_config"
            },
            "bandit_rules": {
                "B608 (SQL injection)": "sqli_error",
                "B201 (Flask debug)": "misconfig",
                "B105 (Hardcoded password)": "info_leak_headers",
                "B602 (Shell injection)": "command_injection"
            },
            "gitleaks": "All secrets → info_leak_headers (CWE-798)",
            "unmapped": "generic_code_issue or misconfig"
        },
        "file_priority_guide": {
            "critical_priority": ["Routes/controllers/handlers/views (+30)", "Auth/login/session/token (+25)"],
            "high_priority": ["Database/ORM/query/model (+20)", "API definitions (+15)"],
            "medium_priority": ["Config/settings (+10)"],
            "low_priority": ["Tests (-10)", "Generated/vendor (-20)"]
        },
        "expected_outcomes": {
            "automated_scanners": "10-50 findings from Semgrep/Bandit/Gitleaks (many low severity)",
            "llm_manual_review": "5-15 high-value findings from reviewing priority files",
            "correlation": "30-50% of SAST findings confirmed by DAST testing",
            "total_time": "Phase 2 (clone+index): 5-10 min, Phase 3 (scan+review): 30-60 min"
        },
        "troubleshooting": {
            "tool_not_found": "Error: 'semgrep not found' → Rebuild Kali: docker compose build kali",
            "no_repo_cloned": "Error: 'No cloned repository found' → Call sast_clone_repo first",
            "path_traversal": "Error: 'Path traversal not allowed' → Don't use '..' in file_path",
            "no_python_files": "Message: 'No Python files found' → Bandit is Python-only, graceful skip",
            "empty_rag_index": "wm_recall returns nothing → Call sast_index_repo first to populate RAG"
        }
    }


def _get_autonomous_runbook() -> dict:
    """Complete 6-phase runbook for autonomous execution mode."""
    from tools_auto_run import RUNBOOK_STEPS

    phases_detail = {}
    for phase_num, steps in RUNBOOK_STEPS.items():
        phases_detail[f"phase_{phase_num}"] = {
            "phase_number": phase_num,
            "name": _get_phase_name(phase_num),
            "steps": [
                {
                    "id": s["id"],
                    "name": s["name"],
                    "tool": s["tool"],
                    "args_template": s["args_template"],
                    "description": s["description"],
                    "approval_required": s["approval_required"],
                    "on_failure": s["on_failure"],
                    "is_gate_check": s.get("is_gate_check", False),
                    "condition": s.get("condition"),
                    "repeatable": s.get("repeatable", False),
                    "repeat_until": s.get("repeat_until", {}),
                }
                for s in steps
            ],
            "gate_requirements": _get_phase_gate_requirements(phase_num),
        }

    approval_points = []
    for phase_num, steps in RUNBOOK_STEPS.items():
        for step in steps:
            if step.get("approval_required", False):
                approval_points.append({
                    "step_id": step["id"],
                    "step_name": step["name"],
                    "tool": step["tool"],
                    "reason": _get_approval_reason(step),
                })

    return {
        "title": "Autonomous Pentest Runbook",
        "description": "Complete 6-phase runbook for automated penetration testing",
        "overview": {
            "how_it_works": [
                "1. Call orchestration_auto_run() with action='next' to get next step",
                "2. Execute the returned tool call",
                "3. Report back with action='step_done' and result_summary",
                "4. Repeat until status='complete'",
                "5. At approval gates: relay message to user, wait for confirmation, then proceed"
            ],
            "actions": {
                "next": "Get next pending step (returns tool call instructions)",
                "step_done": "Mark current step completed and get next step",
                "step_failed": "Handle step failure per on_failure policy (retry/skip/halt)",
                "status": "Get runbook progress (phase, step, % complete)",
                "reset": "Reset all steps to pending (start over)"
            },
            "state_persistence": "State stored in wm_plans table with title='Autonomous Runbook Execution'",
        },
        "usage_examples": {
            "start_runbook": {
                "call": "orchestration_auto_run()",
                "returns": {
                    "status": "ok",
                    "next_step": {
                        "id": "1.1",
                        "name": "Verify scope allowlist",
                        "tool": "scope_get_allowlist",
                        "args": {},
                    },
                    "instructions": "Call scope_get_allowlist() and report back with step_done"
                }
            },
            "complete_step": {
                "call": "orchestration_auto_run(action='step_done', result_summary='Found 5 allowed domains')",
                "returns": {
                    "status": "ok",
                    "next_step": {
                        "id": "1.2",
                        "name": "Subdomain enumeration",
                        "tool": "subdomain_enum",
                        "args": {"domain": "example.com"},
                    }
                }
            },
            "check_progress": {
                "call": "orchestration_auto_run(action='status')",
                "returns": {
                    "status": "ok",
                    "runbook": {
                        "phase": 2,
                        "step": 3,
                        "progress_pct": 35,
                        "steps_done": 11,
                        "steps_total": 31
                    }
                }
            },
            "handle_failure": {
                "call": "orchestration_auto_run(action='step_failed', error='Target unreachable')",
                "returns": {
                    "status": "ok",
                    "message": "Step 1.3 failed. Skipping...",
                    "next_step": {"id": "1.4", "tool": "tech_detection"}
                }
            },
            "reset_runbook": {
                "call": "orchestration_auto_run(action='reset')",
                "returns": {
                    "status": "ok",
                    "message": "All steps reset to pending",
                    "steps_total": 40
                },
                "when_to_use": "After fixing critical errors (e.g., scope misconfiguration) or to restart assessment from beginning"
            }
        },
        "approval_points": approval_points,
        "phases": phases_detail,
        "argument_resolution": {
            "description": "Template variables in args_template are resolved dynamically",
            "variables": {
                "{target_domain}": "Extracted from current_base_url (e.g., example.com)",
                "{base_url}": "Current assessment base URL (from mcp_service)",
                "{next_unvalidated_finding}": "First finding with status='potential'",
                "{next_validated_finding}": "First finding with status='validated'",
                "{next_confirmed_finding}": "First finding with status='confirmed'",
            },
            "error_handling": "If variable cannot be resolved, step returns error with explanation"
        },
        "repeatable_steps": {
            "description": "Some steps repeat until conditions met (e.g., testing_next until coverage ≥25%)",
            "examples": [
                {
                    "step_id": "4.5",
                    "tool": "testing_next",
                    "repeat_until": {"coverage_pct": 25, "findings_count": 3},
                    "behavior": "Keeps returning testing_next until both coverage ≥25% AND findings ≥3"
                },
                {
                    "step_id": "5.2",
                    "tool": "validate_repro",
                    "repeat_until": {"validated_findings_count": 1},
                    "behavior": "Keeps validating findings until at least 1 is validated"
                }
            ]
        },
        "gate_checks": {
            "description": "Steps with is_gate_check=True verify phase requirements before advancing",
            "behavior": [
                "Gate check step calls orchestration_status() to verify metrics",
                "Result stored in step's result field",
                "Next step (advance) has condition='gates_met'",
                "If gates not met, advance step returns status='blocked' with unmet_conditions",
                "LLM must complete required work to meet gates, then retry"
            ]
        },
        "failure_policies": {
            "retry": "Keep step in_progress, return same step for retry (e.g., testing_next can retry on error)",
            "skip": "Mark step done, advance to next (e.g., optional ssl_analysis can be skipped)",
            "halt": "Mark runbook failed, stop execution (e.g., critical scope_get_allowlist must succeed)"
        }
    }


def _get_phase_name(phase_num: int) -> str:
    """Get phase name by number."""
    phase_names = {
        1: "Reconnaissance",
        2: "Mapping & Enumeration",
        3: "SAST Code Review",
        4: "Vulnerability Assessment",
        5: "Exploitation",
        6: "Post-Exploitation & Reporting",
    }
    return phase_names.get(phase_num, f"Phase {phase_num}")


def _get_phase_gate_requirements(phase_num: int) -> dict:
    """Get gate requirements for phase transition."""
    gates = {
        1: {"assets_count": 3, "endpoints_count": 5},
        2: {"endpoints_count": 5, "findings_count": 1},
        3: {"sast_verified_pct": 100, "code_audit_queued": 1, "code_audit_pct": 100},
        4: {"confirmed_hypotheses_count": 1, "findings_count": 3, "coverage_pct": 25},
        5: {"confirmed_findings_count": 1},
        6: {},  # No gates for final phase
    }
    return gates.get(phase_num, {})


def _get_approval_reason(step: dict) -> str:
    """Get reason why step requires approval."""
    tool = step["tool"]
    reasons = {
        "testing_build_matrix": "Generates test matrix with potentially hundreds of test cells. Review coverage strategy before proceeding.",
        "orchestration_advance": "Phase transition. Review phase completeness and gate metrics before advancing.",
        "validate_repro": "Validation attempts modify application state. Confirm findings are worth validating.",
        "evidence_generate_report": "Final report generation. Verify all findings documented before creating deliverable.",
    }
    return reasons.get(tool, "Human oversight required for this critical operation.")


def _get_code_audit_workflow() -> dict:
    """Return code audit workflow guide."""
    return {
        "title": "LLM-Powered Full Code Audit Workflow",
        "description": "Systematic function-level security review of entire codebase using 4 code audit tools",
        "purpose": "Discover vulnerabilities missed by SAST scanners (40-60% of real issues) - business logic flaws, complex data flows, context-dependent bugs",
        "when_to_use": "Phase 3 (SAST Code Review) after running SAST scanners, or when you have source code access and want comprehensive security coverage",
        "prerequisites": [
            "Assessment has git_repo_url configured",
            "Source code cloned via sast_clone_repo",
            "Source code indexed via sast_index_repo"
        ],
        "workflow": {
            "step_1": {
                "name": "Enumerate Functions",
                "tool": "code_audit_enumerate",
                "purpose": "Parse all indexed source files, extract ALL functions with line numbers, create prioritized audit queue",
                "parameters": {
                    "risk_filter": "Optional: 'critical'|'high'|'medium'|'low' - only enumerate files at/above this tier",
                    "max_functions": "Optional (default 0=unlimited): cap on queue size for focused review",
                    "include_tests": "Optional (default false): include test functions in audit queue"
                },
                "returns": "functions_queued, by_risk_tier (critical/high/medium/low), covered_by_sast (auto-marked as reviewed), files_processed",
                "strategy": "Start with risk_filter='critical' for focused audit, or omit for comprehensive review",
                "example": "code_audit_enumerate(risk_filter='high', max_functions=50)"
            },
            "step_2": {
                "name": "Get Next Function",
                "tool": "code_audit_get_next",
                "purpose": "Get next unreviewed function, prioritized by risk score (highest first)",
                "parameters": {
                    "risk_tier": "Optional: filter to specific tier ('critical'|'high'|'medium'|'low')"
                },
                "returns": "queue_item_id, function_name, file, risk_score, risk_tier, function_code, local_file_path, investigation_guide, remaining_unreviewed, remaining_by_tier",
                "strategy": "Review highest-risk functions first (critical → high → medium → low)",
                "example": "code_audit_get_next(risk_tier='critical')"
            },
            "step_3": {
                "name": "LLM Security Review",
                "actor": "LLM (you)",
                "purpose": "Analyze function code for security vulnerabilities using investigation_guide checklist",
                "checklist": [
                    "Input Validation: Are all inputs validated? Type/length/format checks? Whitelist of allowed values?",
                    "SQL Injection: Parameterized queries used? Input sanitized? String concatenation in SQL?",
                    "Authentication/Authorization: Permissions checked? User identity verified? Hardcoded credentials?",
                    "Code Injection: eval()/exec()/subprocess calls? User input to code execution? Template injections?",
                    "Data Exposure: Sensitive data logged? Verbose error messages? Data encrypted?"
                ],
                "verdict_options": {
                    "safe": "No security issues found",
                    "suspicious": "Potential issue, needs dynamic testing (e.g., inject_payload to confirm SQLi)",
                    "vulnerable": "Confirmed security vulnerability"
                }
            },
            "step_4": {
                "name": "Mark Function Reviewed",
                "tool": "code_audit_mark_reviewed",
                "purpose": "Record LLM verdict after security review",
                "parameters": {
                    "queue_item_id": "Required: ID from code_audit_get_next",
                    "verdict": "Required: 'safe'|'suspicious'|'vulnerable'",
                    "vuln_class": "Optional (for vulnerable): e.g., 'sql_injection', 'xss', 'auth_bypass'",
                    "severity": "Optional (for vulnerable): 'info'|'low'|'medium'|'high'|'critical'",
                    "description": "Optional: Detailed vulnerability description",
                    "code_snippet": "Optional: Vulnerable code snippet",
                    "remediation": "Optional: Remediation guidance",
                    "cwe": "Optional: CWE ID (e.g., 'CWE-89')",
                    "suspicious_reason": "Optional (for suspicious): Reason for suspicion"
                },
                "actions": {
                    "vulnerable": "Creates finding card automatically via safe_add_card",
                    "suspicious": "Stores reason in metadata for follow-up dynamic testing",
                    "safe": "Just marks reviewed, no further action"
                },
                "example": "code_audit_mark_reviewed(queue_item_id='abc123', verdict='vulnerable', vuln_class='sql_injection', severity='high', description='Unsanitized input in SQL query')"
            },
            "step_5": {
                "name": "Check Progress",
                "tool": "code_audit_progress",
                "purpose": "Show audit completion stats",
                "returns": "total, reviewed, unreviewed, completion_pct, by_verdict (safe/suspicious/vulnerable/covered_by_sast), by_risk_tier, findings_created",
                "strategy": "Call periodically to track progress. Aim for 100% completion_pct before advancing to Phase 4.",
                "example": "code_audit_progress()"
            },
            "step_6": {
                "name": "Follow-Up on Suspicious Functions",
                "purpose": "Test suspicious functions dynamically using pentest tools",
                "tools": ["inject_payload", "inject_batch", "recon_endpoint"],
                "strategy": "Query wm_knowledge(category='code_audit_queue', metadata.verdict='suspicious') to find functions needing dynamic testing. Test with payloads, then update verdict to vulnerable/safe."
            }
        },
        "risk_tier_descriptions": {
            "critical": "≥80 score: Auth/crypto/payment functions, eval/exec calls, hardcoded secrets",
            "high": "≥60 score: Database access, API routes, session management, file operations",
            "medium": "≥40 score: Config, validators, business logic",
            "low": "<40 score: Utilities, helpers, static content"
        },
        "phase_gate": {
            "requirement": "min_code_audit_pct: 100 AND min_code_audit_queued: 1",
            "blocking_behavior": "When source code is indexed, gate BLOCKS unless code_audit_enumerate has been called (queued ≥1) AND all queued functions are reviewed (100%). If no source code is indexed, gate auto-passes.",
            "how_to_satisfy": "Call code_audit_enumerate to queue functions, then code_audit_get_next + code_audit_mark_reviewed until all reviewed"
        },
        "expected_outcome": "10-20% of functions flagged as suspicious or vulnerable. Dynamic testing of suspicious functions yields 40-60% more findings than SAST alone.",
        "time_estimate": {
            "enumerate": "1-2 minutes (local processing)",
            "review_per_function": "2-5 minutes (LLM analysis + checklist)",
            "total_for_50_functions": "100-250 minutes (1.5-4 hours)",
            "strategy": "Use risk_filter to focus on critical/high functions first"
        },
        "integration_with_sast": {
            "auto_skip": "Functions covered by SAST findings (line overlap) are auto-marked as reviewed with verdict='covered_by_sast'",
            "focus": "Code audit focuses on functions NOT flagged by SAST scanners - the 40-60% of vulnerabilities SAST misses"
        }
    }
