"""
Endpoint Analysis Tools - endpoint_probe, endpoint_analyze_exchange, endpoint_execute_plan.

LLM-driven endpoint analysis with deterministic test execution.
Phase 1: Probe gathers context. Phase 2: LLM analyzes and plans.
Phase 3: Engine executes library payloads. Phase 4: LLM crafts adaptive payloads.
Phase 5: Engine executes follow-up payloads.
"""

from __future__ import annotations

import json
import logging
from typing import Any, Dict, List

from mcp.types import Tool, TextContent

logger = logging.getLogger("autopentest-mcp")


from lib.tool_helpers import _json_content, _error_content, _get_db


def _detect_db_type(error_indicators: List[Dict]) -> str:
    """Detect database type from error indicator patterns."""
    for ind in error_indicators:
        pattern = ind.get("pattern", "").lower()
        if any(kw in pattern for kw in ("mysql", "mariadb")):
            return "mysql"
        if any(kw in pattern for kw in ("postgresql", "pg_")):
            return "postgresql"
        if any(kw in pattern for kw in ("mssql", "sql server")):
            return "mssql"
        if "oracle" in pattern:
            return "oracle"
        if "sqlite" in pattern:
            return "sqlite"
    return "generic"


async def _upsert_endpoint(
    db: Any, endpoint_id: str, url: str, method: str, analysis: Dict
) -> None:
    """Save or update endpoint in wm_endpoints with discovered parameters."""
    if not endpoint_id:
        return

    try:
        existing = await db.get_by_id("endpoints", endpoint_id)
        if existing:
            merged_params = existing.get("parameters", {})
            merged_params.update(analysis.get("parameters", {}))
            merged_meta = existing.get("metadata", {})
            merged_meta["response_indicators"] = analysis.get("response_indicators", {})
            await db.update_endpoint(
                endpoint_id=endpoint_id,
                parameters=merged_params,
                metadata=merged_meta,
            )
    except Exception as exc:
        logger.debug("Failed to upsert endpoint: %s", exc)


# ---------------------------------------------------------------------------
# Instructions template for LLM
# ---------------------------------------------------------------------------

_ANALYSIS_INSTRUCTIONS = (
    "Analyze the probe results above (request/response, reflected params, "
    "RAG context, graph context, response indicators). For each parameter, "
    "determine which vuln classes to test and why. Fill in "
    "test_plan_template.tests[] with entries like:\n"
    '  {"vuln_class": "sqli", "parameter": "q", "location": "query", '
    '"use_library": true}\n'
    "Set use_library=true for the first pass. After execution, review "
    "suspicious_signals and craft targeted payloads for a second pass by "
    "calling endpoint_execute_plan again with explicit payloads[]."
)


# ---------------------------------------------------------------------------
# Tool definitions
# ---------------------------------------------------------------------------

def get_endpoint_analysis_tools() -> List[Tool]:
    """Return the three endpoint analysis tools."""
    return [
        # 1 ---- endpoint_probe -----------------------------------------------
        Tool(
            name="endpoint_probe",
            description=(
                "Probe an endpoint with baseline request to gather comprehensive context before testing: reflected "
                "parameters (XSS/injection candidates), error patterns (database errors, stack traces), tech "
                "indicators (server versions, frameworks), security headers (CSP, CORS, X-Frame-Options), RAG "
                "knowledge (similar vulnerabilities found elsewhere), graph neighbors (related endpoints/hypotheses), "
                "existing coverage (already tested vuln classes). Returns structured analysis plus empty "
                "test_plan_template for LLM to complete with testing strategy. Phase 1 of endpoint analysis workflow. "

                "**When to use:** Phase 3 (Assessment) before endpoint_execute_plan() to understand endpoint behavior "
                "and plan testing. Essential first step in LLM-driven endpoint testing workflow. Use after discovering "
                "endpoint via crawler_start(), openapi_list_endpoints(), or coverage_next(). Cheaper than blind fuzzing - "
                "gathers intelligence before testing. Use to detect error-prone parameters (reflected in response), "
                "identify database type (error messages), understand parameter handling (JSON/form-data/query). "

                "**Dependencies:** Requires scope_validate_target() for URL. Optional endpoint_id from "
                "wm_add_endpoint() enables graph context (related hypotheses, prior findings). Follow with LLM "
                "analysis of returned context to create test plan, then endpoint_execute_plan() to execute tests. "
                "Use wm_recall() to search RAG knowledge for similar endpoints tested previously. "

                "**Budget impact:** LOW - single baseline request (1 HTTP request) plus local analysis (RAG query, "
                "graph traversal, parameter reflection detection). Completes <2 seconds. Response classifier runs "
                "deterministically on baseline to detect error patterns, reflected parameters, tech indicators. "

                "**Failure modes:** Network errors return minimal analysis (no response to analyze, but still returns "
                "test_plan_template). 403/401 responses indicate authentication required (note in analysis). Timeout "
                "suggests slow endpoint (increase timeout parameter or skip). Empty response body limits analysis "
                "(can't detect reflections/errors). "

                "**Risk level:** CAUTION - Sends one legitimate request to endpoint. May trigger rate limiting if "
                "called repeatedly. Request logged by target application. No attack payloads sent - pure reconnaissance. "

                "**Returns:** Comprehensive analysis JSON with: request (sent), response (received), reflected_params[] "
                "(parameters echoed in response body/headers - XSS candidates), error_indicators[] (database errors, "
                "stack traces, error codes), tech_indicators[] (server headers, framework hints), security_headers "
                "(CSP, CORS policies), rag_context (similar endpoints, prior vulns), graph_context (neighbors, "
                "hypotheses), coverage_status (vuln classes already tested), test_plan_template (empty tests[] for "
                "LLM to fill). Use analysis to inform intelligent test plan creation."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "url": {
                        "type": "string",
                        "description": "Full URL of the endpoint to probe",
                    },
                    "method": {
                        "type": "string",
                        "description": "HTTP method (GET, POST, etc.)",
                        "default": "GET",
                    },
                    "headers": {
                        "type": "object",
                        "description": "Optional request headers",
                    },
                    "body": {
                        "type": "string",
                        "description": "Optional request body (for POST/PUT)",
                    },
                    "endpoint_id": {
                        "type": "string",
                        "description": "Optional world model endpoint ID for graph context",
                    },
                },
                "required": ["url"],
            },
        ),

        # 2 ---- endpoint_analyze_exchange ------------------------------------
        Tool(
            name="endpoint_analyze_exchange",
            description=(
                "Analyze an existing HTTP request/response pair offline without sending new request. Useful for "
                "analyzing proxy traffic captures (Burp Suite history), http_send() results, or crawler discoveries. "
                "Runs same analysis as endpoint_probe() (reflected params, error patterns, tech indicators, security "
                "headers) but on provided exchange instead of live request. Returns test_plan_template for LLM "
                "to fill. Alternative to endpoint_probe() when you already have request/response data. "

                "**When to use:** Phase 3 (Assessment) when you have request/response data from other sources: "
                "Burp Suite proxy history (interesting exchanges found manually), http_send() custom requests "
                "(you sent request with specific headers/payload), crawler_results() discoveries (endpoints found "
                "during crawl), browser interactions (browser_get_state() captured exchanges). Use instead of "
                "endpoint_probe() to avoid duplicate requests when you have fresh data. "

                "**Dependencies:** Requires complete request/response objects with method, url, status (minimum). "
                "Optional endpoint_id from wm_add_endpoint() enables graph context. No network interaction - "
                "purely analyzes provided data. Follow with endpoint_execute_plan() to test based on analysis. "

                "**Budget impact:** ZERO - no network requests, pure local analysis of provided exchange. Response "
                "classifier, reflection detector, error pattern matcher run on response body/headers. Completes <500ms. "

                "**Failure modes:** Incomplete request/response objects return partial analysis (e.g., missing body "
                "means no reflection detection). Invalid status code (not 1-599) causes validation error. Malformed "
                "headers object (not key-value pairs) returns minimal analysis. "

                "**Risk level:** SAFE - no network activity, purely analyzes existing data. Does not send requests "
                "or interact with target. Offline analysis tool. "

                "**Returns:** Same comprehensive analysis as endpoint_probe(): reflected_params[], error_indicators[], "
                "tech_indicators[], security_headers, rag_context, graph_context, coverage_status, "
                "test_plan_template. Use to analyze interesting traffic without re-sending requests. Ideal for "
                "analyzing manual testing discoveries or proxy captures before automated testing."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "request": {
                        "type": "object",
                        "description": "Request object: {method, url, headers, body}",
                        "properties": {
                            "method": {"type": "string"},
                            "url": {"type": "string"},
                            "headers": {"type": "object"},
                            "body": {"type": "string"},
                        },
                        "required": ["method", "url"],
                    },
                    "response": {
                        "type": "object",
                        "description": "Response object: {status, headers, body, timing}",
                        "properties": {
                            "status": {"type": "integer"},
                            "headers": {"type": "object"},
                            "body": {"type": "string"},
                            "timing": {"type": "object"},
                        },
                        "required": ["status"],
                    },
                    "endpoint_id": {
                        "type": "string",
                        "description": "Optional world model endpoint ID for graph context",
                    },
                },
                "required": ["request", "response"],
            },
        ),

        # 3 ---- endpoint_execute_plan ----------------------------------------
        Tool(
            name="endpoint_execute_plan",
            description=(
                "Execute structured endpoint test plan with library payloads or LLM-crafted payloads. Tests specific "
                "parameters for specific vuln classes (sqli, xss, ssrf, path_traversal, etc.) using deterministic "
                "payload execution. Loads curated payloads from vulnerability-specific libraries when use_library=true "
                "(recommended first pass), or uses explicit payloads[] for targeted follow-up. Tracks budget per "
                "(vuln_class, parameter) pair to prevent over-testing. Returns suspicious_signals for adaptive "
                "LLM-driven follow-up. Phase 3 of endpoint analysis workflow. HIGH RISK - sends attack payloads. "

                "**When to use:** Phase 3 (Assessment) after endpoint_probe() gathers context and LLM creates test "
                "plan. Essential second step in LLM-driven testing workflow. Two-pass strategy: (1) use_library=true "
                "for broad initial testing with curated payloads (~20/vuln class), (2) review suspicious_signals, "
                "craft targeted payloads[], call again with use_library=false for adaptive follow-up. Use after "
                "coverage_next() provides exact tool invocation for coverage cells. "

                "**Dependencies:** Requires test_plan object with base_url, method, tests[] array. Get test plan by: "
                "(1) endpoint_probe() + LLM analysis, (2) coverage_next() exact tool_call, (3) manual construction "
                "from openapi_get_endpoint(). Requires scope_validate_target() for base_url. Use max_payloads to "
                "control budget (default 20 per vuln_class/parameter pair). Follow suspicious results with "
                "validate_repro() for confirmation. "

                "**Budget impact:** MEDIUM to HIGH - depends on test plan size. Formula: Σ(vuln_class tests × "
                "payloads_per_test). 3 parameters × 2 vuln classes × 20 payloads = 120 requests. use_library=true "
                "loads ~20-100 payloads per vuln class (sqli_db=100, xss=50, ssrf=30). Explicit payloads[] = exact "
                "count. Budget tracked per (vuln_class, parameter) - won't re-test if already tested. "

                "**Failure modes:** WAF blocks (403/429) - try evasion payloads or reduce aggressiveness. Parameter "
                "location errors (query vs body) - verify with endpoint_probe() first. Timeout on time-based tests - "
                "increase timeout or skip blind vuln classes. False positives common - use validate_negative_control() "
                "to confirm. Budget exhausted mid-plan returns partial results. "

                "**Risk level:** HIGH RISK - Sends actual attack payloads (SQL injection, XSS, command injection, "
                "path traversal). May: corrupt data (successful SQLi), trigger errors (application crashes), cause "
                "DoS (resource-intensive payloads), trigger IDS/WAF alerts, get IP banned. Only use on authorized "
                "targets. Some payloads destructive (SQL DROP, DELETE). "

                "**Returns:** Comprehensive test results with: tests_executed (count), results_by_parameter[] (per "
                "parameter: vuln_class, payloads_tested, vulnerable_count, passed_count, error_count, "
                "suspicious_signals[] (payloads with interesting responses for follow-up)), suspicious_responses[] "
                "(full request/response for manual review), classification_summary (vulnerable/passed/error counts), "
                "coverage_updated (coverage_mark() called automatically). Use suspicious_signals to craft targeted "
                "payloads for second pass. Vulnerable classifications trigger add_card() automatically."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "test_plan": {
                        "type": "object",
                        "description": (
                            "Test plan with endpoint_id, base_url, method, path, "
                            "headers, base_body, max_payloads, and tests[] array"
                        ),
                        "properties": {
                            "endpoint_id": {"type": "string"},
                            "base_url": {"type": "string"},
                            "method": {"type": "string"},
                            "path": {"type": "string"},
                            "headers": {"type": "object"},
                            "base_body": {"type": "string"},
                            "max_payloads": {
                                "type": "integer",
                                "description": "Max payloads per (vuln_class, parameter), default 20",
                            },
                            "tests": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "vuln_class": {"type": "string"},
                                        "parameter": {"type": "string"},
                                        "location": {
                                            "type": "string",
                                            "enum": ["query", "body", "path", "header", "cookie"],
                                        },
                                        "use_library": {"type": "boolean"},
                                        "payloads": {
                                            "type": "array",
                                            "items": {"type": "string"},
                                        },
                                        "max_payloads": {"type": "integer"},
                                    },
                                    "required": ["vuln_class", "parameter", "location"],
                                },
                            },
                        },
                        "required": ["base_url", "method", "tests"],
                    },
                    "payload_type": {
                        "type": "string",
                        "enum": ["hardcoded", "llm_generated", "targeted"],
                        "description": "Type of payloads being executed (for decision tree tracking)",
                        "default": "hardcoded",
                    },
                },
                "required": ["test_plan"],
            },
        ),
    ]


# ---------------------------------------------------------------------------
# Tool handler
# ---------------------------------------------------------------------------

async def handle_endpoint_analysis_tool(
    name: str, arguments: dict, mcp_service: Any
) -> List[TextContent]:
    """Handle endpoint_probe, endpoint_analyze_exchange, endpoint_execute_plan."""

    if name == "endpoint_probe":
        return await _handle_endpoint_probe(arguments, mcp_service)
    elif name == "endpoint_analyze_exchange":
        return await _handle_endpoint_analyze_exchange(arguments, mcp_service)
    elif name == "endpoint_execute_plan":
        return await _handle_endpoint_execute_plan(arguments, mcp_service)
    else:
        return _error_content(f"Unknown tool: {name}")


# ---------------------------------------------------------------------------
# endpoint_probe
# ---------------------------------------------------------------------------

async def _handle_endpoint_probe(
    args: dict, mcp_service: Any
) -> List[TextContent]:
    """Probe an endpoint: send baseline, gather context, return analysis."""
    url = args.get("url")
    if not url:
        return _error_content("url is required")

    method = (args.get("method") or "GET").upper()
    headers = args.get("headers") or {}
    body = args.get("body") or ""
    endpoint_id = args.get("endpoint_id") or ""

    try:
        # Get HTTP client and DB
        http_client = _get_http_client(mcp_service)
        db = await _get_db(mcp_service)

        # Send baseline request
        result = await http_client.send({
            "method": method,
            "url": url,
            "headers": headers,
            "body": body if body else None,
        })

        if not result.get("success"):
            return _error_content(
                f"Baseline request failed: {result.get('error', {}).get('message', 'unknown')}"
            )

        request_data = result["request"]
        request_data["body"] = body
        response_data = result["response"]

        # Run analysis
        return await _build_analysis_response(
            request_data, response_data, endpoint_id, db, url, method, headers, body
        )

    except Exception as exc:
        logger.error("endpoint_probe error: %s", exc, exc_info=True)
        return _error_content(str(exc))


# ---------------------------------------------------------------------------
# endpoint_analyze_exchange
# ---------------------------------------------------------------------------

async def _handle_endpoint_analyze_exchange(
    args: dict, mcp_service: Any
) -> List[TextContent]:
    """Analyze an existing request/response pair."""
    request_data = args.get("request")
    response_data = args.get("response")
    if not request_data or not response_data:
        return _error_content("Both request and response are required")

    endpoint_id = args.get("endpoint_id") or ""
    url = request_data.get("url", "")
    method = (request_data.get("method") or "GET").upper()
    headers = request_data.get("headers") or {}
    body = request_data.get("body") or ""

    try:
        db = await _get_db(mcp_service)

        return await _build_analysis_response(
            request_data, response_data, endpoint_id, db, url, method, headers, body
        )

    except Exception as exc:
        logger.error("endpoint_analyze_exchange error: %s", exc, exc_info=True)
        return _error_content(str(exc))


# ---------------------------------------------------------------------------
# endpoint_execute_plan
# ---------------------------------------------------------------------------

async def _handle_endpoint_execute_plan(
    args: dict, mcp_service: Any
) -> List[TextContent]:
    """Execute a test plan using the TestPlanExecutor."""
    test_plan = args.get("test_plan")
    if not test_plan:
        return _error_content("test_plan is required")

    tests = test_plan.get("tests")
    if not tests:
        return _error_content("test_plan.tests[] is required and must not be empty")

    try:
        http_client = _get_http_client(mcp_service)
        db = await _get_db(mcp_service)

        from lib.test_plan_executor import TestPlanExecutor
        # Round 11 Fix 1C: Pass mcp_service for card bridging
        executor = TestPlanExecutor(http_client=http_client, db=db, mcp_service=mcp_service)

        results = await executor.execute(test_plan)

        # Record test attempts in orchestrator for decision tree state tracking
        try:
            from lib.test_orchestrator import get_orchestrator
            assessment_id = mcp_service.current_assessment_id
            base_url = test_plan.get("base_url") or "unknown"
            if assessment_id:
                orchestrator = get_orchestrator(assessment_id, base_url)
                payload_type = args.get("payload_type", "hardcoded")
                endpoint_id = test_plan.get("endpoint_id", "")
                for test_result in results.get("results_by_test", []):
                    orchestrator.record_test_attempt(
                        vuln_spec_id=test_result.get("vuln_class", ""),
                        endpoint_id=endpoint_id,
                        parameter=test_result.get("parameter"),
                        payload_type=payload_type,
                    )
        except Exception:
            pass  # Never let tracking break execution

        return _json_content({"success": True, **results})

    except Exception as exc:
        logger.error("endpoint_execute_plan error: %s", exc, exc_info=True)
        return _error_content(str(exc))


# ---------------------------------------------------------------------------
# Shared analysis builder
# ---------------------------------------------------------------------------

async def _build_analysis_response(
    request_data: Dict,
    response_data: Dict,
    endpoint_id: str,
    db: Any,
    url: str,
    method: str,
    headers: Dict,
    body: str,
) -> List[TextContent]:
    """Build the full analysis response shared by probe and analyze_exchange."""
    from urllib.parse import urlparse
    from lib.response_analyzer import analyze_response

    # Run deterministic analysis
    analysis = analyze_response(request_data, response_data)

    # Save/update endpoint in world model with discovered parameters
    await _upsert_endpoint(db, endpoint_id, url, method, analysis)

    # RAG context
    rag_context = {"relevant_knowledge": []}
    try:
        parsed = urlparse(url)
        path = parsed.path or "/"
        query = f"{method} {path}"
        knowledge = await db.recall_knowledge(query=query, limit=5)
        target_knowledge = await db.recall_knowledge(target=url, limit=3)
        # Deduplicate
        seen_ids = set()
        combined = []
        for k in knowledge + target_knowledge:
            kid = k.get("id")
            if kid not in seen_ids:
                seen_ids.add(kid)
                combined.append({
                    "id": kid,
                    "title": k.get("title", ""),
                    "category": k.get("category", ""),
                    "target": k.get("target", ""),
                })
        rag_context["relevant_knowledge"] = combined
    except Exception as exc:
        logger.debug("RAG context lookup failed: %s", exc)

    # Graph context
    graph_context = {"neighbors": {}, "existing_findings": [], "existing_coverage": []}
    if endpoint_id:
        try:
            neighbors = await db.get_neighbors(
                entity_type="endpoint", entity_id=endpoint_id, depth=2
            )
            graph_context["neighbors"] = neighbors

            # Look for existing findings linked to this endpoint
            findings_rels = await db.query_relationships(
                source_type="endpoint", source_id=endpoint_id,
                rel_type="has_finding"
            )
            for rel in findings_rels:
                finding = await db.get_by_id("findings", rel["target_id"])
                if finding:
                    graph_context["existing_findings"].append({
                        "id": finding["id"],
                        "title": finding.get("title", ""),
                        "severity": finding.get("severity", ""),
                        "status": finding.get("status", ""),
                    })

            # Look for existing coverage
            coverage_rows = await db._fetchall(
                "SELECT * FROM wm_coverage_matrix WHERE assessment_id = $1 AND endpoint_id = $2",
                (db._assessment_id, endpoint_id),
            )
            for row in coverage_rows:
                d = dict(row)
                graph_context["existing_coverage"].append({
                    "vuln_class": d.get("vuln_class", ""),
                    "parameter": d.get("parameter", ""),
                    "status": d.get("status", ""),
                    "priority": d.get("priority", 50),
                })
        except Exception as exc:
            logger.debug("Graph context lookup failed: %s", exc)

    # Build test plan template
    parsed = urlparse(url)
    base_url = f"{parsed.scheme}://{parsed.netloc}" if parsed.scheme else ""

    # Detect db_type from error indicators
    response_indicators = analysis.get("response_indicators", {})
    db_type = _detect_db_type(response_indicators.get("error_indicators", []))

    test_plan_template = {
        "endpoint_id": endpoint_id,
        "base_url": base_url,
        "method": method,
        "path": parsed.path or "/",
        "headers": headers,
        "base_body": body,
        "max_payloads": 20,
        "endpoint_context": {
            "db_type": db_type,
            "response_type": response_indicators.get("response_type", ""),
            "tech_stack": response_indicators.get("server_tech", ""),
            "reflected_params": analysis.get("reflected_params", []),
        },
        "tests": [],
    }

    # Probe response
    probe_info = {
        "request": {
            "method": method,
            "url": url,
            "headers": headers,
        },
        "response": {
            "status": response_data.get("status"),
            "status_text": response_data.get("status_text", ""),
            "headers": response_data.get("headers", {}),
            "body_preview": (response_data.get("body") or "")[:500],
            "body_length": len(response_data.get("body") or ""),
            "timing": response_data.get("timing", {}),
        },
    }

    # Exchange-level analysis (static header/body/cookie checks)
    exchange_analysis_result = {"recommended_tests": [], "priority_adjustments": {},
                                "risk_signals": [], "detected_technologies": [],
                                "parameter_recommendations": {}}
    try:
        from lib.exchange_analyzer import get_exchange_analyzer
        ea = get_exchange_analyzer()
        ea_result = ea.analyze(
            request={"method": method, "url": url, "headers": headers,
                     "parameters": [{"name": p, "type": source}
                                    for source, params in analysis.get("parameters", {}).items()
                                    for p in (params if isinstance(params, list) else params.keys())]},
            response={"status": response_data.get("status", 200),
                      "headers": response_data.get("headers", {}),
                      "body": (response_data.get("body") or "")[:4096]},
        )
        exchange_analysis_result = {
            "recommended_tests": list(dict.fromkeys(ea_result.recommended_tests)),
            "priority_adjustments": ea_result.priority_adjustments,
            "risk_signals": ea_result.risk_signals,
            "detected_technologies": ea_result.detected_technologies,
            "parameter_recommendations": ea_result.parameter_recommendations,
        }
    except Exception as exc:
        logger.debug("Exchange analysis failed: %s", exc)

    # Checklist-based applicability (42-class vuln specs)
    checklist_recommendations = []
    try:
        from lib.vuln_checklist import get_vuln_checklist
        checklist = get_vuln_checklist()
        endpoint_meta = {
            "method": method,
            "path": parsed.path or "/",
            "parameters": [{"name": p, "type": source}
                           for source, params in analysis.get("parameters", {}).items()
                           for p in (params if isinstance(params, list) else params.keys())],
            "requires_auth": bool(headers.get("Authorization") or headers.get("Cookie")),
            "content_type": response_data.get("headers", {}).get("content-type", ""),
        }
        applicable_specs = checklist.get_applicable_specs(endpoint_meta)
        checklist_recommendations = [
            {"id": s.id, "name": s.name, "category": s.category,
             "tool_name": s.tool_name, "parameter_bound": s.parameter_bound,
             "severity_range": s.severity_range, "description": s.description}
            for s in applicable_specs
        ]
    except Exception as exc:
        logger.debug("Checklist lookup failed: %s", exc)

    return _json_content({
        "success": True,
        "probe": probe_info,
        "analysis": analysis,
        "exchange_analysis": exchange_analysis_result,
        "checklist_recommendations": checklist_recommendations,
        "rag_context": rag_context,
        "graph_context": graph_context,
        "test_plan_template": test_plan_template,
        "instructions": _ANALYSIS_INSTRUCTIONS,
    })


# ---------------------------------------------------------------------------
# Service helpers
# ---------------------------------------------------------------------------

def _get_http_client(mcp_service: Any) -> Any:
    """Get or create the HttpClient from the MCP service."""
    # Guard against missing assessment context
    if not mcp_service or not mcp_service.current_assessment_id:
        raise ValueError("No active assessment. Call load_assessment first.")

    # The HTTP client is created per-assessment with cache invalidation
    cache_key = mcp_service.current_assessment_id
    if (hasattr(mcp_service, '_endpoint_analysis_http_client') and
        hasattr(mcp_service, '_endpoint_analysis_cache_key') and
        mcp_service._endpoint_analysis_cache_key == cache_key and
        mcp_service._endpoint_analysis_http_client is not None):
        return mcp_service._endpoint_analysis_http_client

    # Build exchange logger callback for audit trail
    async def _log_exchange(request, response, correlation_ids, timing):
        if hasattr(mcp_service, 'activity_logger') and mcp_service.activity_logger:
            await mcp_service.activity_logger.log_http_exchange(
                url=request.get("url", ""),
                method=request.get("method", ""),
                status_code=response.get("status", 0),
                request_headers=request.get("headers", {}),
                response_headers=response.get("headers", {}),
                response_body_preview=(response.get("body") or "")[:2048],
                correlation_id=correlation_ids.get("request_id", ""),
                timing_ms=timing.get("duration_ms", 0),
            )

    # Use per-assessment scope via scope_provider (same pattern as tools_http.py)
    from lib.http_client import HttpClient
    client = HttpClient(
        config={
            "engagement_id": str(mcp_service.current_assessment_id or "endpoint-analysis"),
            "proxy_url": None,
            "max_rps": 10.0,
            "max_concurrent": 5,
            "default_timeout": 15000,
            "max_total_requests": 5000,
        },
        assessment_id=mcp_service.current_assessment_id,
        scope_provider=mcp_service.scope_provider,
        exchange_logger=_log_exchange,
    )
    mcp_service._endpoint_analysis_http_client = client
    mcp_service._endpoint_analysis_cache_key = cache_key
    return client


# _get_db imported from lib.tool_helpers above
