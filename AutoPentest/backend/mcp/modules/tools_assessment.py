"""
Assessment Management Tools - load_assessment, update_phase,
orchestration_status, orchestration_advance.
"""
import json
import logging
from typing import Any, List

from mcp.types import Tool, TextContent

logger = logging.getLogger("autopentest-mcp")


def get_assessment_tools() -> List[Tool]:
    return [
        Tool(
            name="load_assessment",
            description=(
                "Load an existing assessment to begin work. Returns comprehensive context including scope, "
                "target domains/IPs, workspace structure, existing findings/observations/info cards, recon data, "
                "phase content, and recent command history. IMPORTANT: USER creates assessments via the web "
                "interface - you cannot create new assessments, only load existing ones. "

                "**When to use:** REQUIRED as the first tool call at the start of every session. All other tools "
                "depend on an assessment being loaded to function. Use skip_data=false (default) for full context "
                "on initial load, skip_data=true for quick session reloads without re-displaying all data. "

                "**Dependencies:** NONE - this is the entry point tool. Requires user to have created assessment "
                "via web interface first. Enables all 97 other tools once loaded. Follow with orchestration_status() "
                "to see current phase and progress, scope_get_allowlist() to verify targets, list_recon() or "
                "list_cards() to review existing discoveries. "

                "**Budget impact:** LOW - local database query, no network requests. Loads assessment metadata, "
                "cards, recon data, and command history. Completes in <500ms for typical assessments (<100 findings). "
                "May take 1-2 seconds for very large assessments (>500 cards + extensive recon data). "

                "**Failure modes:** 'Assessment not found' if name doesn't match existing assessment (check spelling, "
                "ask user for correct name). Assessment names are case-sensitive. Empty assessment returns minimal "
                "context (expected for new assessments - proceed with recon). Workspace not created yet is normal "
                "for fresh assessments. "

                "**Risk level:** SAFE - read-only operation loading local data. No network activity or target "
                "interaction. Sets internal state (current_assessment_id) used by all subsequent tool calls. "

                "**Returns:** Full assessment context formatted as markdown: client/scope/limitations, target "
                "domains/IPs, workspace structure tree, context documents list (user-provided files), phase content "
                "(5 phases), all cards grouped by type/severity (findings/observations/infos), recon data summary "
                "(endpoints/technologies/subdomains), recent command history. Use this context to understand what's "
                "been discovered, what phase you're in, and what to do next."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "Name of the assessment to load"
                    },
                    "skip_data": {
                        "type": "boolean",
                        "description": "If true, only reloads assessment without returning any context.",
                        "default": False
                    }
                },
                "required": ["name"]
            }
        ),
        Tool(
            name="update_phase",
            description=(
                "Update or append content to a specific phase section in the assessment report. Supports full "
                "markdown formatting for rich documentation. Use to document discoveries, methodologies, findings "
                "summaries, and next steps for each phase of the pentest. Content is preserved across sessions. "

                "**When to use:** Throughout testing to document phase progress and findings. Call at phase milestones: "
                "Phase 1 after recon (subdomains, services, technologies found), Phase 2 after mapping (endpoints, "
                "forms, attack surface documented), Phase 3 during assessment (testing approach, initial findings), "
                "Phase 4 during exploitation (successful exploits, impact demonstrated), Phase 5 for final reporting "
                "(executive summary, recommendations). Update incrementally as you discover new information. "

                "**Dependencies:** Requires load_assessment() first. No follow-up required - content saved to database "
                "and visible in web interface. Can call multiple times per phase to append/update content. Use with "
                "orchestration_status() to confirm current phase before documenting. "

                "**Budget impact:** LOW - local database write, no network requests. Stores markdown content in "
                "assessment sections table. Completes <50ms regardless of content size. Content length unlimited. "

                "**Failure modes:** 'No assessment loaded' if load_assessment() not called first. Invalid phase_number "
                "(must be 1.0-5.0) returns error. Content replaces existing phase content (not appended) - read "
                "existing content first if you want to preserve it. Empty content allowed (clears phase section). "

                "**Risk level:** SAFE - local documentation tool, no network activity or target interaction. Creates "
                "permanent record visible to user in web interface and final report. "

                "**Returns:** Simple confirmation 'Phase {N} updated'. Content is persisted to database and immediately "
                "visible in assessment report. Use for documenting methodology, findings summaries, screenshots "
                "(as markdown links), command outputs, and narrative of testing activities."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "phase_number": {
                        "type": "number",
                        "description": "Phase number (1.0, 2.0, 3.0, etc.)"
                    },
                    "title": {
                        "type": "string",
                        "description": "Section title"
                    },
                    "content": {
                        "type": "string",
                        "description": "Phase content (markdown supported)"
                    }
                },
                "required": ["phase_number", "content"]
            }
        ),

        # ---- orchestration_status -----------------------------------------
        Tool(
            name="orchestration_status",
            description=(
                "Get current assessment phase, comprehensive metrics (assets, endpoints, findings counts), gate "
                "conditions for advancing to the next phase, and overall progress percentage. Provides structured "
                "view of testing progress and readiness for phase transitions. Phases: 1=Recon, 2=Mapping, "
                "3=Vuln Assessment, 4=Exploitation, 5=Reporting. "

                "**When to use:** Call periodically (every 20-30 tool calls) to check progress and phase readiness. "
                "Use after major discoveries (found many subdomains, completed crawl, discovered critical finding) "
                "to see if metrics meet gate conditions for phase transition. Call before orchestration_advance() "
                "to verify you meet gate requirements. Use to decide what to do next: if asset count low, do more "
                "recon; if endpoint count low, crawl more; if finding count low, test more. "

                "**Dependencies:** Requires load_assessment() first. No follow-up required unless you want to advance "
                "phases (use orchestration_advance). Works with world model data - more accurate after using wm_* "
                "tools (wm_add_asset, wm_add_endpoint, wm_add_finding) instead of legacy recon data. "

                "**Budget impact:** LOW - local database aggregation query across world model tables. Counts assets, "
                "endpoints, hypotheses, findings. Completes <200ms for typical assessments (<500 world model entities). "

                "**Failure modes:** 'No assessment loaded' if load_assessment() not called first. Empty metrics "
                "(all zeros) indicates no world model data yet - expected early in assessment, proceed with recon "
                "and use wm_add_* tools. Phase stuck at 1? Add assets. Stuck at 2? Add endpoints. Stuck at 3? "
                "Create hypotheses and findings. "

                "**Risk level:** SAFE - read-only query of local world model database. No network activity or target "
                "interaction. Returns JSON with current state and gate conditions. "

                "**Returns:** JSON with current_phase (1-5), phase_name, progress_pct (0-100), metrics (assets, "
                "endpoints, hypotheses, findings), and next_gates array (conditions needed to advance). Example: "
                "{current_phase: 2, phase_name: 'Mapping', progress_pct: 45, metrics: {assets: 15, endpoints: 8, "
                "findings: 0}, next_gates: [{condition: 'min_endpoints', required: 10, current: 8, met: false}]}. "
                "Use to guide testing strategy and phase progression decisions."
            ),
            inputSchema={
                "type": "object",
                "properties": {},
            },
        ),

        # ---- orchestration_advance ----------------------------------------
        Tool(
            name="orchestration_advance",
            description=(
                "Request a phase transition to advance the assessment to the next phase. Validates gate conditions "
                "(minimum assets, endpoints, hypotheses, or findings) before allowing the transition. Gates ensure "
                "proper pentest methodology: adequate recon before mapping, sufficient mapping before assessment, "
                "validated findings before reporting. Use force=true to skip validation (not recommended). "

                "**When to use:** After completing phase objectives and orchestration_status() shows gate conditions "
                "met. Typical progression: Phase 1→2 after discovering attack surface (assets, subdomains, services), "
                "Phase 2→3 after mapping all endpoints (crawl, API spec parsing, coverage matrix built), Phase 3→4 "
                "after testing produces findings (vulnerabilities confirmed), Phase 4→5 after exploitation and "
                "validation complete (evidence collected, risk assessed). DO NOT advance prematurely - incomplete "
                "phases lead to missed vulnerabilities. "

                "**Dependencies:** Requires load_assessment() first. MUST call orchestration_status() before advancing "
                "to verify gate conditions met. If gates not met, do more work in current phase: Phase 1 needs "
                "subdomain_enum, scan, tech_detection; Phase 2 needs crawler_start, coverage_init; Phase 3 needs "
                "endpoint_execute_plan, auth_diff_test; Phase 4 needs validate_repro, evidence_bundle. "

                "**Budget impact:** LOW - local database write to update phase metadata. Also calls update_phase() "
                "internally to document transition. Completes <100ms. No network requests. "

                "**Failure modes:** 'Gate conditions not met' if metrics insufficient (message shows which conditions "
                "failed - add more assets/endpoints/findings as needed). 'Invalid target_phase' if not 1-5 or trying "
                "to skip phases (can only advance +1 at a time, or use force=true). 'No assessment loaded' if "
                "load_assessment() not called. force=true always succeeds but may indicate incomplete methodology. "

                "**Risk level:** SAFE - local state management only, no network activity or target interaction. "
                "Changes assessment phase metadata visible in web interface. Affects tool recommendations in "
                "workflow-guide resource. "

                "**Returns:** JSON with success boolean, new_phase number, message, and gates_met array showing which "
                "conditions passed/failed. Example: {success: true, new_phase: 3, message: 'Advanced to Phase 3', "
                "gates_met: [{condition: 'min_endpoints', met: true}]}. On failure: {success: false, error: 'Gate "
                "conditions not met', gates_met: [{condition: 'min_findings', required: 5, current: 2, met: false}]}. "
                "Use success to confirm transition, review gates_met to understand what's missing."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "target_phase": {
                        "type": "integer",
                        "minimum": 1,
                        "maximum": 5,
                        "description": "Target phase number (1-5)",
                    },
                    "force": {
                        "type": "boolean",
                        "description": "Override gate conditions (default: false)",
                    },
                },
                "required": ["target_phase"],
            },
        ),
    ]


async def handle_assessment_tool(name: str, arguments: dict, mcp_service) -> List[TextContent]:
    if name == "load_assessment":
        return await _handle_load_assessment(arguments, mcp_service)
    elif name == "update_phase":
        return await _handle_update_phase(arguments, mcp_service)
    elif name == "orchestration_status":
        return await _handle_orchestration_status(arguments, mcp_service)
    elif name == "orchestration_advance":
        return await _handle_orchestration_advance(arguments, mcp_service)
    return [TextContent(type="text", text=f"Unknown assessment tool: {name}")]


async def _handle_load_assessment(arguments: dict, mcp_service) -> List[TextContent]:
    """Handle load_assessment - Load and display full assessment data"""
    assessment_name = arguments["name"]
    skip_data = arguments.get("skip_data", False)

    assessment = await mcp_service.get_assessment_by_name(assessment_name)

    if not assessment:
        return [TextContent(
            type="text",
            text=f"Assessment '{assessment_name}' not found. USER must create it via the interface first."
        )]

    mcp_service.current_assessment_id = assessment["id"]
    mcp_service.current_assessment_name = assessment["name"]

    if skip_data:
        return [TextContent(
            type="text",
            text=f"Assessment '{assessment_name}' loaded successfully."
        )]

    full_data = await mcp_service.get_assessment_full_data(assessment["id"])
    assessment_data = full_data['assessment']

    response = f"**Assessment Loaded: {assessment_data['name']}**\n\n"
    response += "## State of Work\n"
    response += f"**Client:** {assessment_data.get('client_name', 'N/A')}\n"
    response += f"**Environment:** {assessment_data.get('environment', 'non_specifie')}\n"
    response += f"**Scope:** {assessment_data.get('scope', 'N/A')}\n"
    response += f"**Limitations:** {assessment_data.get('limitations', 'N/A')}\n\n"

    response += "## Basic Information\n"
    response += f"**Target Domains:** {', '.join(assessment_data.get('target_domains', [])) or 'N/A'}\n"
    response += f"**IP Scopes:** {', '.join(assessment_data.get('ip_scopes', [])) or 'N/A'}\n\n"

    response += f"**Workspace:** `{assessment_data.get('workspace_path', 'Not created')}`\n\n"

    # Generate workspace tree structure and list context documents
    if assessment_data.get('workspace_path') and assessment_data.get('container_name'):
        try:
            from utils.tree_generator import generate_workspace_tree, get_context_files_list
            import logging
            log = logging.getLogger("autopentest-mcp")

            tree_structure = await generate_workspace_tree(
                container_name=assessment_data.get('container_name'),
                workspace_path=assessment_data.get('workspace_path'),
                max_depth=2
            )

            response += "## Workspace Structure\n\n"
            response += "```\n"
            response += tree_structure
            response += "\n```\n\n"

            context_files = await get_context_files_list(
                container_name=assessment_data.get('container_name'),
                workspace_path=assessment_data.get('workspace_path')
            )

            if context_files:
                response += "## Context Documents Provided by User\n\n"
                response += "The following documents are available in `/context` for additional context:\n\n"
                for file_info in context_files:
                    response += f"- `{file_info['filename']}` ({file_info['size_human']}) - {file_info['type']}\n"
                    response += f"  Path: `{file_info['path']}`\n"
                response += "\n**Note:** You can read these files using the `execute()` tool with commands like `cat` or `less`.\n\n"
        except Exception as e:
            pass

    # Add sections information
    sections = full_data.get('sections', [])
    if sections:
        response += "## Assessment Phases\n"
        for section in sections:
            phase_num = section.get('section_type', '').replace('phase_', '')
            if phase_num.isdigit():
                phase_names = {
                    '1': 'Reconnaissance',
                    '2': 'Mapping & Enumeration',
                    '3': 'Vulnerability Assessment',
                    '4': 'Exploitation',
                    '5': 'Post-Exploitation & Reporting'
                }
                phase_name = phase_names.get(phase_num, f'Phase {phase_num}')
                content = section.get('content', '')
                if content:
                    response += f"**Phase {phase_num} - {phase_name}:**\n{content}\n\n"
                else:
                    response += f"**Phase {phase_num} - {phase_name}:** No content yet\n"

    # Add all cards with full details
    cards = full_data.get('cards', [])
    if cards:
        cards = [c for c in cards if c.get('status') != 'false_positive']
        findings = [c for c in cards if c.get('card_type') == 'finding']
        observations = [c for c in cards if c.get('card_type') == 'observation']
        infos = [c for c in cards if c.get('card_type') == 'info']

        if findings:
            response += f"\n## Findings ({len(findings)} total)\n"
            for severity in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO']:
                severity_findings = [f for f in findings if f.get('severity') == severity]
                if severity_findings:
                    response += f"\n### {severity} ({len(severity_findings)})\n"
                    for finding in severity_findings:
                        response += f"\n**[ID: {finding['id']}] {finding.get('title', 'Untitled')}**\n"
                        if finding.get('target_service'):
                            response += f"- **Target:** {finding['target_service']}\n"
                        if finding.get('status'):
                            response += f"- **Status:** {finding['status']}\n"
                        if finding.get('technical_analysis'):
                            response += f"- **Technical Analysis:**\n{finding['technical_analysis']}\n"
                        if finding.get('proof'):
                            response += f"- **Proof:**\n{finding['proof']}\n"
                        if finding.get('notes'):
                            response += f"- **Notes:** {finding['notes']}\n"
                        response += f"- **Created:** {finding.get('created_at', 'N/A')}\n"

        if observations:
            response += f"\n## Observations ({len(observations)} total)\n"
            for obs in observations:
                response += f"\n**[ID: {obs['id']}] {obs.get('title', 'Untitled')}**\n"
                if obs.get('target_service'):
                    response += f"- **Target:** {obs['target_service']}\n"
                if obs.get('notes'):
                    response += f"- **Notes:**\n{obs['notes']}\n"
                if obs.get('technical_analysis'):
                    response += f"- **Analysis:** {obs['technical_analysis']}\n"
                response += f"- **Created:** {obs.get('created_at', 'N/A')}\n"

        if infos:
            response += f"\n## Info Cards ({len(infos)} total)\n"
            for info in infos:
                response += f"\n**[ID: {info['id']}] {info.get('title', 'Untitled')}**\n"
                if info.get('context'):
                    response += f"- **Context:**\n{info['context']}\n"
                if info.get('notes'):
                    response += f"- **Notes:** {info['notes']}\n"
                response += f"- **Created:** {info.get('created_at', 'N/A')}\n"

    # Add reconnaissance data
    recon_data = full_data.get('recon_data', [])
    if recon_data:
        response += "\n## Reconnaissance Data\n"
        endpoints = [r for r in recon_data if r.get('data_type') == 'endpoint']
        technologies = [r for r in recon_data if r.get('data_type') == 'technology']
        subdomains = [r for r in recon_data if r.get('data_type') == 'subdomain']

        if endpoints:
            response += f"**Endpoints ({len(endpoints)}):** {', '.join([e.get('name', '') for e in endpoints[:5]])}\n"
        if technologies:
            response += f"**Technologies ({len(technologies)}):** {', '.join([t.get('name', '') for t in technologies[:5]])}\n"
        if subdomains:
            response += f"**Subdomains ({len(subdomains)}):** {', '.join([s.get('name', '') for s in subdomains[:5]])}\n"

    # Add recent command history
    try:
        limit = await mcp_service.get_command_history_limit()
        commands_response = await mcp_service.http_client.get(
            f"{mcp_service.backend_url}/assessments/{assessment['id']}/commands",
            params={"limit": limit}
        )
        commands_response.raise_for_status()
        commands = commands_response.json()

        if commands:
            response += f"\n## Recent Commands ({len(commands)} most recent)\n"
            for cmd in commands:
                response += f"\n`{cmd.get('command', 'N/A')}`\n"
                if cmd.get('stdout'):
                    response += f"{cmd['stdout']}\n"
                if cmd.get('stderr'):
                    response += f"Error: {cmd['stderr']}\n"
    except Exception:
        pass

    response += "\nReady to begin assessment work!"

    return [TextContent(type="text", text=response)]


async def _handle_update_phase(arguments: dict, mcp_service) -> List[TextContent]:
    """Handle update_phase - Update phase content"""
    if not mcp_service.current_assessment_id:
        return [TextContent(type="text", text="No assessment loaded. Use 'load_assessment' first.")]

    phase_num = arguments["phase_number"]
    section_type = f"phase_{int(phase_num)}"

    await mcp_service.update_section(
        assessment_id=mcp_service.current_assessment_id,
        section_type=section_type,
        section_number=phase_num,
        title=arguments.get("title"),
        content=arguments["content"]
    )

    return [TextContent(
        type="text",
        text=f"Phase {phase_num} updated"
    )]


# ---------------------------------------------------------------------------
# Orchestration handlers
# ---------------------------------------------------------------------------

def _json_content(data: Any) -> List[TextContent]:
    return [TextContent(type="text", text=json.dumps(data, indent=2, default=str))]


def _error_content(message: str) -> List[TextContent]:
    return [TextContent(type="text", text=json.dumps({"success": False, "error": message}, indent=2))]


async def _handle_orchestration_status(arguments: dict, mcp_service) -> List[TextContent]:
    """Handle orchestration_status - get current phase, metrics, and gates."""
    if not mcp_service.current_assessment_id:
        return _error_content("No assessment loaded. Use 'load_assessment' first.")

    try:
        from lib.world_model_db import get_world_model_db
        from lib.phase_orchestrator import PhaseOrchestrator

        db = await get_world_model_db(mcp_service.current_assessment_id)
        orchestrator = PhaseOrchestrator(db)
        status = await orchestrator.get_status()

        return _json_content({
            "success": True,
            **status,
            "message": (
                f"Phase {status['current_phase']}: {status['phase_name']} "
                f"({status['progress_pct']}% complete)"
            ),
        })
    except Exception as exc:
        logger.error("orchestration_status failed: %s", exc, exc_info=True)
        return _error_content(f"Error: {exc}")


async def _handle_orchestration_advance(arguments: dict, mcp_service) -> List[TextContent]:
    """Handle orchestration_advance - request phase transition."""
    if not mcp_service.current_assessment_id:
        return _error_content("No assessment loaded. Use 'load_assessment' first.")

    target_phase = arguments.get("target_phase")
    if not isinstance(target_phase, int) or target_phase < 1 or target_phase > 5:
        return _error_content("target_phase is required and must be an integer 1-5")

    force = arguments.get("force", False)

    try:
        from lib.world_model_db import get_world_model_db
        from lib.phase_orchestrator import PhaseOrchestrator

        db = await get_world_model_db(mcp_service.current_assessment_id)
        orchestrator = PhaseOrchestrator(db)
        result = await orchestrator.advance(target_phase=target_phase, force=force)

        if result.get("success"):
            # Sync phase to backend assessment sections
            try:
                phase_names = {
                    1: "Reconnaissance",
                    2: "Mapping & Enumeration",
                    3: "Vulnerability Assessment",
                    4: "Exploitation",
                    5: "Post-Exploitation & Reporting",
                }
                await mcp_service.update_section(
                    assessment_id=mcp_service.current_assessment_id,
                    section_type=f"phase_{target_phase}",
                    section_number=float(target_phase),
                    title=phase_names.get(target_phase, f"Phase {target_phase}"),
                    content=f"Phase {target_phase} started via orchestration_advance.",
                )
            except Exception:
                pass

        return _json_content(result)
    except Exception as exc:
        logger.error("orchestration_advance failed: %s", exc, exc_info=True)
        return _error_content(f"Error: {exc}")
