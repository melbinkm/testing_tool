"""
Assessment Management Tools - load_assessment, update_phase,
orchestration_status, orchestration_advance.
"""
import json
import logging
from typing import Any, List

from mcp.types import Tool, TextContent

logger = logging.getLogger("autopentest-mcp")


def get_assessment_tools() -> List[Tool]:
    return [
        Tool(
            name="load_assessment",
            description=(
                "Load an existing assessment to begin work. Returns comprehensive context including scope, "
                "target domains/IPs, workspace structure, existing findings/observations/info cards, recon data, "
                "phase content, and recent command history. IMPORTANT: USER creates assessments via the web "
                "interface - you cannot create new assessments, only load existing ones. "

                "**When to use:** REQUIRED as the first tool call at the start of every session. All other tools "
                "depend on an assessment being loaded to function. Use skip_data=false (default) for initial session "
                "load to get full context (200-500 lines: findings, workspace tree, phase notes). Use skip_data=true "
                "for session reloads when you only need to refresh state without re-displaying verbose output (saves "
                "~4500 tokens). Both modes show critical config (git_repo_url, target domains, IP scopes). "

                "**Dependencies:** NONE - this is the entry point tool. Requires user to have created assessment "
                "via web interface first. Enables all 97 other tools once loaded. Follow with orchestration_status() "
                "to see current phase and progress, scope_get_allowlist() to verify targets, list_recon() or "
                "list_cards() to review existing discoveries. "

                "**Budget impact:** LOW - local database query, no network requests. Loads assessment metadata, "
                "cards, recon data, and command history. Completes in <500ms for typical assessments (<100 findings). "
                "May take 1-2 seconds for very large assessments (>500 cards + extensive recon data). "

                "**Failure modes:** 'Assessment not found' if name doesn't match existing assessment (check spelling, "
                "ask user for correct name). Assessment names are case-sensitive. Empty assessment returns minimal "
                "context (expected for new assessments - proceed with recon). Workspace not created yet is normal "
                "for fresh assessments. "

                "**Risk level:** SAFE - read-only operation loading local data. No network activity or target "
                "interaction. Sets internal state (current_assessment_id) used by all subsequent tool calls. "

                "**Returns:** Full assessment context formatted as markdown: client/scope/limitations, target "
                "domains/IPs, workspace structure tree, context documents list (user-provided files), phase content "
                "(5 phases), all cards grouped by type/severity (findings/observations/infos), recon data summary "
                "(endpoints/technologies/subdomains), recent command history. Use this context to understand what's "
                "been discovered, what phase you're in, and what to do next."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "Name of the assessment to load"
                    },
                    "skip_data": {
                        "type": "boolean",
                        "description": (
                            "If true, returns concise output (~10 lines) showing only critical config "
                            "(git_repo_url, target domains, IP scopes, ports). Use for session reloads to save tokens. "
                            "If false (default), returns full verbose output (~200-500 lines) including workspace tree, "
                            "all findings, observations, phase notes, recon data. Use false for initial load to get "
                            "complete context. Both modes set internal state (current_assessment_id, current_base_url)."
                        ),
                        "default": False
                    }
                },
                "required": ["name"]
            }
        ),
        Tool(
            name="update_phase",
            description=(
                "Update or append content to a specific phase section in the assessment report. Supports full "
                "markdown formatting for rich documentation. Use to document discoveries, methodologies, findings "
                "summaries, and next steps for each phase of the pentest. Content is preserved across sessions. "

                "**When to use:** Throughout testing to document phase progress and findings. Call at phase milestones: "
                "Phase 1 after recon (subdomains, services, technologies found), Phase 2 after mapping (endpoints, "
                "forms, attack surface documented), Phase 3 after SAST review (code findings verified), Phase 4 during "
                "vulnerability assessment (testing approach, initial findings), Phase 5 during exploitation (successful "
                "exploits, impact demonstrated), Phase 6 for final reporting (executive summary, recommendations). "
                "Update incrementally as you discover new information. "

                "**Dependencies:** Requires load_assessment() first. No follow-up required - content saved to database "
                "and visible in web interface. Can call multiple times per phase to append/update content. Use with "
                "orchestration_status() to confirm current phase before documenting. "

                "**Budget impact:** LOW - local database write, no network requests. Stores markdown content in "
                "assessment sections table. Completes <50ms regardless of content size. Content length unlimited. "

                "**Failure modes:** 'No assessment loaded' if load_assessment() not called first. Invalid phase_number "
                "(must be 1.0-6.0) returns error. Content replaces existing phase content (not appended) - read "
                "existing content first if you want to preserve it. Empty content allowed (clears phase section). "

                "**Risk level:** SAFE - local documentation tool, no network activity or target interaction. Creates "
                "permanent record visible to user in web interface and final report. "

                "**Returns:** Simple confirmation 'Phase {N} updated'. Content is persisted to database and immediately "
                "visible in assessment report. Use for documenting methodology, findings summaries, screenshots "
                "(as markdown links), command outputs, and narrative of testing activities."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "phase_number": {
                        "type": "number",
                        "description": "Phase number (1.0, 2.0, 3.0, etc.)"
                    },
                    "title": {
                        "type": "string",
                        "description": "Section title"
                    },
                    "content": {
                        "type": "string",
                        "description": "Phase content (markdown supported)"
                    }
                },
                "required": ["phase_number", "content"]
            }
        ),

        # ---- orchestration_status -----------------------------------------
        Tool(
            name="orchestration_status",
            description=(
                "Get current assessment phase, comprehensive metrics (assets, endpoints, findings counts), gate "
                "conditions for advancing to the next phase, and overall progress percentage. Provides structured "
                "view of testing progress and readiness for phase transitions. Phases: 1=Recon, 2=Mapping, "
                "3=SAST Review, 4=Vuln Assessment, 5=Exploitation, 6=Reporting. "

                "**When to use:** Call periodically (every 20-30 tool calls) to check progress and phase readiness. "
                "Use after major discoveries (found many subdomains, completed crawl, discovered critical finding) "
                "to see if metrics meet gate conditions for phase transition. Call before orchestration_advance() "
                "to verify you meet gate requirements. Use to decide what to do next: if asset count low, do more "
                "recon; if endpoint count low, crawl more; if finding count low, test more. "

                "**Dependencies:** Requires load_assessment() first. No follow-up required unless you want to advance "
                "phases (use orchestration_advance). Works with world model data - more accurate after using wm_* "
                "tools (wm_add_asset, wm_add_endpoint, wm_add_finding) instead of legacy recon data. "

                "**Budget impact:** LOW - local database aggregation query across world model tables. Counts assets, "
                "endpoints, hypotheses, findings. Completes <200ms for typical assessments (<500 world model entities). "

                "**Failure modes:** 'No assessment loaded' if load_assessment() not called first. Empty metrics "
                "(all zeros) indicates no world model data yet - expected early in assessment, proceed with recon "
                "and use wm_add_* tools. Phase stuck at 1? Add assets. Stuck at 2? Add endpoints. Stuck at 3? "
                "Create hypotheses and findings. "

                "**Risk level:** SAFE - read-only query of local world model database. No network activity or target "
                "interaction. Returns JSON with current state and gate conditions. "

                "**Returns:** JSON with current_phase (1-6), phase_name, progress_pct (0-100), metrics (assets, "
                "endpoints, hypotheses, findings), and next_gates array (conditions needed to advance). Example: "
                "{current_phase: 2, phase_name: 'Mapping', progress_pct: 45, metrics: {assets: 15, endpoints: 8, "
                "findings: 0}, next_gates: [{condition: 'min_endpoints', required: 10, current: 8, met: false}]}. "
                "Use to guide testing strategy and phase progression decisions."
            ),
            inputSchema={
                "type": "object",
                "properties": {},
            },
        ),

        # ---- orchestration_advance ----------------------------------------
        Tool(
            name="orchestration_advance",
            description=(
                "Request a phase transition to advance the assessment to the next phase, or mark phase 6 complete. "
                "Validates gate conditions (minimum assets, endpoints, hypotheses, findings, or SAST verification) before "
                "allowing the transition. Gates ensure proper pentest methodology: adequate recon before mapping, sufficient "
                "mapping before SAST, complete SAST before assessment, validated findings before reporting. Use force=true to "
                "skip validation (not recommended). "

                "**When to use:** After completing phase objectives and orchestration_status() shows gate conditions "
                "met. Typical progression: Phase 1→2 after discovering attack surface (assets, subdomains, services), "
                "Phase 2→3 after mapping all endpoints (crawl, API spec parsing, coverage matrix built), Phase 3→4 "
                "after SAST verification complete (all SAST findings reviewed), Phase 4→5 after testing produces findings "
                "(vulnerabilities confirmed), Phase 5→6 after exploitation and validation complete (evidence collected, "
                "risk assessed), Phase 6→Complete (target_phase=7) after final reporting and evidence bundle ready. "
                "DO NOT advance prematurely - incomplete phases lead to missed vulnerabilities. "

                "**Dependencies:** Requires load_assessment() first. MUST call orchestration_status() before advancing "
                "to verify gate conditions met. If gates not met, do more work in current phase: Phase 1 needs "
                "subdomain_enum, scan, tech_detection; Phase 2 needs crawler_start, coverage_init; Phase 3 needs "
                "sast_clone_repo, sast_scan_semgrep, sast_mark_verified; Phase 4 needs endpoint_execute_plan, "
                "auth_diff_test; Phase 5 needs validate_repro, evidence_bundle. "

                "**Budget impact:** LOW - local database write to update phase metadata. Also calls update_phase() "
                "internally to document transition. Completes <100ms. No network requests. "

                "**Failure modes:** 'Gate conditions not met' if metrics insufficient (message shows which conditions "
                "failed - add more assets/endpoints/findings as needed). 'Invalid target_phase' if not 1-7 or trying "
                "to skip phases (can only advance +1 at a time, or use force=true). 'No assessment loaded' if "
                "load_assessment() not called. force=true always succeeds but may indicate incomplete methodology. "

                "**Risk level:** SAFE - local state management only, no network activity or target interaction. "
                "Changes assessment phase metadata visible in web interface. Affects tool recommendations in "
                "workflow-guide resource. "

                "**Returns:** JSON with success boolean, new_phase number, message, and gates_met array showing which "
                "conditions passed/failed. Example: {success: true, new_phase: 3, message: 'Advanced to Phase 3', "
                "gates_met: [{condition: 'min_endpoints', met: true}]}. On failure: {success: false, error: 'Gate "
                "conditions not met', gates_met: [{condition: 'min_findings', required: 5, current: 2, met: false}]}. "
                "Use success to confirm transition, review gates_met to understand what's missing."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "target_phase": {
                        "type": "integer",
                        "minimum": 1,
                        "maximum": 7,
                        "description": "Target phase number (1-6 for transitions, 7 to mark phase 6 complete)",
                    },
                    "force": {
                        "type": "boolean",
                        "description": "Override gate conditions (default: false)",
                    },
                },
                "required": ["target_phase"],
            },
        ),
        # ---- set_instructions ---------------------------------------------
        Tool(
            name="set_instructions",
            description=(
                "Store natural-language testing directives for this assessment. Instructions guide "
                "the AI agent's testing focus and priorities. Stored in the world model knowledge "
                "base and retrievable via the autopentest://instructions resource or wm_recall."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "text": {
                        "type": "string",
                        "description": "Testing instructions (e.g., 'Focus on IDOR vulnerabilities in the /api/users endpoint')"
                    }
                },
                "required": ["text"]
            },
        ),
        # ---- orchestration_resume ----------------------------------------
        Tool(
            name="orchestration_resume",
            description=(
                "Resume from the last saved workflow checkpoint after a crash or session restart. "
                "Returns the phase, step, and saved state so the LLM can pick up where it left off. "
                "Returns 'no checkpoint found' if no checkpoint has been saved yet. "

                "**When to use:** At session start after load_assessment() if you suspect a previous "
                "session was interrupted. Also useful to inspect what was saved at the last checkpoint. "

                "**Dependencies:** Requires load_assessment() first. Checkpoints are created by the "
                "orchestration system during phase transitions or manually via orchestration_checkpoint. "

                "**Budget impact:** ZERO - local database read only. "

                "**Risk level:** SAFE - read-only query."
            ),
            inputSchema={
                "type": "object",
                "properties": {},
            },
        ),

        # ---- orchestration_checkpoint -------------------------------------
        Tool(
            name="orchestration_checkpoint",
            description=(
                "Create a git-backed checkpoint in the assessment workspace. Commits all current "
                "workspace files as a git commit tagged with the phase number. Use to create safe "
                "rollback points before risky operations. Also saves a durable checkpoint to the "
                "world model for crash recovery. "

                "**When to use:** Before exploitation attempts, after completing major discovery, "
                "at phase transitions, or before any operation you might want to undo. "

                "**Dependencies:** Requires load_assessment() first. Workspace directory must exist. "

                "**Budget impact:** ZERO - local git operation only. "

                "**Risk level:** SAFE - creates a git commit, no network activity."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "description": {
                        "type": "string",
                        "description": "Description of this checkpoint (e.g. 'Pre-exploitation Phase 4 complete')",
                    },
                    "phase": {
                        "type": "integer",
                        "minimum": 1,
                        "maximum": 6,
                        "description": "Phase number for this checkpoint (default: current phase)",
                    },
                },
                "required": ["description"],
            },
        ),

        # ---- orchestration_rollback ---------------------------------------
        Tool(
            name="orchestration_rollback",
            description=(
                "List available git checkpoints or roll back the workspace to a specific checkpoint. "
                "Without checkpoint_hash, lists all available checkpoints. With checkpoint_hash, "
                "performs a hard reset to that commit. "

                "**When to use:** When exploitation caused unwanted side effects, when you need to "
                "undo workspace changes, or to inspect available rollback points. "

                "**Dependencies:** Requires load_assessment() first. Workspace must have git checkpoints. "

                "**Budget impact:** ZERO - local git operation only. "

                "**Risk level:** CAUTION - rollback is destructive to workspace files after the checkpoint. "
                "Creates a safety checkpoint before rolling back."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "checkpoint_hash": {
                        "type": "string",
                        "description": "Git commit hash to roll back to (omit to list checkpoints)",
                    },
                },
            },
        ),
        Tool(
            name="assessment_add_target",
            description=(
                "Add an additional target URL to the current assessment for multi-target testing. "
                "Stores the target as a wm_asset with 'application' kind and associated metadata. "
                "Use this to expand the scope to additional APIs, web apps, or services. "

                "**When to use:** Phase 1-2 when discovering additional in-scope targets. "

                "**Dependencies:** Requires load_assessment() first. "

                "**Budget impact:** ZERO - local database write. "

                "**Risk level:** SAFE - no network activity."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "target_url": {
                        "type": "string",
                        "description": "Target URL to add (e.g., https://api.example.com)",
                    },
                    "target_type": {
                        "type": "string",
                        "enum": ["web_app", "api", "mobile_backend", "graphql", "websocket"],
                        "default": "web_app",
                    },
                    "label": {
                        "type": "string",
                        "description": "Human-readable label for this target",
                    },
                },
                "required": ["target_url"],
            },
        ),
    ]


async def handle_assessment_tool(name: str, arguments: dict, mcp_service) -> List[TextContent]:
    if name == "load_assessment":
        return await _handle_load_assessment(arguments, mcp_service)
    elif name == "update_phase":
        return await _handle_update_phase(arguments, mcp_service)
    elif name == "orchestration_status":
        return await _handle_orchestration_status(arguments, mcp_service)
    elif name == "orchestration_advance":
        return await _handle_orchestration_advance(arguments, mcp_service)
    elif name == "set_instructions":
        return await _handle_set_instructions(arguments, mcp_service)
    elif name == "orchestration_resume":
        return await _handle_orchestration_resume(arguments, mcp_service)
    elif name == "orchestration_checkpoint":
        return await _handle_orchestration_checkpoint(arguments, mcp_service)
    elif name == "orchestration_rollback":
        return await _handle_orchestration_rollback(arguments, mcp_service)
    elif name == "assessment_add_target":
        return await _handle_assessment_add_target(arguments, mcp_service)
    return [TextContent(type="text", text=f"Unknown assessment tool: {name}")]


async def _handle_load_assessment(arguments: dict, mcp_service) -> List[TextContent]:
    """Handle load_assessment - Load and display full assessment data"""
    assessment_name = arguments["name"]
    skip_data = arguments.get("skip_data", False)

    assessment = await mcp_service.get_assessment_by_name(assessment_name)

    if not assessment:
        return [TextContent(
            type="text",
            text=f"Assessment '{assessment_name}' not found. USER must create it via the interface first."
        )]

    mcp_service.current_assessment_id = assessment["id"]
    mcp_service.current_assessment_name = assessment["name"]
    # Extract base URL from target domains if available (Fix B3)
    if assessment.get("target_domains") and len(assessment["target_domains"]) > 0:
        mcp_service.current_base_url = assessment["target_domains"][0]
    elif assessment.get("ip_scopes") and len(assessment["ip_scopes"]) > 0:
        # Fallback: construct base_url from first IP scope (Fix: autonomous runbook support)
        ip_scope = assessment["ip_scopes"][0]
        # Extract IP from CIDR notation (e.g., "172.26.240.78/32" -> "172.26.240.78")
        ip = ip_scope.split('/')[0] if '/' in ip_scope else ip_scope
        # Use port from scope if available, otherwise default to 80
        port = assessment.get("ports", [80])[0] if assessment.get("ports") else 80
        mcp_service.current_base_url = f"http://{ip}:{port}" if port != 80 else f"http://{ip}"
    else:
        mcp_service.current_base_url = None

    # Invalidate scope cache to ensure fresh scope is loaded
    if mcp_service.scope_provider:
        mcp_service.scope_provider.invalidate_cache(assessment["id"])

    # Initialize ActivityLogger for this assessment (Fix 3)
    mcp_service.initialize_activity_logger()

    # Validate scope config if available
    scope_validation_msg = ""
    try:
        scope = await mcp_service.scope_provider.get_scope(assessment["id"]) if mcp_service.scope_provider else None
        if scope:
            from lib.config_validator import validate_scope_config
            scope_dict = scope.model_dump()
            is_valid, validation_errors = validate_scope_config(scope_dict)
            if not is_valid:
                scope_validation_msg = "\n**Scope Validation Warnings:**\n"
                for err in validation_errors[:5]:
                    scope_validation_msg += f"  - {err}\n"
    except Exception:
        pass  # Non-critical: don't block load on validation failure

    if skip_data:
        # Show critical config even with skip_data (Fix: SAST visibility)
        response = f"Assessment '{assessment_name}' loaded successfully.\n\n"

        # Show target scope
        if assessment.get("target_domains"):
            response += f"**Target Domains:** {', '.join(assessment['target_domains'])}\n"
        if assessment.get("ip_scopes"):
            response += f"**IP Scopes:** {', '.join(assessment['ip_scopes'])}\n"
        if assessment.get("ports"):
            response += f"**Ports:** {', '.join(str(p) for p in assessment['ports'])}\n"

        # Show git_repo_url if present (CRITICAL for SAST workflow)
        if assessment.get('git_repo_url'):
            response += f"\n**Git Repository:** {assessment['git_repo_url']}\n"
            response += "  → SAST tools available: `sast_clone_repo`, `sast_scan_semgrep`, `sast_scan_bandit`, `sast_scan_gitleaks`\n"
            response += "  → Run SAST scans in Phase 3 before advancing to Phase 4\n"

        # Check for resume checkpoint
        try:
            from lib.world_model_db import get_world_model_db
            from lib.phase_orchestrator import PhaseOrchestrator
            db = await get_world_model_db(assessment["id"])
            orch = PhaseOrchestrator(db)
            cp = await orch.load_checkpoint()
            if cp:
                response += f"\n**Resume Available:** Phase {cp.get('phase', '?')}, step '{cp.get('step', '?')}'\n"
                response += "  -> Call `orchestration_resume` to load checkpoint state\n"
        except Exception:
            pass

        if scope_validation_msg:
            response += scope_validation_msg

        return [TextContent(type="text", text=response)]

    full_data = await mcp_service.get_assessment_full_data(assessment["id"])
    assessment_data = full_data['assessment']

    response = f"**Assessment Loaded: {assessment_data['name']}**\n\n"
    response += "## State of Work\n"
    response += f"**Client:** {assessment_data.get('client_name', 'N/A')}\n"
    response += f"**Environment:** {assessment_data.get('environment', 'non_specifie')}\n"
    response += f"**Scope:** {assessment_data.get('scope', 'N/A')}\n"
    response += f"**Limitations:** {assessment_data.get('limitations', 'N/A')}\n\n"

    response += "## Basic Information\n"
    response += f"**Target Domains:** {', '.join(assessment_data.get('target_domains', [])) or 'N/A'}\n"
    response += f"**IP Scopes:** {', '.join(assessment_data.get('ip_scopes', [])) or 'N/A'}\n"

    # Show git_repo_url if present (Fix 1: SAST visibility)
    if assessment_data.get('git_repo_url'):
        response += f"**Git Repository:** {assessment_data['git_repo_url']}\n"
        response += "  → SAST tools available: `sast_clone_repo`, `sast_scan_semgrep`, `sast_scan_bandit`, `sast_scan_gitleaks`\n"
        response += "  → Run SAST scans in Phase 3 before advancing to Phase 4\n"

    response += "\n"
    response += f"**Workspace:** `{assessment_data.get('workspace_path', 'Not created')}`\n\n"

    # Generate workspace tree structure and list context documents
    if assessment_data.get('workspace_path') and assessment_data.get('container_name'):
        try:
            from utils.tree_generator import generate_workspace_tree, get_context_files_list
            import logging
            log = logging.getLogger("autopentest-mcp")

            tree_structure = await generate_workspace_tree(
                container_name=assessment_data.get('container_name'),
                workspace_path=assessment_data.get('workspace_path'),
                max_depth=2
            )

            response += "## Workspace Structure\n\n"
            response += "```\n"
            response += tree_structure
            response += "\n```\n\n"

            context_files = await get_context_files_list(
                container_name=assessment_data.get('container_name'),
                workspace_path=assessment_data.get('workspace_path')
            )

            if context_files:
                response += "## Context Documents Provided by User\n\n"
                response += "The following documents are available in `/context` for additional context:\n\n"
                for file_info in context_files:
                    response += f"- `{file_info['filename']}` ({file_info['size_human']}) - {file_info['type']}\n"
                    response += f"  Path: `{file_info['path']}`\n"
                response += "\n**Note:** You can read these files using the `execute()` tool with commands like `cat` or `less`.\n\n"
        except Exception as e:
            pass

    # Add sections information
    sections = full_data.get('sections', [])
    if sections:
        response += "## Assessment Phases\n"
        for section in sections:
            phase_num = section.get('section_type', '').replace('phase_', '')
            if phase_num.isdigit():
                phase_names = {
                    '1': 'Reconnaissance',
                    '2': 'Mapping & Enumeration',
                    '3': 'SAST Code Review',
                    '4': 'Vulnerability Assessment',
                    '5': 'Exploitation',
                    '6': 'Post-Exploitation & Reporting'
                }
                phase_name = phase_names.get(phase_num, f'Phase {phase_num}')
                content = section.get('content', '')
                if content:
                    response += f"**Phase {phase_num} - {phase_name}:**\n{content}\n\n"
                else:
                    response += f"**Phase {phase_num} - {phase_name}:** No content yet\n"

    # Add all cards with full details
    cards = full_data.get('cards', [])
    if cards:
        cards = [c for c in cards if c.get('status') != 'false_positive']
        findings = [c for c in cards if c.get('card_type') == 'finding']
        observations = [c for c in cards if c.get('card_type') == 'observation']
        infos = [c for c in cards if c.get('card_type') == 'info']

        if findings:
            response += f"\n## Findings ({len(findings)} total)\n"
            for severity in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'INFO']:
                severity_findings = [f for f in findings if f.get('severity') == severity]
                if severity_findings:
                    response += f"\n### {severity} ({len(severity_findings)})\n"
                    for finding in severity_findings:
                        response += f"\n**[ID: {finding['id']}] {finding.get('title', 'Untitled')}**\n"
                        if finding.get('target_service'):
                            response += f"- **Target:** {finding['target_service']}\n"
                        if finding.get('status'):
                            response += f"- **Status:** {finding['status']}\n"
                        if finding.get('technical_analysis'):
                            response += f"- **Technical Analysis:**\n{finding['technical_analysis']}\n"
                        if finding.get('proof'):
                            response += f"- **Proof:**\n{finding['proof']}\n"
                        if finding.get('notes'):
                            response += f"- **Notes:** {finding['notes']}\n"
                        response += f"- **Created:** {finding.get('created_at', 'N/A')}\n"

        if observations:
            response += f"\n## Observations ({len(observations)} total)\n"
            for obs in observations:
                response += f"\n**[ID: {obs['id']}] {obs.get('title', 'Untitled')}**\n"
                if obs.get('target_service'):
                    response += f"- **Target:** {obs['target_service']}\n"
                if obs.get('notes'):
                    response += f"- **Notes:**\n{obs['notes']}\n"
                if obs.get('technical_analysis'):
                    response += f"- **Analysis:** {obs['technical_analysis']}\n"
                response += f"- **Created:** {obs.get('created_at', 'N/A')}\n"

        if infos:
            response += f"\n## Info Cards ({len(infos)} total)\n"
            for info in infos:
                response += f"\n**[ID: {info['id']}] {info.get('title', 'Untitled')}**\n"
                if info.get('context'):
                    response += f"- **Context:**\n{info['context']}\n"
                if info.get('notes'):
                    response += f"- **Notes:** {info['notes']}\n"
                response += f"- **Created:** {info.get('created_at', 'N/A')}\n"

    # Add reconnaissance data
    recon_data = full_data.get('recon_data', [])
    if recon_data:
        response += "\n## Reconnaissance Data\n"
        endpoints = [r for r in recon_data if r.get('data_type') == 'endpoint']
        technologies = [r for r in recon_data if r.get('data_type') == 'technology']
        subdomains = [r for r in recon_data if r.get('data_type') == 'subdomain']

        if endpoints:
            response += f"**Endpoints ({len(endpoints)}):** {', '.join([e.get('name', '') for e in endpoints[:5]])}\n"
        if technologies:
            response += f"**Technologies ({len(technologies)}):** {', '.join([t.get('name', '') for t in technologies[:5]])}\n"
        if subdomains:
            response += f"**Subdomains ({len(subdomains)}):** {', '.join([s.get('name', '') for s in subdomains[:5]])}\n"

    # Add recent command history
    try:
        limit = await mcp_service.get_command_history_limit()
        commands_response = await mcp_service.http_client.get(
            f"{mcp_service.backend_url}/assessments/{assessment['id']}/commands",
            params={"limit": limit}
        )
        commands_response.raise_for_status()
        commands = commands_response.json()

        if commands:
            response += f"\n## Recent Commands ({len(commands)} most recent)\n"
            for cmd in commands:
                response += f"\n`{cmd.get('command', 'N/A')}`\n"
                if cmd.get('stdout'):
                    response += f"{cmd['stdout']}\n"
                if cmd.get('stderr'):
                    response += f"Error: {cmd['stderr']}\n"
    except Exception:
        pass

    if scope_validation_msg:
        response += scope_validation_msg

    response += "\nReady to begin assessment work!"

    return [TextContent(type="text", text=response)]


async def _handle_update_phase(arguments: dict, mcp_service) -> List[TextContent]:
    """Handle update_phase - Update phase content"""
    if not mcp_service.current_assessment_id:
        return [TextContent(type="text", text="No assessment loaded. Use 'load_assessment' first.")]

    phase_num = arguments["phase_number"]
    section_type = f"phase_{int(phase_num)}"

    await mcp_service.update_section(
        assessment_id=mcp_service.current_assessment_id,
        section_type=section_type,
        section_number=phase_num,
        title=arguments.get("title"),
        content=arguments["content"]
    )

    return [TextContent(
        type="text",
        text=f"Phase {phase_num} updated"
    )]


# ---------------------------------------------------------------------------
# Orchestration handlers
# ---------------------------------------------------------------------------

from lib.tool_helpers import _get_db, _json_content, _error_content


async def _handle_orchestration_status(arguments: dict, mcp_service) -> List[TextContent]:
    """Handle orchestration_status - get current phase, metrics, and gates."""
    if not mcp_service.current_assessment_id:
        return _error_content("No assessment loaded. Use 'load_assessment' first.")

    try:
        from lib.world_model_db import get_world_model_db
        from lib.phase_orchestrator import PhaseOrchestrator

        db = await get_world_model_db(mcp_service.current_assessment_id)
        orchestrator = PhaseOrchestrator(db, activity_logger=mcp_service.activity_logger)  # Fix 3
        status = await orchestrator.get_status()

        return _json_content({
            "success": True,
            **status,
            "message": (
                f"Phase {status['current_phase']}: {status['phase_name']} "
                f"({status['progress_pct']}% complete)"
            ),
        })
    except Exception as exc:
        logger.error("orchestration_status failed: %s", exc, exc_info=True)
        return _error_content(f"Error: {exc}")


async def _handle_orchestration_advance(arguments: dict, mcp_service) -> List[TextContent]:
    """Handle orchestration_advance - request phase transition or completion."""
    if not mcp_service.current_assessment_id:
        return _error_content("No assessment loaded. Use 'load_assessment' first.")

    target_phase = arguments.get("target_phase")
    if not isinstance(target_phase, int) or target_phase < 1 or target_phase > 7:
        return _error_content("target_phase is required and must be an integer 1-7 (7 = complete phase 6)")

    force = arguments.get("force", False)

    try:
        from lib.world_model_db import get_world_model_db
        from lib.phase_orchestrator import PhaseOrchestrator

        db = await get_world_model_db(mcp_service.current_assessment_id)
        orchestrator = PhaseOrchestrator(db, activity_logger=mcp_service.activity_logger)  # Fix 3

        # Handle assessment completion (target_phase=7)
        if target_phase == 7:
            result = await orchestrator.complete_assessment()
            if result.get("success"):
                # Sync phase completion to backend
                try:
                    await mcp_service.update_section(
                        assessment_id=mcp_service.current_assessment_id,
                        section_type="phase_6",
                        section_number="6",
                        title="Post-Exploitation & Reporting",
                        content="Assessment completed successfully."
                    )
                except Exception:
                    pass  # Don't fail on section update
            return _json_content(result)

        # Handle normal phase transition
        result = await orchestrator.advance(target_phase=target_phase, force=force)

        if result.get("success"):
            # Sync phase to backend assessment sections
            try:
                phase_names = {
                    1: "Reconnaissance",
                    2: "Mapping & Enumeration",
                    3: "SAST Code Review",
                    4: "Vulnerability Assessment",
                    5: "Exploitation",
                    6: "Post-Exploitation & Reporting",
                }
                await mcp_service.update_section(
                    assessment_id=mcp_service.current_assessment_id,
                    section_type=f"phase_{target_phase}",
                    section_number=float(target_phase),
                    title=phase_names.get(target_phase, f"Phase {target_phase}"),
                    content=f"Phase {target_phase} started via orchestration_advance.",
                )

                # Create stub sections for any skipped phases (Fix 6)
                previous_phase = result.get("previous_phase", 1)
                for skipped_phase in range(previous_phase + 1, target_phase):
                    try:
                        await mcp_service.update_section(
                            assessment_id=mcp_service.current_assessment_id,
                            section_type=f"phase_{skipped_phase}",
                            section_number=float(skipped_phase),
                            title=f"{phase_names.get(skipped_phase, f'Phase {skipped_phase}')} - Skipped",
                            content=f"Phase {skipped_phase} was skipped during force advance from phase {previous_phase} to phase {target_phase}."
                        )
                    except Exception:
                        pass

            except Exception:
                pass

        return _json_content(result)
    except Exception as exc:
        logger.error("orchestration_advance failed: %s", exc, exc_info=True)
        return _error_content(f"Error: {exc}")


# ---------------------------------------------------------------------------
# Checkpoint / recovery handlers (#4, #12, #25)
# ---------------------------------------------------------------------------

async def _handle_orchestration_resume(arguments: dict, mcp_service) -> List[TextContent]:
    """Handle orchestration_resume - load last checkpoint for crash recovery."""
    if not mcp_service.current_assessment_id:
        return _error_content("No assessment loaded. Use 'load_assessment' first.")

    try:
        from lib.world_model_db import get_world_model_db
        from lib.phase_orchestrator import PhaseOrchestrator

        db = await get_world_model_db(mcp_service.current_assessment_id)
        orchestrator = PhaseOrchestrator(db, activity_logger=mcp_service.activity_logger)

        cp = await orchestrator.resume_from_checkpoint()
        if cp is None:
            return _json_content({
                "success": True,
                "checkpoint_found": False,
                "message": "No checkpoint found. Start work from the beginning.",
            })

        return _json_content({
            "success": True,
            "checkpoint_found": True,
            "phase": cp["phase"],
            "step": cp["step"],
            "state": cp["state"],
            "saved_at": cp["saved_at"],
            "message": f"Checkpoint found: Phase {cp['phase']}, step '{cp['step']}'. Resume from here.",
        })
    except Exception as exc:
        logger.error("orchestration_resume failed: %s", exc, exc_info=True)
        return _error_content(f"Error: {exc}")


async def _handle_orchestration_checkpoint(arguments: dict, mcp_service) -> List[TextContent]:
    """Handle orchestration_checkpoint - create git + DB checkpoint."""
    if not mcp_service.current_assessment_id:
        return _error_content("No assessment loaded. Use 'load_assessment' first.")

    description = arguments.get("description", "Manual checkpoint")

    try:
        from lib.world_model_db import get_world_model_db
        from lib.phase_orchestrator import PhaseOrchestrator
        from lib.git_checkpoint import create_checkpoint

        db = await get_world_model_db(mcp_service.current_assessment_id)
        orchestrator = PhaseOrchestrator(db, activity_logger=mcp_service.activity_logger)

        # Determine phase
        phase = arguments.get("phase")
        if phase is None:
            phase = await orchestrator._get_current_phase()

        # Save durable checkpoint to world model
        cp_result = await orchestrator.save_checkpoint(
            phase=phase,
            step=description,
            state_dict={"description": description},
        )

        # Create git checkpoint in workspace
        workspace_dir = f"{mcp_service.workspace_dir}/{mcp_service.current_assessment_id}"
        git_result = None
        try:
            import os
            os.makedirs(workspace_dir, exist_ok=True)
            git_cp = await create_checkpoint(workspace_dir, description, phase)
            if git_cp:
                git_result = git_cp.to_dict()
                # Store git hash in world model
                await db.store_knowledge(
                    source_tool="orchestration_checkpoint",
                    category="other",
                    title=f"git_checkpoint_{git_cp.hash[:8]}",
                    content=f"{git_cp.hash}|{description}|{phase}",
                    metadata={"hash": git_cp.hash, "phase": phase},
                )
        except Exception as e:
            logger.warning("Git checkpoint failed (non-critical): %s", e)

        return _json_content({
            "success": True,
            "checkpoint_key": cp_result.get("checkpoint_key"),
            "git_checkpoint": git_result,
            "phase": phase,
            "message": f"Checkpoint created for Phase {phase}: {description}",
        })
    except Exception as exc:
        logger.error("orchestration_checkpoint failed: %s", exc, exc_info=True)
        return _error_content(f"Error: {exc}")


async def _handle_orchestration_rollback(arguments: dict, mcp_service) -> List[TextContent]:
    """Handle orchestration_rollback - list or execute git rollback."""
    if not mcp_service.current_assessment_id:
        return _error_content("No assessment loaded. Use 'load_assessment' first.")

    checkpoint_hash = arguments.get("checkpoint_hash")
    workspace_dir = f"{mcp_service.workspace_dir}/{mcp_service.current_assessment_id}"

    try:
        from lib.git_checkpoint import list_checkpoints, rollback_to_checkpoint, create_checkpoint

        if not checkpoint_hash:
            # List mode
            checkpoints = await list_checkpoints(workspace_dir)
            return _json_content({
                "success": True,
                "checkpoints": [cp.to_dict() for cp in checkpoints],
                "count": len(checkpoints),
                "message": f"Found {len(checkpoints)} checkpoints." + (
                    " Use checkpoint_hash to roll back." if checkpoints else
                    " No checkpoints yet. Use orchestration_checkpoint to create one."
                ),
            })

        # Rollback mode - create safety checkpoint first
        safety = await create_checkpoint(workspace_dir, "Pre-rollback safety checkpoint", 0)

        ok = await rollback_to_checkpoint(workspace_dir, checkpoint_hash)
        if not ok:
            return _error_content(f"Rollback to {checkpoint_hash} failed. Check if the hash is valid.")

        return _json_content({
            "success": True,
            "rolled_back_to": checkpoint_hash,
            "safety_checkpoint": safety.to_dict() if safety else None,
            "message": f"Rolled back workspace to checkpoint {checkpoint_hash[:8]}.",
        })
    except Exception as exc:
        logger.error("orchestration_rollback failed: %s", exc, exc_info=True)
        return _error_content(f"Error: {exc}")


async def _handle_set_instructions(args: dict, mcp_service) -> List[TextContent]:
    """Store natural-language testing directives for this assessment."""
    text = args.get("text", "").strip()
    if not text:
        return _error_content("Instructions text cannot be empty")

    db = await _get_db(mcp_service)
    await db.store_knowledge(
        "set_instructions",         # source_tool
        "other",                    # category (valid KNOWLEDGE_CATEGORIES value)
        "assessment_instructions",  # title
        text,                       # content
        metadata={"source": "set_instructions", "char_count": len(text)}
    )
    return _json_content({
        "success": True,
        "message": f"Instructions stored ({len(text)} chars)",
        "char_count": len(text),
        "resource_uri": "autopentest://instructions"
    })


# ---------------------------------------------------------------------------
# Multi-target support
# ---------------------------------------------------------------------------

async def _handle_assessment_add_target(args: dict, mcp_service) -> List[TextContent]:
    """Add an additional target URL to the current assessment."""
    from urllib.parse import urlparse
    import uuid as uuid_mod

    target_url = args.get("target_url", "").strip()
    target_type = args.get("target_type", "web_app")
    label = args.get("label", "")

    if not target_url:
        return _error_content("target_url is required")

    # Validate URL format
    try:
        parsed = urlparse(target_url)
        if not parsed.scheme or not parsed.netloc:
            return _error_content(
                f"Invalid URL format: {target_url}. Must include scheme (http/https) and host."
            )
    except Exception as e:
        return _error_content(f"URL parse error: {e}")

    db = await _get_db(mcp_service)

    # Store as wm_asset
    try:
        metadata = {
            "url": target_url,
            "target_type": target_type,
            "label": label or parsed.netloc,
            "added_via": "assessment_add_target",
        }

        asset = await db.add_asset(
            kind="application",
            name=label or parsed.netloc,
            metadata=metadata,
            tags=[],
        )

        return _json_content({
            "success": True,
            "asset_id": asset["id"],
            "target_url": target_url,
            "target_type": target_type,
            "label": label or parsed.netloc,
            "message": f"Target added: {target_url}",
        })
    except Exception as e:
        return _error_content(f"Failed to add target: {e}")
