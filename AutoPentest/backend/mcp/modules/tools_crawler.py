"""
Crawler Tools - crawler_start, crawler_status, crawler_results.

Authenticated web crawling engine that discovers pages, links, forms,
and JavaScript-embedded endpoints. Auto-populates the world model.
"""

from __future__ import annotations

import json
import logging
from typing import Any, Dict, List

from mcp.types import Tool, TextContent

logger = logging.getLogger("autopentest-mcp")


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

from lib.tool_helpers import _json_content, _error_content


# ---------------------------------------------------------------------------
# Lazy-initialized singleton
# ---------------------------------------------------------------------------

_crawler_instance = None


def _get_crawler(mcp_service=None):
    """Lazily initialize and return the WebCrawler singleton."""
    global _crawler_instance
    if _crawler_instance is None:
        from lib.web_crawler import WebCrawler

        # Get session manager
        # Use per-assessment scope validator via scope_provider
        scope_validator = None
        if mcp_service and mcp_service.scope_provider and mcp_service.current_assessment_id:
            try:
                # Get scope validator asynchronously - but we're in sync context
                # so we'll pass it to the crawler/session manager to load lazily
                pass  # Handled by HttpClient assessment_id/scope_provider params
            except Exception as e:
                logger.info(f"Scope validator not available for crawler: {e}")

        # Get session manager from browser tools
        import os
        from lib.browser_session import CorrelationManager, SessionManager
        engagement_id = "default"
        if mcp_service and getattr(mcp_service, "current_assessment_id", None):
            engagement_id = str(mcp_service.current_assessment_id)
        else:
            engagement_id = os.environ.get("ENGAGEMENT_ID", "default")

        correlation = CorrelationManager(engagement_id)
        max_sessions = int(os.environ.get("MAX_BROWSER_SESSIONS", "5"))
        session_manager = SessionManager(
            correlation_manager=correlation,
            scope_validator=scope_validator,
            max_sessions=max_sessions,
        )

        _crawler_instance = WebCrawler(
            session_manager=session_manager,
            scope_validator=scope_validator,
            db=None,  # Set per-call from mcp_service
            mcp_service=mcp_service,
        )
    return _crawler_instance


# ---------------------------------------------------------------------------
# Tool definitions
# ---------------------------------------------------------------------------

def get_crawler_tools() -> List[Tool]:
    """Return the three crawler tools."""
    return [
        # 1 ---- crawler_start ------------------------------------------
        Tool(
            name="crawler_start",
            description=(
                "Start comprehensive authenticated or unauthenticated web crawling from a seed URL. Discovers "
                "all linked pages, forms, JavaScript-embedded API endpoints, and assets. Parses sitemap.xml and "
                "robots.txt automatically. Extracts endpoints from JavaScript bundles using AST analysis. "
                "Validates scope for all discovered links. Auto-populates world model with endpoints and triggers "
                "coverage_discover() for automatic coverage matrix extension. "

                "**When to use:** Phase 2 (Mapping) after completing initial recon (recon_pipeline_run or manual "
                "scans). Essential for discovering authenticated attack surface - call with identity_id to crawl "
                "as logged-in user and find admin pages, API endpoints, hidden functionality. Use after "
                "credentials_add() to test with discovered credentials. Critical for JavaScript-heavy SPAs where "
                "endpoints aren't in HTML (extract_js=true finds fetch/axios calls). "

                "**Dependencies:** Requires scope_validate_target(start_url) first. For authenticated crawling, "
                "requires credentials_add() or auth_get_identities() to get identity_id. Follow with "
                "coverage_discover() to detect new endpoints and extend coverage matrix, then coverage_next() "
                "to get prioritized tests. Use crawler_status(crawl_id) to monitor long crawls, "
                "crawler_results(crawl_id) to retrieve discovered endpoints/forms. "

                "**Budget impact:** MEDIUM - 50-200 requests depending on max_pages. Each page = 1 request. "
                "JavaScript parsing (extract_js=true) adds no requests but increases processing time. "
                "Sitemap/robots parsing = 2 extra requests. Use max_pages to control budget (default 200). "
                "BFS strategy more efficient for broad discovery, DFS for deep path exploration. "

                "**Failure modes:** Empty results if JavaScript-only SPA with no server-side HTML links (check "
                "extract_js=true). Rate limiting/WAF blocks (429/403) - reduce max_pages or add delays. "
                "Session expiry during long crawls (re-crawl with fresh identity_id). Out-of-scope links "
                "silently filtered (expected behavior). Sitemap parse failures logged but don't stop crawl. "

                "**Risk level:** CAUTION - Active web traffic detectable by analytics/WAF. Authenticated crawling "
                "reveals what access level can see (may trigger account activity alerts). JavaScript extraction "
                "may find internal/development endpoints not meant for production. Respects scope boundaries "
                "and robots.txt by default. "

                "**Returns:** Crawl summary with: crawl_id (use in status/results tools), pages_crawled (count), "
                "endpoints_found (count), forms_discovered (count), js_endpoints (count), duration, sitemap_urls "
                "(count from sitemap.xml). All discovered endpoints auto-added to world model as wm_endpoints "
                "and visible in coverage matrix. Use crawler_results(crawl_id) for detailed endpoint list. "
                "Triggers discovery_monitor to detect new attack surface changes."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "start_url": {
                        "type": "string",
                        "description": "The URL to start crawling from (must include http:// or https://)",
                    },
                    "max_pages": {
                        "type": "integer",
                        "minimum": 1,
                        "maximum": 1000,
                        "description": "Maximum pages to crawl (default: 200)",
                    },
                    "max_depth": {
                        "type": "integer",
                        "minimum": 1,
                        "maximum": 50,
                        "description": "Maximum link depth from start URL (default: 10)",
                    },
                    "strategy": {
                        "type": "string",
                        "enum": ["bfs", "dfs"],
                        "description": "Crawl strategy: breadth-first (default) or depth-first",
                    },
                    "identity_id": {
                        "type": "string",
                        "description": "Identity ID for authenticated crawling (optional)",
                    },
                    "extract_js": {
                        "type": "boolean",
                        "description": "Extract API endpoints from JavaScript (default: true)",
                    },
                    "parse_sitemap": {
                        "type": "boolean",
                        "description": "Parse sitemap.xml and robots.txt (default: true)",
                    },
                    "exclude_patterns": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "URL patterns to exclude (glob or regex)",
                    },
                    "session_id": {
                        "type": "string",
                        "description": "Reuse an existing browser session (optional)",
                    },
                },
                "required": ["start_url"],
            },
        ),

        # 2 ---- crawler_status -----------------------------------------
        Tool(
            name="crawler_status",
            description=(
                "Check the real-time progress status of a running or completed web crawl. Shows pages visited, "
                "pages still queued, endpoints/forms discovered so far, errors encountered, elapsed time, and "
                "estimated completion. Essential for monitoring long-running crawls (large sites with max_pages>100). "

                "**When to use:** Phase 2 (Mapping) during crawler_start() execution to monitor progress. Web "
                "crawls can take 2-15 minutes for large sites - call this periodically (every 30-60 seconds) "
                "to check status. Use to detect if crawler is stuck (pages_queued=0 but not complete) or "
                "hitting errors (error_count increasing). Call after crawl completes to verify success before "
                "retrieving results with crawler_results(). "

                "**Dependencies:** Requires crawler_start() to have been called and returned crawl_id. Returns "
                "'crawl not found' if crawl_id invalid or expired (crawl state persists 24 hours after completion, "
                "then auto-deleted). No follow-up required - pure status query. Use crawler_results() once status "
                "shows 'completed'. "

                "**Budget impact:** ZERO - local query of crawler state in memory/cache, no network requests. "
                "Returns instantly with current progress. Crawler state tracked in-memory during run, persisted "
                "to database on completion. "

                "**Failure modes:** 'Crawl not found' if crawl_id invalid, expired (>24h old), or crawler was "
                "never started. Stale status if crawler crashed mid-run (shows last known state). Status 'running' "
                "with pages_queued=0 and no progress indicates stuck crawler (restart with new crawler_start()). "

                "**Risk level:** SAFE - read-only query of crawler state, no network activity or target interaction. "
                "Shows only what crawler_start() has done so far. "

                "**Returns:** JSON with: crawl_id, status (queued/running/completed/failed), pages_visited (count), "
                "pages_queued (remaining), endpoints_found (count), forms_found (count), js_endpoints_found (count), "
                "error_count, errors[] (sample of errors encountered), elapsed_seconds, estimated_completion_seconds "
                "(null if can't estimate), started_at, completed_at (null if running). Use status='completed' as "
                "trigger to call crawler_results()."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "crawl_id": {
                        "type": "string",
                        "description": "Crawl ID returned by crawler_start",
                    },
                },
                "required": ["crawl_id"],
            },
        ),

        # 3 ---- crawler_results ----------------------------------------
        Tool(
            name="crawler_results",
            description=(
                "Get comprehensive results from a completed or in-progress web crawl. Returns discovered pages, "
                "API endpoints, HTML forms, JavaScript-extracted endpoints, and sitemap URLs. Supports filtering "
                "by result_type for focused analysis. All endpoints already auto-added to world model - use this "
                "for detailed review, reporting, and manual endpoint selection. "

                "**When to use:** Phase 2 (Mapping) after crawler_status() shows 'completed' or to review partial "
                "results from running crawl. Use result_type='endpoints' to see all API endpoints for coverage_init(), "
                "result_type='forms' to identify input vectors for browser_test_xss() or form fuzzing, "
                "result_type='js_endpoints' to see hidden API calls found in JavaScript bundles, "
                "result_type='pages' for sitemap/navigation structure. Default result_type='all' for comprehensive view. "

                "**Dependencies:** Requires crawler_start() to have been called and returned crawl_id. Use "
                "crawler_status() first to verify crawl completion (status='completed') for full results. Can "
                "call on running crawls for partial results. Endpoints already in world model - this retrieves "
                "crawl-specific view with URLs, HTTP methods, parameters discovered. "

                "**Budget impact:** ZERO - local query of stored crawl results, no network requests. Results "
                "cached in memory during crawl, persisted to database on completion. Returns instantly even for "
                "crawls with 200+ pages and 500+ endpoints. "

                "**Failure modes:** 'Crawl not found' if crawl_id invalid or expired (>24h). Empty results {} if "
                "crawl in early stages (no pages visited yet). result_type filter case-sensitive (use exact values: "
                "'pages', 'endpoints', 'forms', 'js_endpoints', 'all'). Partial results if crawl failed mid-run "
                "(returns what was discovered before failure). "

                "**Risk level:** SAFE - read-only query of stored results, no network activity or target interaction. "

                "**Returns:** JSON with filtered results based on result_type: pages[] (URL, title, status_code, "
                "links_found, forms_found), endpoints[] (method, URL, parameters[], content_type, auth_required), "
                "forms[] (action_url, method, fields[], field_types[], field_names[]), js_endpoints[] (method, URL, "
                "source_file, call_type [fetch/axios/xhr]), sitemap_urls[] (from sitemap.xml). If result_type='all', "
                "returns object with all arrays. Use endpoints for coverage matrix, forms for injection testing, "
                "js_endpoints for hidden API discovery, pages for navigation mapping."
            ),
            inputSchema={
                "type": "object",
                "properties": {
                    "crawl_id": {
                        "type": "string",
                        "description": "Crawl ID returned by crawler_start",
                    },
                    "result_type": {
                        "type": "string",
                        "enum": ["pages", "endpoints", "forms", "js_endpoints", "all"],
                        "description": "Type of results to return (default: all)",
                    },
                },
                "required": ["crawl_id"],
            },
        ),
    ]


# ---------------------------------------------------------------------------
# Tool handler dispatch
# ---------------------------------------------------------------------------

async def handle_crawler_tool(
    name: str,
    arguments: dict,
    mcp_service: Any = None,
) -> List[TextContent]:
    """Dispatch a crawler tool call to the appropriate handler."""
    try:
        if name == "crawler_start":
            return await _handle_crawler_start(arguments, mcp_service)
        elif name == "crawler_status":
            return await _handle_crawler_status(arguments, mcp_service)
        elif name == "crawler_results":
            return await _handle_crawler_results(arguments, mcp_service)
    except Exception as exc:
        logger.error("Crawler tool %s failed: %s", name, exc, exc_info=True)
        return _error_content(f"Error in {name}: {exc}")

    return _error_content(f"Unknown crawler tool: {name}")


# ---------------------------------------------------------------------------
# Individual handlers
# ---------------------------------------------------------------------------

async def _handle_crawler_start(arguments: dict, mcp_service: Any = None) -> List[TextContent]:
    """Handle crawler_start - begin a new crawl."""
    start_url = arguments.get("start_url")
    if not start_url or not isinstance(start_url, str):
        return _error_content("start_url is required and must be a string")

    if not start_url.startswith(("http://", "https://")):
        return _error_content("start_url must include http:// or https://")

    from lib.web_crawler import CrawlConfig

    config = CrawlConfig(
        start_url=start_url,
        max_pages=arguments.get("max_pages", 200),
        max_depth=arguments.get("max_depth", 10),
        strategy=arguments.get("strategy", "bfs"),
        identity_id=arguments.get("identity_id"),
        extract_js=arguments.get("extract_js", True),
        parse_sitemap=arguments.get("parse_sitemap", True),
        exclude_patterns=arguments.get("exclude_patterns", []),
        session_id=arguments.get("session_id"),
    )

    crawler = _get_crawler(mcp_service)

    # Set the DB for this crawl from current assessment
    if mcp_service and getattr(mcp_service, "current_assessment_id", None):
        from lib.world_model_db import get_world_model_db
        crawler.db = await get_world_model_db(mcp_service.current_assessment_id)

    result = await crawler.crawl(config)

    # Auto-notify discovery monitor about new endpoints
    if mcp_service and getattr(mcp_service, "current_assessment_id", None):
        try:
            await _notify_discovery(mcp_service, result)
        except Exception as exc:
            logger.debug("Discovery notification skipped: %s", exc)

    return _json_content({
        "success": True,
        **result,
        "message": (
            f"Crawl completed: {result.get('pages_crawled', 0)} pages visited, "
            f"{result.get('endpoints_found', 0)} endpoints found, "
            f"{result.get('forms_found', 0)} forms discovered, "
            f"{result.get('js_endpoints_found', 0)} JS endpoints extracted."
        ),
    })


async def _handle_crawler_status(arguments: dict, mcp_service: Any = None) -> List[TextContent]:
    """Handle crawler_status - check crawl progress."""
    crawl_id = arguments.get("crawl_id")
    if not crawl_id or not isinstance(crawl_id, str):
        return _error_content("crawl_id is required")

    crawler = _get_crawler(mcp_service)
    status = await crawler.get_status(crawl_id)

    if status is None:
        return _error_content(f"Crawl not found: {crawl_id}")

    return _json_content({
        "success": True,
        **status,
    })


async def _handle_crawler_results(arguments: dict, mcp_service: Any = None) -> List[TextContent]:
    """Handle crawler_results - get crawl output."""
    crawl_id = arguments.get("crawl_id")
    if not crawl_id or not isinstance(crawl_id, str):
        return _error_content("crawl_id is required")

    result_type = arguments.get("result_type", "all")

    crawler = _get_crawler(mcp_service)
    results = await crawler.get_results(crawl_id, result_type)

    if results is None:
        return _error_content(f"Crawl not found: {crawl_id}")

    return _json_content({
        "success": True,
        **results,
    })


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

async def _notify_discovery(mcp_service: Any, crawl_result: Dict) -> None:
    """Best-effort notification to service about newly discovered endpoints.

    This allows the discovery feedback loop (coverage_discover) to pick up
    new endpoints without requiring a manual call.
    """
    if hasattr(mcp_service, "_on_endpoint_discovered"):
        endpoints_found = crawl_result.get("endpoints_found", 0)
        if endpoints_found > 0:
            await mcp_service._on_endpoint_discovered({
                "source": "crawler",
                "count": endpoints_found,
                "crawl_id": crawl_result.get("crawl_id", ""),
            })
