"""
Web Crawler - BFS/DFS crawling engine for security assessments.

Crawls web applications using Playwright, discovers pages, forms, and
JavaScript-referenced endpoints.  Automatically populates the world model
with discovered assets, endpoints, and relationships.
"""

from __future__ import annotations

import asyncio
import collections
import fnmatch
import json
import logging
import re
import time
import uuid
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Set, Tuple
from urllib.parse import urljoin, urlparse, urlunparse, parse_qs

logger = logging.getLogger("autopentest-mcp")

# ---------------------------------------------------------------------------
# JS endpoint regex patterns
# ---------------------------------------------------------------------------

JS_ENDPOINT_PATTERNS: List[re.Pattern] = [
    re.compile(r"""(?:fetch|axios\.(?:get|post|put|delete|patch))\s*\(\s*['"](\/[^'"]+)['"]""", re.I),
    re.compile(r"""\$\.(?:ajax|get|post)\s*\(\s*(?:\{[^}]*url\s*:\s*)?['"](\/[^'"]+)['"]""", re.I),
    re.compile(r"""\.open\s*\(\s*['"][A-Z]+['"]\s*,\s*['"](\/[^'"]+)['"]"""),
    re.compile(r"""['"](\/(api|v\d|rest|graphql)\b[^'"]*?)['"]"""),
]

_SKIP_EXTENSIONS = frozenset({
    ".png", ".jpg", ".jpeg", ".gif", ".svg", ".ico", ".webp", ".bmp",
    ".pdf", ".zip", ".gz", ".tar", ".rar", ".7z",
    ".mp3", ".mp4", ".avi", ".mov", ".wmv", ".flv", ".webm",
    ".woff", ".woff2", ".ttf", ".eot", ".otf", ".css", ".map",
})

# ---------------------------------------------------------------------------
# Data classes
# ---------------------------------------------------------------------------

@dataclass
class CrawlConfig:
    """Configuration for a crawl session."""
    start_url: str
    max_pages: int = 200
    max_depth: int = 10
    strategy: str = "bfs"
    identity_id: Optional[str] = None
    extract_js: bool = True
    parse_sitemap: bool = True
    exclude_patterns: List[str] = field(default_factory=list)
    session_id: Optional[str] = None

    def __post_init__(self) -> None:
        if self.strategy not in ("bfs", "dfs"):
            raise ValueError(f"strategy must be 'bfs' or 'dfs', got '{self.strategy}'")
        if self.max_pages < 1:
            raise ValueError("max_pages must be >= 1")
        if self.max_depth < 1:
            raise ValueError("max_depth must be >= 1")


@dataclass
class CrawlState:
    """Mutable state tracked during a crawl."""
    crawl_id: str
    visited: Set[str] = field(default_factory=set)
    queue: collections.deque = field(default_factory=collections.deque)
    pages_crawled: int = 0
    endpoints_found: int = 0
    forms_found: int = 0
    js_endpoints_found: int = 0
    errors: int = 0
    start_time: float = 0.0
    status: str = "running"


@dataclass
class CrawlResult:
    """Result of crawling a single page."""
    url: str
    status_code: int
    links: List[str] = field(default_factory=list)
    forms: List[Dict] = field(default_factory=list)
    js_endpoints: List[str] = field(default_factory=list)
    parameters: List[Dict] = field(default_factory=list)


# ---------------------------------------------------------------------------
# WebCrawler
# ---------------------------------------------------------------------------

class WebCrawler:
    """BFS/DFS web crawling engine with world-model integration.

    Discovers pages, forms, and JS-referenced API endpoints. All findings
    are persisted to the world model for downstream vulnerability testing.
    """

    _active_crawls: Dict[str, CrawlState] = {}
    _crawl_results: Dict[str, List[CrawlResult]] = {}

    def __init__(self, session_manager: Any, scope_validator: Any,
                 db: Any, mcp_service: Any = None) -> None:
        self._session_manager = session_manager
        self._scope_validator = scope_validator
        self._db = db
        self._mcp_service = mcp_service
        self._known_endpoints: Set[Tuple[str, str]] = set()
        self._known_assets: Dict[str, str] = {}

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    async def crawl(self, config: CrawlConfig) -> Dict[str, Any]:
        """Execute a BFS/DFS crawl from *config.start_url*.  Returns summary dict."""
        crawl_id = str(uuid.uuid4())
        state = CrawlState(crawl_id=crawl_id, start_time=time.time())
        WebCrawler._active_crawls[crawl_id] = state
        WebCrawler._crawl_results[crawl_id] = []

        parsed_start = urlparse(config.start_url)
        base_url = f"{parsed_start.scheme}://{parsed_start.netloc}"
        logger.info("Starting %s crawl %s on %s (max_pages=%d, max_depth=%d)",
                     config.strategy.upper(), crawl_id, config.start_url,
                     config.max_pages, config.max_depth)

        # Obtain or create browser session
        session = None
        try:
            if config.session_id:
                session = self._session_manager.get_session(config.session_id)
            if session is None:
                session = self._session_manager.get_active_session()
            if session is None:
                session = await self._session_manager.create_session({"headless": True})
        except Exception as exc:
            logger.error("Failed to initialise browser session: %s", exc)
            state.status = "stopped"
            state.errors += 1
            return self._build_summary(state)

        page = session.get_page()

        # Seed the queue
        state.queue.append((config.start_url, 0))
        if config.parse_sitemap:
            await self._seed_from_sitemap(state, base_url)

        await self._ensure_asset(parsed_start.netloc)

        # Main crawl loop
        try:
            while state.queue and state.pages_crawled < config.max_pages and state.status == "running":
                url, depth = state.queue.popleft() if config.strategy == "bfs" else state.queue.pop()

                normalized = self._normalize_url(url, base_url)
                if normalized is None or normalized in state.visited:
                    continue
                if depth > config.max_depth:
                    continue
                if self._should_skip_url(normalized, config.exclude_patterns):
                    continue
                ext = self._get_extension(urlparse(normalized).path)
                if ext in _SKIP_EXTENSIONS:
                    continue
                if self._scope_validator is not None:
                    try:
                        vr = self._scope_validator.validate_target(normalized)
                        if not vr.valid:
                            continue
                    except Exception:
                        continue

                state.visited.add(normalized)
                crawl_result = CrawlResult(url=normalized, status_code=0)

                # Navigate
                try:
                    nav = await session.navigate(normalized, wait_until="networkidle", timeout=30000)
                    crawl_result.status_code = nav.get("status") or 0
                except Exception as exc:
                    logger.debug("Navigation failed for %s: %s", normalized, exc)
                    state.errors += 1
                    WebCrawler._crawl_results[crawl_id].append(crawl_result)
                    continue

                state.pages_crawled += 1

                # Extract links
                try:
                    crawl_result.links = await self._extract_links(page)
                except Exception:
                    pass
                # Extract JS endpoints
                if config.extract_js:
                    try:
                        crawl_result.js_endpoints = await self._extract_js_endpoints(page)
                        state.js_endpoints_found += len(crawl_result.js_endpoints)
                    except Exception:
                        pass
                # Extract forms
                try:
                    crawl_result.forms = await self._extract_forms(page)
                    state.forms_found += len(crawl_result.forms)
                except Exception:
                    pass

                crawl_result.parameters = self._extract_parameters(normalized)

                # Persist and capture
                await self._persist_page(crawl_result, base_url)
                await self._capture_page_knowledge(page, normalized)

                # Enqueue discovered links (scope-filtered)
                for link in crawl_result.links:
                    ln = self._normalize_url(link, base_url)
                    if ln and ln not in state.visited and not self._should_skip_url(ln, config.exclude_patterns):
                        if self._scope_validator is not None:
                            try:
                                if not self._scope_validator.validate_target(ln).valid:
                                    continue
                            except Exception:
                                continue
                        state.queue.append((ln, depth + 1))

                # Enqueue JS endpoints as full URLs
                for js_ep in crawl_result.js_endpoints:
                    jn = self._normalize_url(urljoin(base_url, js_ep), base_url)
                    if jn and jn not in state.visited:
                        state.queue.append((jn, depth + 1))

                WebCrawler._crawl_results[crawl_id].append(crawl_result)

                if state.pages_crawled % 10 == 0:
                    logger.info("Crawl %s: %d pages, %d endpoints, %d forms, %d JS, %d errors",
                                crawl_id, state.pages_crawled, state.endpoints_found,
                                state.forms_found, state.js_endpoints_found, state.errors)
                await asyncio.sleep(0)

        except Exception as exc:
            logger.error("Crawl %s aborted: %s", crawl_id, exc)
            state.errors += 1
        finally:
            if state.status == "running":
                state.status = "completed"

        await self._save_crawl_state(state, config)
        logger.info("Crawl %s done: %d pages in %.1fs", crawl_id,
                     state.pages_crawled, time.time() - state.start_time)
        return self._build_summary(state)

    async def get_status(self, crawl_id: str) -> Dict[str, Any]:
        """Return current crawl state for *crawl_id*."""
        state = WebCrawler._active_crawls.get(crawl_id)
        if state is None:
            return {"crawl_id": crawl_id, "status": "not_found"}
        return self._build_summary(state)

    async def get_results(self, crawl_id: str, result_type: str = "all") -> Dict[str, Any]:
        """Return crawl results filtered by *result_type* (pages/endpoints/forms/js_endpoints/all)."""
        results = WebCrawler._crawl_results.get(crawl_id)
        if results is None:
            return {"crawl_id": crawl_id, "status": "not_found", "results": []}

        if result_type == "pages":
            return {"crawl_id": crawl_id, "pages": [
                {"url": r.url, "status_code": r.status_code, "link_count": len(r.links)} for r in results]}
        elif result_type == "endpoints":
            eps, seen = [], set()
            for r in results:
                d = self._url_to_endpoint_data(r.url)
                k = f"{d['method']}:{d['path']}"
                if k not in seen:
                    seen.add(k); eps.append(d)
            return {"crawl_id": crawl_id, "endpoints": eps}
        elif result_type == "forms":
            forms = []
            for r in results:
                for f in r.forms:
                    fc = dict(f); fc["page_url"] = r.url; forms.append(fc)
            return {"crawl_id": crawl_id, "forms": forms}
        elif result_type == "js_endpoints":
            js = set()
            for r in results:
                js.update(r.js_endpoints)
            return {"crawl_id": crawl_id, "js_endpoints": sorted(js)}
        else:  # 'all'
            pages, ep_set, eps, forms, js = [], set(), [], [], set()
            for r in results:
                pages.append({"url": r.url, "status_code": r.status_code, "link_count": len(r.links)})
                d = self._url_to_endpoint_data(r.url)
                k = f"{d['method']}:{d['path']}"
                if k not in ep_set:
                    ep_set.add(k); eps.append(d)
                for f in r.forms:
                    fc = dict(f); fc["page_url"] = r.url; forms.append(fc)
                js.update(r.js_endpoints)
            return {"crawl_id": crawl_id, "pages": pages, "endpoints": eps,
                    "forms": forms, "js_endpoints": sorted(js)}

    # ------------------------------------------------------------------
    # Extraction helpers
    # ------------------------------------------------------------------

    async def _extract_links(self, page: Any) -> List[str]:
        """Extract all href attributes from a/area/link tags, resolve relative URLs."""
        raw: List[str] = await page.evaluate("""() => {
            const hrefs = new Set();
            for (const sel of ['a[href]', 'area[href]', 'link[href]']) {
                document.querySelectorAll(sel).forEach(el => {
                    const h = el.getAttribute('href');
                    if (h && !h.startsWith('javascript:') && !h.startsWith('mailto:')
                        && !h.startsWith('tel:') && !h.startsWith('data:') && h !== '#')
                        hrefs.add(h);
                });
            }
            return Array.from(hrefs);
        }""")
        base = page.url
        resolved, seen = [], set()
        for href in raw:
            try:
                p = urlparse(urljoin(base, href))
                clean = urlunparse((p.scheme, p.netloc, p.path, p.params, p.query, ""))
                if clean not in seen:
                    seen.add(clean); resolved.append(clean)
            except Exception:
                continue
        return resolved

    async def _extract_js_endpoints(self, page: Any) -> List[str]:
        """Extract API paths from inline script tags using JS_ENDPOINT_PATTERNS."""
        scripts: List[str] = await page.evaluate("""() => {
            const c = [];
            document.querySelectorAll('script').forEach(s => {
                if (s.textContent && s.textContent.trim().length > 0) c.push(s.textContent);
            });
            return c;
        }""")
        eps: Set[str] = set()
        for content in scripts:
            for pat in JS_ENDPOINT_PATTERNS:
                for m in pat.finditer(content):
                    ep = m.group(1)
                    if ep and len(ep) > 1 and not ep.startswith("//"):
                        eps.add(ep.rstrip("; \t\n\r"))
        return sorted(eps)

    async def _extract_forms(self, page: Any) -> List[Dict]:
        """Discover HTML forms via FormAnalyzer."""
        try:
            from lib.browser_forms import FormAnalyzer
        except ImportError:
            from browser_forms import FormAnalyzer
        return await FormAnalyzer().discover_forms(page)

    # ------------------------------------------------------------------
    # Sitemap & robots.txt
    # ------------------------------------------------------------------

    async def _seed_from_sitemap(self, state: CrawlState, base_url: str) -> None:
        """Seed the crawl queue from sitemap.xml and robots.txt."""
        try:
            for url in await self._parse_sitemap(base_url):
                if url not in state.visited:
                    state.queue.append((url, 1))
        except Exception as exc:
            logger.debug("Sitemap parsing failed: %s", exc)

        try:
            paths, extra_sitemaps = await self._parse_robots_txt(base_url)
            for p in paths:
                full = urljoin(base_url, p)
                if full not in state.visited:
                    state.queue.append((full, 1))
            for sm in extra_sitemaps:
                try:
                    for url in await self._parse_sitemap(sm):
                        if url not in state.visited:
                            state.queue.append((url, 1))
                except Exception:
                    pass
        except Exception as exc:
            logger.debug("robots.txt parsing failed: %s", exc)

    async def _parse_sitemap(self, base_url: str) -> List[str]:
        """Fetch sitemap.xml and extract <loc> URLs.  Supports sitemap index files."""
        sitemap_url = f"{base_url.rstrip('/')}/sitemap.xml"
        content = await self._fetch_text(sitemap_url)
        if not content:
            return []

        loc_re = re.compile(r"<loc>\s*(.*?)\s*</loc>", re.I | re.DOTALL)
        locs = loc_re.findall(content)

        urls: List[str] = []
        if "<sitemapindex" in content.lower():
            for child_url in locs:
                try:
                    child = await self._fetch_text(child_url.strip())
                    if child:
                        urls.extend(l.strip() for l in loc_re.findall(child) if l.strip())
                except Exception:
                    pass
        else:
            urls = [l.strip() for l in locs if l.strip()]

        logger.info("Sitemap: %d URLs from %s", len(urls), sitemap_url)
        return urls

    async def _parse_robots_txt(self, base_url: str) -> Tuple[List[str], List[str]]:
        """Parse robots.txt, return (interesting_paths, sitemap_urls)."""
        content = await self._fetch_text(f"{base_url.rstrip('/')}/robots.txt")
        if not content:
            return [], []

        paths: List[str] = []
        sitemaps: List[str] = []
        for line in content.splitlines():
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            lower = line.lower()
            if lower.startswith(("allow:", "disallow:")):
                p = line.split(":", 1)[1].strip()
                if p and p != "/":
                    paths.append(p)
            elif lower.startswith("sitemap:"):
                parts = line.split(None, 1)
                if len(parts) > 1 and parts[1].startswith("http"):
                    sitemaps.append(parts[1].strip())
        return paths, sitemaps

    # ------------------------------------------------------------------
    # URL utilities
    # ------------------------------------------------------------------

    @staticmethod
    def _normalize_url(url: str, base_url: str) -> Optional[str]:
        """Resolve relative URL, strip fragment, lowercase scheme+host.  None if invalid."""
        try:
            p = urlparse(urljoin(base_url, url))
            if p.scheme not in ("http", "https") or not p.netloc:
                return None
            return urlunparse((p.scheme.lower(), p.netloc.lower(),
                               p.path or "/", p.params, p.query, ""))
        except Exception:
            return None

    @staticmethod
    def _should_skip_url(url: str, exclude_patterns: List[str]) -> bool:
        """True if url matches any glob or regex exclude pattern."""
        for pat in exclude_patterns:
            if fnmatch.fnmatch(url, pat):
                return True
            if pat.startswith("^") or "(" in pat:
                try:
                    if re.search(pat, url):
                        return True
                except re.error:
                    pass
        return False

    @staticmethod
    def _url_to_endpoint_data(url: str) -> Dict[str, Any]:
        """Parse URL into {method, path, parameters} for world model."""
        p = urlparse(url)
        params = parse_qs(p.query, keep_blank_values=True)
        schema = {n: {"location": "query", "type": "string",
                       "example": v[0] if v else ""} for n, v in params.items()}
        return {"method": "GET", "path": p.path or "/", "parameters": schema}

    @staticmethod
    def _get_extension(path: str) -> str:
        """Lowercase file extension from URL path, or empty string."""
        i = path.rfind(".")
        if i == -1:
            return ""
        ext = path[i:].lower()
        return "" if "/" in ext else ext

    @staticmethod
    def _extract_parameters(url: str) -> List[Dict]:
        """Extract query parameters from URL as list of dicts."""
        params = parse_qs(urlparse(url).query, keep_blank_values=True)
        return [{"name": n, "value": v[0] if v else "", "location": "query"}
                for n, v in params.items()]

    # ------------------------------------------------------------------
    # World model persistence
    # ------------------------------------------------------------------

    async def _ensure_asset(self, hostname: str) -> str:
        """Ensure a domain asset exists for *hostname*, return its id."""
        if hostname in self._known_assets:
            return self._known_assets[hostname]
        try:
            r = await self._db.add_asset(kind="domain", name=hostname,
                                         metadata={"discovered_by": "web_crawler"}, tags=["crawled"])
            self._known_assets[hostname] = r["id"]
            return r["id"]
        except Exception:
            fid = f"asset-{hostname}"
            self._known_assets[hostname] = fid
            return fid

    async def _add_endpoint_safe(self, asset_id: str, method: str, path: str,
                                 params: Dict, meta: Dict) -> None:
        """Add endpoint + has_endpoint relationship, skip if already known."""
        key = (method, path)
        if key in self._known_endpoints:
            return
        try:
            ep = await self._db.add_endpoint(
                asset_id=asset_id, method=method, path=path,
                parameters=params, auth_required=False, metadata=meta)
            self._known_endpoints.add(key)
            try:
                await self._db.add_relationship(
                    source_type="asset", source_id=asset_id,
                    target_type="endpoint", target_id=ep["id"],
                    rel_type="has_endpoint")
            except Exception:
                pass
            for s in WebCrawler._active_crawls.values():
                if s.status == "running":
                    s.endpoints_found += 1
                    break
        except Exception as exc:
            logger.debug("Failed to add endpoint %s %s: %s", method, path, exc)

    async def _persist_page(self, result: CrawlResult, base_url: str) -> None:
        """Persist page, form, and JS endpoints to world model."""
        hostname = urlparse(result.url).netloc.lower()
        asset_id = await self._ensure_asset(hostname)

        # Page endpoint
        ep = self._url_to_endpoint_data(result.url)
        await self._add_endpoint_safe(asset_id, ep["method"], ep["path"], ep["parameters"],
                                      {"status_code": result.status_code, "discovered_by": "web_crawler",
                                       "source_url": result.url})

        # Form action endpoints
        for form in result.forms:
            action = form.get("action", "")
            if not action:
                continue
            ap = urlparse(urljoin(result.url, action))
            fm = (form.get("method") or "GET").upper()
            fp = ap.path or "/"
            form_params: Dict[str, Any] = {}
            for fld in form.get("fields", []):
                n = fld.get("name", "")
                if n:
                    form_params[n] = {"location": "body" if fm == "POST" else "query",
                                      "type": fld.get("type", "string"),
                                      "required": fld.get("required", False)}
            await self._add_endpoint_safe(asset_id, fm, fp, form_params,
                                          {"discovered_by": "web_crawler", "source": "form",
                                           "form_id": form.get("form_id", ""), "page_url": result.url})

        # JS-discovered endpoints
        for js_ep in result.js_endpoints:
            jp = js_ep if js_ep.startswith("/") else f"/{js_ep}"
            await self._add_endpoint_safe(asset_id, "GET", jp, {},
                                          {"discovered_by": "web_crawler", "source": "javascript",
                                           "page_url": result.url})

    # ------------------------------------------------------------------
    # Knowledge capture
    # ------------------------------------------------------------------

    async def _capture_page_knowledge(self, page: Any, url: str) -> None:
        """Store page HTML in the knowledge store (capped at 64KB)."""
        if self._mcp_service is None:
            return
        try:
            content = await page.content()
            if not content or len(content) < 50:
                return
            try:
                from lib.knowledge_capture import capture_knowledge
            except ImportError:
                from knowledge_capture import capture_knowledge
            await capture_knowledge(
                mcp_service=self._mcp_service, source_tool="web_crawler",
                category="page_content", title=f"Page: {url}",
                content=content[:65536], target=url,
                metadata={"url": url, "content_length": len(content)},
                tags=["crawl", "page_content"])
        except Exception as exc:
            logger.debug("Page knowledge capture failed for %s: %s", url, exc)

    async def _save_crawl_state(self, state: CrawlState, config: CrawlConfig) -> None:
        """Save final crawl summary to knowledge store for resumability."""
        if self._mcp_service is None:
            return
        try:
            summary = {
                "crawl_id": state.crawl_id, "start_url": config.start_url,
                "strategy": config.strategy, "pages_crawled": state.pages_crawled,
                "endpoints_found": state.endpoints_found, "forms_found": state.forms_found,
                "js_endpoints_found": state.js_endpoints_found, "errors": state.errors,
                "elapsed_seconds": round(time.time() - state.start_time, 2),
                "status": state.status, "visited_urls": sorted(state.visited),
            }
            try:
                from lib.knowledge_capture import capture_knowledge
            except ImportError:
                from knowledge_capture import capture_knowledge
            await capture_knowledge(
                mcp_service=self._mcp_service, source_tool="web_crawler",
                category="scan_output", title=f"Crawl summary: {config.start_url}",
                content=json.dumps(summary, indent=2), target=config.start_url,
                metadata={"crawl_id": state.crawl_id, "pages": state.pages_crawled},
                tags=["crawl", "summary"])
        except Exception as exc:
            logger.debug("Failed to save crawl state: %s", exc)

    # ------------------------------------------------------------------
    # HTTP helper
    # ------------------------------------------------------------------

    async def _fetch_text(self, url: str) -> Optional[str]:
        """Fetch URL text via browser page.evaluate(fetch(...)).  Returns None on failure."""
        session = self._session_manager.get_active_session()
        if session is None:
            return None
        page = session.get_page()
        if page is None:
            return None
        try:
            return await page.evaluate("""async (url) => {
                try {
                    const r = await fetch(url, {method:'GET', credentials:'same-origin'});
                    return r.ok ? await r.text() : null;
                } catch(e) { return null; }
            }""", url)
        except Exception:
            return None

    # ------------------------------------------------------------------
    # Summary
    # ------------------------------------------------------------------

    @staticmethod
    def _build_summary(state: CrawlState) -> Dict[str, Any]:
        """Build a summary dict from crawl state."""
        elapsed = time.time() - state.start_time if state.start_time else 0
        return {
            "crawl_id": state.crawl_id, "status": state.status,
            "pages_crawled": state.pages_crawled, "endpoints_found": state.endpoints_found,
            "forms_found": state.forms_found, "js_endpoints_found": state.js_endpoints_found,
            "errors": state.errors, "elapsed_seconds": round(elapsed, 2),
            "urls_visited": len(state.visited), "queue_remaining": len(state.queue),
        }
