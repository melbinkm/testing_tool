"""Hallucination filter and report validation for AutoPentest findings.

Validates findings against evidence artifacts to detect speculative content,
missing evidence references, and unsupported claims. Used as a pre-step
before report generation and as a standalone validation tool.
"""
from __future__ import annotations

import re
import logging
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

logger = logging.getLogger("autopentest-mcp")

# Phrases that indicate speculation rather than confirmed findings
SPECULATIVE_PHRASES = [
    "potential",
    "might be",
    "could be",
    "possibly",
    "may be vulnerable",
    "appears to",
    "seems to",
    "likely vulnerable",
    "suspected",
    "unconfirmed",
    "theoretical",
    "hypothetical",
    "not yet verified",
    "further testing needed",
    "needs confirmation",
]

# Compiled pattern for efficient matching (case-insensitive, word boundaries)
_SPECULATIVE_PATTERN = re.compile(
    r'\b(' + '|'.join(re.escape(p) for p in SPECULATIVE_PHRASES) + r')\b',
    re.IGNORECASE,
)


@dataclass
class ValidationResult:
    """Result of finding validation."""
    is_valid: bool
    issues: List[str] = field(default_factory=list)
    confidence: float = 1.0


def validate_finding(
    finding: Dict[str, Any],
    evidence_artifacts: Optional[List[Dict[str, Any]]] = None,
) -> ValidationResult:
    """Validate a single finding against its evidence.

    Checks:
    - Finding has non-empty description or title
    - High/critical severity findings have at least one evidence reference
    - Description does not consist entirely of speculative phrases
    - Referenced evidence IDs exist in provided artifacts

    Args:
        finding: Finding dict with keys like title, description, severity,
                 evidence_ids, metadata.
        evidence_artifacts: List of evidence/artifact dicts with 'id' keys.

    Returns:
        ValidationResult with is_valid, issues list, and confidence score.
    """
    issues: List[str] = []
    evidence_artifacts = evidence_artifacts or []

    finding_id = finding.get("id", "unknown")
    title = finding.get("title", "")
    severity = (finding.get("severity") or "").lower()

    # Check 1: non-empty description or title
    description = finding.get("description") or finding.get("metadata", {}).get("description", "")
    if not title and not description:
        issues.append("Finding has no title or description")

    # Check 2: high/critical findings need evidence references
    if severity in ("high", "critical"):
        evidence_ids = finding.get("evidence_ids") or []
        has_metadata_evidence = bool(finding.get("metadata", {}).get("evidence"))
        if not evidence_ids and not has_metadata_evidence and not evidence_artifacts:
            issues.append(
                f"Severity '{severity}' finding has no evidence references"
            )

    # Check 3: description not entirely speculative
    text_to_check = f"{title} {description}".strip()
    if text_to_check:
        cleaned = _SPECULATIVE_PATTERN.sub("", text_to_check).strip()
        # Remove punctuation and whitespace to check if anything substantive remains
        substantive = re.sub(r'[\s\W]+', '', cleaned)
        if not substantive:
            issues.append("Finding description consists entirely of speculative phrases")

    # Check 4: referenced evidence IDs exist in artifacts
    evidence_ids = finding.get("evidence_ids") or []
    if evidence_ids and evidence_artifacts:
        artifact_ids = {a.get("id") for a in evidence_artifacts if a.get("id")}
        missing = [eid for eid in evidence_ids if eid not in artifact_ids]
        if missing:
            issues.append(
                f"Referenced evidence IDs not found in artifacts: {', '.join(missing)}"
            )

    # Calculate confidence: 1.0 minus 0.25 per issue (floor at 0.0)
    confidence = max(0.0, 1.0 - 0.25 * len(issues))
    is_valid = len(issues) == 0

    return ValidationResult(
        is_valid=is_valid,
        issues=issues,
        confidence=confidence,
    )


def filter_speculative_content(text: str) -> str:
    """Remove sentences containing speculative phrases from text.

    Splits text into sentences, removes those containing speculative
    phrases, and returns the cleaned text.

    Args:
        text: Input text to filter.

    Returns:
        Cleaned text with speculative sentences removed.
    """
    if not text:
        return text

    # Split on sentence boundaries (period, exclamation, question mark followed by space or end)
    sentences = re.split(r'(?<=[.!?])\s+', text)

    filtered = []
    for sentence in sentences:
        if not _SPECULATIVE_PATTERN.search(sentence):
            filtered.append(sentence)

    return " ".join(filtered).strip()


async def verify_evidence_integrity(
    finding_id: str,
    db: Any,
) -> ValidationResult:
    """Verify that a finding's evidence references are intact in the database.

    Queries wm_findings for the finding, then checks that referenced
    evidence IDs exist in wm_knowledge or wm_observations.

    Args:
        finding_id: The finding ID to verify.
        db: WorldModelDatabase instance.

    Returns:
        ValidationResult with integrity check results.
    """
    issues: List[str] = []

    # Fetch the finding
    finding = await db.get_by_id("findings", finding_id)
    if finding is None:
        return ValidationResult(
            is_valid=False,
            issues=[f"Finding not found: {finding_id}"],
            confidence=0.0,
        )

    # Check evidence_ids references
    evidence_ids = finding.get("evidence_ids") or []
    if not evidence_ids:
        # Not necessarily invalid, but note it
        if (finding.get("severity") or "").lower() in ("high", "critical"):
            issues.append("High/critical finding has no evidence_ids references")
    else:
        for eid in evidence_ids:
            # Check in knowledge store
            knowledge = await db.get_by_id("knowledge", eid)
            if knowledge is None:
                # Check in observations
                observation = await db.get_by_id("observations", eid)
                if observation is None:
                    issues.append(f"Evidence reference '{eid}' not found in knowledge or observations")

    confidence = max(0.0, 1.0 - 0.25 * len(issues))
    return ValidationResult(
        is_valid=len(issues) == 0,
        issues=issues,
        confidence=confidence,
    )
