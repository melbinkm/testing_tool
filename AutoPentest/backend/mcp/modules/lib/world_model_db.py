"""
World Model Database - SQLite-backed persistence for the pentesting world model.

Ported from world-model-mcp/src/database.ts to Python.

Stores assets, endpoints, identities, hypotheses, observations, and findings
in a local SQLite database.  Uses aiosqlite for async I/O when available,
falling back to synchronous sqlite3 wrapped in ``asyncio.to_thread``.
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import sqlite3
import uuid
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

logger = logging.getLogger("autopentest-mcp")

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

DEFAULT_DB_PATH = "./data/world-model.db"

# Valid enum values for validation
ASSET_KINDS = ("domain", "subdomain", "ip", "service", "application")
ENDPOINT_STATUSES = ("discovered", "tested", "vulnerable", "safe")
IDENTITY_STATUSES = ("active", "revoked", "expired")
HYPOTHESIS_SEVERITIES = ("info", "low", "medium", "high", "critical")
HYPOTHESIS_STATUSES = ("proposed", "testing", "confirmed", "rejected")
FINDING_STATUSES = ("draft", "confirmed", "reported", "remediated")
OBSERVATION_TYPES = ("request", "response", "behavior", "anomaly")
PLAN_STATUSES = ("active", "completed", "abandoned")
PLAN_STEP_STATUSES = ("pending", "in_progress", "done", "skipped")

# Graph / relationship constants
VALID_ENTITY_TYPES = ("asset", "endpoint", "identity", "hypothesis", "finding")
VALID_REL_TYPES = (
    "domain_has_subdomain", "resolves_to", "has_endpoint", "has_finding",
    "tested_by", "targets", "confirms",
)

# Coverage matrix constants
COVERAGE_STATUSES = ("pending", "in_progress", "passed", "vulnerable", "skipped", "error")
VULN_CLASSES = (
    "sqli", "xss", "idor", "injection", "auth_bypass", "ssrf",
    "path_traversal", "overflow", "type_confusion", "info_disclosure",
    "misconfig", "nuclei",
)

# Knowledge store constants
CHUNK_SIZE = 65536  # 64KB
KNOWLEDGE_CATEGORIES = (
    "scan_output", "page_content", "http_exchange",
    "fuzz_result", "command_output", "error",
    "form_data", "navigation", "timing", "other",
)

# ---------------------------------------------------------------------------
# Schema DDL
# ---------------------------------------------------------------------------

_SCHEMA_SQL = """
CREATE TABLE IF NOT EXISTS assets (
    id          TEXT PRIMARY KEY,
    kind        TEXT NOT NULL,
    name        TEXT NOT NULL,
    metadata    TEXT NOT NULL DEFAULT '{}',
    tags        TEXT NOT NULL DEFAULT '[]',
    discovered_at TEXT NOT NULL,
    updated_at  TEXT NOT NULL
);

CREATE TABLE IF NOT EXISTS endpoints (
    id            TEXT PRIMARY KEY,
    asset_id      TEXT NOT NULL,
    method        TEXT NOT NULL,
    path          TEXT NOT NULL,
    parameters    TEXT NOT NULL DEFAULT '{}',
    auth_required INTEGER NOT NULL DEFAULT 0,
    status        TEXT NOT NULL DEFAULT 'discovered',
    metadata      TEXT NOT NULL DEFAULT '{}',
    discovered_at TEXT NOT NULL,
    updated_at    TEXT NOT NULL,
    FOREIGN KEY (asset_id) REFERENCES assets(id)
);

CREATE TABLE IF NOT EXISTS identities (
    id          TEXT PRIMARY KEY,
    name        TEXT NOT NULL,
    auth_type   TEXT NOT NULL,
    scope       TEXT NOT NULL,
    permissions TEXT NOT NULL DEFAULT '[]',
    status      TEXT NOT NULL DEFAULT 'active',
    metadata    TEXT NOT NULL DEFAULT '{}',
    created_at  TEXT NOT NULL,
    updated_at  TEXT NOT NULL
);

CREATE TABLE IF NOT EXISTS hypotheses (
    id          TEXT PRIMARY KEY,
    title       TEXT NOT NULL,
    description TEXT NOT NULL,
    severity    TEXT NOT NULL DEFAULT 'info',
    status      TEXT NOT NULL DEFAULT 'proposed',
    evidence    TEXT NOT NULL DEFAULT '[]',
    target_id   TEXT,
    created_at  TEXT NOT NULL,
    updated_at  TEXT NOT NULL
);

CREATE TABLE IF NOT EXISTS observations (
    id            TEXT PRIMARY KEY,
    hypothesis_id TEXT NOT NULL,
    type          TEXT NOT NULL,
    content       TEXT NOT NULL,
    metadata      TEXT NOT NULL DEFAULT '{}',
    observed_at   TEXT NOT NULL,
    FOREIGN KEY (hypothesis_id) REFERENCES hypotheses(id)
);

CREATE TABLE IF NOT EXISTS findings (
    id            TEXT PRIMARY KEY,
    hypothesis_id TEXT NOT NULL,
    title         TEXT NOT NULL,
    severity      TEXT NOT NULL,
    confidence    REAL NOT NULL DEFAULT 0.0,
    status        TEXT NOT NULL DEFAULT 'draft',
    evidence_ids  TEXT NOT NULL DEFAULT '[]',
    remediation   TEXT,
    metadata      TEXT NOT NULL DEFAULT '{}',
    created_at    TEXT NOT NULL,
    updated_at    TEXT NOT NULL,
    FOREIGN KEY (hypothesis_id) REFERENCES hypotheses(id)
);

CREATE TABLE IF NOT EXISTS knowledge (
    id           TEXT PRIMARY KEY,
    source_tool  TEXT NOT NULL,
    category     TEXT NOT NULL,
    target       TEXT NOT NULL DEFAULT '',
    title        TEXT NOT NULL,
    content      TEXT NOT NULL,
    chunk_index  INTEGER NOT NULL DEFAULT 0,
    chunk_total  INTEGER NOT NULL DEFAULT 1,
    parent_id    TEXT,
    content_size INTEGER NOT NULL DEFAULT 0,
    embedding    BLOB,
    metadata     TEXT NOT NULL DEFAULT '{}',
    tags         TEXT NOT NULL DEFAULT '[]',
    created_at   TEXT NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_knowledge_category ON knowledge(category);
CREATE INDEX IF NOT EXISTS idx_knowledge_target ON knowledge(target);
CREATE INDEX IF NOT EXISTS idx_knowledge_parent ON knowledge(parent_id);
CREATE INDEX IF NOT EXISTS idx_knowledge_created ON knowledge(created_at DESC);

CREATE TABLE IF NOT EXISTS plans (
    id          TEXT PRIMARY KEY,
    title       TEXT NOT NULL,
    goal        TEXT NOT NULL,
    steps       TEXT NOT NULL DEFAULT '[]',
    status      TEXT NOT NULL DEFAULT 'active',
    reflection  TEXT NOT NULL DEFAULT '',
    created_at  TEXT NOT NULL,
    updated_at  TEXT NOT NULL
);

CREATE TABLE IF NOT EXISTS relationships (
    id          TEXT PRIMARY KEY,
    source_type TEXT NOT NULL,
    source_id   TEXT NOT NULL,
    target_type TEXT NOT NULL,
    target_id   TEXT NOT NULL,
    rel_type    TEXT NOT NULL,
    metadata    TEXT NOT NULL DEFAULT '{}',
    created_at  TEXT NOT NULL
);
CREATE INDEX IF NOT EXISTS idx_rel_source ON relationships(source_type, source_id);
CREATE INDEX IF NOT EXISTS idx_rel_target ON relationships(target_type, target_id);
CREATE INDEX IF NOT EXISTS idx_rel_type ON relationships(rel_type);
CREATE UNIQUE INDEX IF NOT EXISTS idx_rel_unique
    ON relationships(source_type, source_id, target_type, target_id, rel_type);

CREATE TABLE IF NOT EXISTS coverage_matrix (
    id            TEXT PRIMARY KEY,
    endpoint_id   TEXT NOT NULL,
    vuln_class    TEXT NOT NULL,
    status        TEXT NOT NULL DEFAULT 'pending',
    tool_name     TEXT NOT NULL DEFAULT '',
    tool_args     TEXT NOT NULL DEFAULT '{}',
    finding_id    TEXT,
    result_summary TEXT NOT NULL DEFAULT '',
    priority      INTEGER NOT NULL DEFAULT 50,
    attempted_at  TEXT,
    completed_at  TEXT,
    created_at    TEXT NOT NULL,
    updated_at    TEXT NOT NULL,
    FOREIGN KEY (endpoint_id) REFERENCES endpoints(id)
);
CREATE INDEX IF NOT EXISTS idx_cm_status ON coverage_matrix(status);
CREATE INDEX IF NOT EXISTS idx_cm_vuln_class ON coverage_matrix(vuln_class);
CREATE INDEX IF NOT EXISTS idx_cm_priority ON coverage_matrix(priority DESC);
CREATE UNIQUE INDEX IF NOT EXISTS idx_cm_unique ON coverage_matrix(endpoint_id, vuln_class);
"""

# Tables with JSON columns that need automatic deserialization
_JSON_COLUMNS: Dict[str, List[str]] = {
    "assets": ["metadata", "tags"],
    "endpoints": ["parameters", "metadata"],
    "identities": ["permissions", "metadata"],
    "hypotheses": ["evidence"],
    "observations": ["metadata"],
    "findings": ["evidence_ids", "metadata"],
    "knowledge": ["metadata", "tags"],
    "plans": ["steps"],
    "relationships": ["metadata"],
    "coverage_matrix": ["tool_args"],
}

_VALID_TABLES = frozenset(_JSON_COLUMNS.keys())


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _now_iso() -> str:
    """Return the current UTC time as an ISO-8601 string."""
    return datetime.now(timezone.utc).isoformat()


def _new_id() -> str:
    """Generate a new UUID-4 string."""
    return str(uuid.uuid4())


def _json_dumps(obj: Any) -> str:
    """Serialize *obj* to a compact JSON string."""
    return json.dumps(obj, separators=(",", ":"), default=str)


def _deserialize_row(table: str, row: sqlite3.Row) -> Dict[str, Any]:
    """Convert a sqlite3.Row into a dict, deserializing JSON columns."""
    d = dict(row)
    json_cols = _JSON_COLUMNS.get(table, [])
    for col in json_cols:
        raw = d.get(col)
        if raw is not None and isinstance(raw, str):
            try:
                d[col] = json.loads(raw)
            except (json.JSONDecodeError, TypeError):
                pass  # leave as raw string
    # Convert auth_required integer back to boolean
    if "auth_required" in d:
        d["auth_required"] = bool(d["auth_required"])
    return d


# ---------------------------------------------------------------------------
# Async compatibility layer
# ---------------------------------------------------------------------------

try:
    import aiosqlite  # type: ignore[import-untyped]
    _HAS_AIOSQLITE = True
except ImportError:
    _HAS_AIOSQLITE = False


# ---------------------------------------------------------------------------
# WorldModelDatabase
# ---------------------------------------------------------------------------

class WorldModelDatabase:
    """Async SQLite database for the pentesting world model.

    Uses *aiosqlite* when available; otherwise wraps synchronous sqlite3
    operations via ``asyncio.to_thread`` so callers always ``await``.
    """

    def __init__(self, db_path: Optional[str] = None) -> None:
        self._db_path = db_path or os.environ.get("WORLD_MODEL_DB", DEFAULT_DB_PATH)
        self._conn: Any = None  # aiosqlite.Connection or sqlite3.Connection
        self._initialized: bool = False
        self._use_aiosqlite: bool = _HAS_AIOSQLITE
        self._write_lock: asyncio.Lock = asyncio.Lock()

    # ------------------------------------------------------------------
    # Lifecycle
    # ------------------------------------------------------------------

    async def init(self) -> None:
        """Open the database and create tables if they do not exist."""
        if self._initialized:
            return

        # Ensure parent directory exists
        db_dir = os.path.dirname(os.path.abspath(self._db_path))
        os.makedirs(db_dir, exist_ok=True)

        if self._use_aiosqlite:
            self._conn = await aiosqlite.connect(self._db_path)
            self._conn.row_factory = sqlite3.Row
            await self._conn.execute("PRAGMA journal_mode=WAL")
            await self._conn.execute("PRAGMA busy_timeout=5000")
            await self._conn.executescript(_SCHEMA_SQL)
            await self._conn.commit()
        else:
            self._conn = sqlite3.connect(self._db_path, check_same_thread=False)
            self._conn.row_factory = sqlite3.Row
            self._conn.execute("PRAGMA journal_mode=WAL")
            self._conn.execute("PRAGMA busy_timeout=5000")
            self._conn.executescript(_SCHEMA_SQL)
            self._conn.commit()

        # Create FTS5 virtual table for knowledge search (best-effort)
        self._has_fts5 = await self._init_fts5()

        self._initialized = True
        logger.info("WorldModelDatabase initialized at %s (aiosqlite=%s, fts5=%s)",
                     self._db_path, self._use_aiosqlite, self._has_fts5)

    async def close(self) -> None:
        """Close the database connection."""
        if self._conn is not None:
            if self._use_aiosqlite:
                await self._conn.close()
            else:
                self._conn.close()
            self._conn = None
            self._initialized = False

    # ------------------------------------------------------------------
    # Internal execute helpers
    # ------------------------------------------------------------------

    async def _execute(self, sql: str, params: tuple = ()) -> Any:
        """Execute a single SQL statement and commit, serialized by write lock."""
        async with self._write_lock:
            if self._use_aiosqlite:
                cursor = await self._conn.execute(sql, params)
                await self._conn.commit()
                return cursor
            else:
                def _sync() -> Any:
                    cursor = self._conn.execute(sql, params)
                    self._conn.commit()
                    return cursor
                return await asyncio.to_thread(_sync)

    async def _execute_batch(self, statements: List[tuple]) -> None:
        """Execute multiple SQL statements in a single transaction under the write lock."""
        if not statements:
            return
        async with self._write_lock:
            if self._use_aiosqlite:
                for sql, params in statements:
                    await self._conn.execute(sql, params)
                await self._conn.commit()
            else:
                def _sync() -> None:
                    for sql, params in statements:
                        self._conn.execute(sql, params)
                    self._conn.commit()
                await asyncio.to_thread(_sync)

    async def _fetchone(self, sql: str, params: tuple = ()) -> Optional[sqlite3.Row]:
        """Fetch a single row."""
        if self._use_aiosqlite:
            cursor = await self._conn.execute(sql, params)
            return await cursor.fetchone()
        else:
            def _sync() -> Optional[sqlite3.Row]:
                cursor = self._conn.execute(sql, params)
                return cursor.fetchone()
            return await asyncio.to_thread(_sync)

    async def _fetchall(self, sql: str, params: tuple = ()) -> List[sqlite3.Row]:
        """Fetch all rows."""
        if self._use_aiosqlite:
            cursor = await self._conn.execute(sql, params)
            return await cursor.fetchall()
        else:
            def _sync() -> List[sqlite3.Row]:
                cursor = self._conn.execute(sql, params)
                return cursor.fetchall()
            return await asyncio.to_thread(_sync)

    # ------------------------------------------------------------------
    # FTS5 setup
    # ------------------------------------------------------------------

    async def _init_fts5(self) -> bool:
        """Create the FTS5 virtual table and sync triggers. Returns True on success."""
        fts_ddl = """
CREATE VIRTUAL TABLE IF NOT EXISTS knowledge_fts USING fts5(
    title, content, category, target,
    content='knowledge', content_rowid='rowid',
    tokenize='porter unicode61'
);

CREATE TRIGGER IF NOT EXISTS knowledge_ai AFTER INSERT ON knowledge BEGIN
    INSERT INTO knowledge_fts(rowid, title, content, category, target)
    VALUES (new.rowid, new.title, new.content, new.category, new.target);
END;

CREATE TRIGGER IF NOT EXISTS knowledge_ad AFTER DELETE ON knowledge BEGIN
    INSERT INTO knowledge_fts(knowledge_fts, rowid, title, content, category, target)
    VALUES ('delete', old.rowid, old.title, old.content, old.category, old.target);
END;

CREATE TRIGGER IF NOT EXISTS knowledge_au AFTER UPDATE ON knowledge BEGIN
    INSERT INTO knowledge_fts(knowledge_fts, rowid, title, content, category, target)
    VALUES ('delete', old.rowid, old.title, old.content, old.category, old.target);
    INSERT INTO knowledge_fts(rowid, title, content, category, target)
    VALUES (new.rowid, new.title, new.content, new.category, new.target);
END;
"""
        try:
            if self._use_aiosqlite:
                await self._conn.executescript(fts_ddl)
                await self._conn.commit()
            else:
                self._conn.executescript(fts_ddl)
                self._conn.commit()
            return True
        except Exception as exc:
            logger.warning("FTS5 not available (knowledge search will use LIKE): %s", exc)
            return False

    # ------------------------------------------------------------------
    # Knowledge store
    # ------------------------------------------------------------------

    def _chunk_content(self, content: str) -> List[str]:
        """Split content into chunks of at most CHUNK_SIZE bytes at newline boundaries."""
        if len(content) <= CHUNK_SIZE:
            return [content]

        chunks: List[str] = []
        start = 0
        while start < len(content):
            end = start + CHUNK_SIZE
            if end >= len(content):
                chunks.append(content[start:])
                break
            # Find nearest newline before the boundary
            nl_pos = content.rfind("\n", start, end)
            if nl_pos > start:
                chunks.append(content[start:nl_pos + 1])
                start = nl_pos + 1
            else:
                # No newline found; hard split
                chunks.append(content[start:end])
                start = end
        return chunks

    async def store_knowledge(
        self,
        source_tool: str,
        category: str,
        title: str,
        content: str,
        target: str = "",
        metadata: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None,
        embedding: Optional[bytes] = None,
    ) -> Dict[str, Any]:
        """Store knowledge with auto-chunking if content exceeds CHUNK_SIZE.

        Returns ``{"id": ..., "chunk_count": ..., "content_size": ...}``.
        """
        if category not in KNOWLEDGE_CATEGORIES:
            raise ValueError(
                f"Invalid category '{category}'. Must be one of: {', '.join(KNOWLEDGE_CATEGORIES)}"
            )
        if not title:
            raise ValueError("title is required")
        if not content:
            raise ValueError("content is required")

        now = _now_iso()
        meta_json = _json_dumps(metadata or {})
        tags_json = _json_dumps(tags or [])
        content_size = len(content)

        chunks = self._chunk_content(content)
        chunk_total = len(chunks)

        head_id = _new_id()

        # Generate embeddings for chunks if an embedder callback is provided
        embeddings: List[Optional[bytes]] = []
        if embedding is not None:
            # Single embedding provided – use for head chunk only
            embeddings = [embedding] + [None] * (chunk_total - 1)
        else:
            embeddings = [None] * chunk_total

        insert_sql = (
            "INSERT INTO knowledge "
            "(id, source_tool, category, target, title, content, chunk_index, "
            "chunk_total, parent_id, content_size, embedding, metadata, tags, created_at) "
            "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"
        )
        statements = []
        for i, chunk_text in enumerate(chunks):
            chunk_id = head_id if i == 0 else _new_id()
            parent = None if i == 0 else head_id
            emb = embeddings[i] if i < len(embeddings) else None
            statements.append((
                insert_sql,
                (chunk_id, source_tool, category, target, title, chunk_text,
                 i, chunk_total, parent, content_size, emb, meta_json, tags_json, now),
            ))
        await self._execute_batch(statements)

        return {"id": head_id, "chunk_count": chunk_total, "content_size": content_size}

    async def recall_knowledge(
        self,
        query: Optional[str] = None,
        query_embedding: Optional[bytes] = None,
        category: Optional[str] = None,
        target: Optional[str] = None,
        source_tool: Optional[str] = None,
        tags: Optional[List[str]] = None,
        since: Optional[str] = None,
        limit: int = 20,
        offset: int = 0,
        full_chunks: bool = False,
        include_content: bool = True,
    ) -> List[Dict[str, Any]]:
        """Hybrid search: FTS5 BM25 + cosine similarity on embeddings.

        Falls back to LIKE search if FTS5 is unavailable.
        Falls back to FTS5-only if embeddings are unavailable.
        """
        import struct
        import math

        results: List[Dict[str, Any]] = []

        # Build filter clauses
        where_parts: List[str] = []
        params: List[Any] = []
        if category:
            where_parts.append("k.category = ?")
            params.append(category)
        if target:
            where_parts.append("k.target = ?")
            params.append(target)
        if source_tool:
            where_parts.append("k.source_tool = ?")
            params.append(source_tool)
        if since:
            where_parts.append("k.created_at >= ?")
            params.append(since)
        # Only return head chunks (chunk_index=0) unless full_chunks requested
        if not full_chunks:
            where_parts.append("k.chunk_index = 0")

        filter_clause = (" AND " + " AND ".join(where_parts)) if where_parts else ""

        if query and self._has_fts5:
            # FTS5 search with BM25 scoring
            fts_sql = (
                "SELECT k.*, bm25(knowledge_fts, 5.0, 1.0, 2.0, 3.0) AS bm25_score "
                "FROM knowledge_fts fts "
                "JOIN knowledge k ON k.rowid = fts.rowid "
                f"WHERE knowledge_fts MATCH ?{filter_clause} "
                "ORDER BY bm25_score "
                "LIMIT 100"
            )
            try:
                rows = await self._fetchall(fts_sql, tuple([query] + params))
            except Exception as fts_exc:
                logger.warning("FTS5 query failed, falling back to LIKE: %s", fts_exc)
                rows = await self._like_search(query, where_parts, params, limit)
        elif query:
            # No FTS5 – fall back to LIKE
            rows = await self._like_search(query, where_parts, params, limit)
        else:
            # Browse mode – no query, just filters
            browse_sql = (
                "SELECT k.* FROM knowledge k "
                f"WHERE 1=1{filter_clause} "
                "ORDER BY k.created_at DESC LIMIT ? OFFSET ?"
            )
            rows = await self._fetchall(browse_sql, tuple(params + [limit, offset]))

        # Convert rows and compute hybrid scores if we have embeddings
        for row in rows:
            entry = _deserialize_row("knowledge", row)
            # Remove raw embedding from output
            entry.pop("embedding", None)

            bm25 = dict(row).get("bm25_score")
            if bm25 is not None:
                entry["bm25_score"] = bm25

            # Cosine similarity if we have both query embedding and row embedding
            raw_emb = dict(row).get("embedding")
            if query_embedding and raw_emb:
                try:
                    cos = self._cosine_similarity(query_embedding, raw_emb)
                    entry["embedding_score"] = round(cos, 4)
                except Exception:
                    pass

            if not include_content:
                entry.pop("content", None)

            results.append(entry)

        # Hybrid re-ranking if we have both BM25 and embedding scores
        has_bm25 = any("bm25_score" in r for r in results)
        has_emb = any("embedding_score" in r for r in results)

        if has_bm25 and has_emb:
            # Normalize BM25 scores (BM25 returns negative values, lower = better)
            bm25_vals = [r.get("bm25_score", 0) for r in results]
            bm25_min = min(bm25_vals) if bm25_vals else 0
            bm25_max = max(bm25_vals) if bm25_vals else 0
            bm25_range = bm25_max - bm25_min if bm25_max != bm25_min else 1.0

            for r in results:
                bm25_raw = r.get("bm25_score", 0)
                # BM25 is negative (lower = more relevant), normalize to 0-1 (higher = better)
                norm_bm25 = 1.0 - ((bm25_raw - bm25_min) / bm25_range) if bm25_range else 0.5
                emb_score = r.get("embedding_score", 0.0)
                r["combined_score"] = round(0.4 * norm_bm25 + 0.6 * emb_score, 4)

            results.sort(key=lambda r: r.get("combined_score", 0), reverse=True)
        elif has_bm25:
            results.sort(key=lambda r: r.get("bm25_score", 0))  # lower BM25 = better
        elif has_emb:
            results.sort(key=lambda r: r.get("embedding_score", 0), reverse=True)

        # Apply limit/offset for FTS results (already applied for browse mode)
        if query:
            results = results[offset:offset + limit]

        # Tag filtering (post-query because FTS5 doesn't index JSON arrays well)
        if tags:
            tag_set = set(tags)
            results = [r for r in results if tag_set & set(r.get("tags", []))]

        return results

    async def _like_search(
        self,
        query: str,
        where_parts: List[str],
        params: List[Any],
        limit: int,
    ) -> List[sqlite3.Row]:
        """Fallback LIKE-based search when FTS5 is unavailable."""
        like_clause = "(k.title LIKE ? OR k.content LIKE ? OR k.category LIKE ? OR k.target LIKE ?)"
        like_param = f"%{query}%"
        filter_clause = (" AND " + " AND ".join(where_parts)) if where_parts else ""
        sql = (
            f"SELECT k.* FROM knowledge k "
            f"WHERE {like_clause}{filter_clause} "
            f"ORDER BY k.created_at DESC LIMIT ?"
        )
        return await self._fetchall(
            sql, tuple([like_param] * 4 + params + [limit])
        )

    @staticmethod
    def _cosine_similarity(a: bytes, b: bytes) -> float:
        """Compute cosine similarity between two float32 embedding byte arrays."""
        import struct
        import math

        n = len(a) // 4
        va = struct.unpack(f"{n}f", a)
        vb = struct.unpack(f"{n}f", b)

        dot = sum(x * y for x, y in zip(va, vb))
        norm_a = math.sqrt(sum(x * x for x in va))
        norm_b = math.sqrt(sum(x * x for x in vb))
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return dot / (norm_a * norm_b)

    async def get_knowledge_chunks(self, parent_id: str) -> List[Dict[str, Any]]:
        """Reassemble all chunks for a parent knowledge entry."""
        rows = await self._fetchall(
            "SELECT * FROM knowledge WHERE id = ? OR parent_id = ? ORDER BY chunk_index",
            (parent_id, parent_id),
        )
        return [_deserialize_row("knowledge", r) for r in rows]

    async def delete_knowledge(self, knowledge_id: str) -> bool:
        """Delete a knowledge entry and all its chunks atomically."""
        await self._execute_batch([
            ("DELETE FROM knowledge WHERE parent_id = ?", (knowledge_id,)),
            ("DELETE FROM knowledge WHERE id = ?", (knowledge_id,)),
        ])
        return True

    async def knowledge_stats(self) -> Dict[str, Any]:
        """Return knowledge store statistics."""
        total_row = await self._fetchone(
            "SELECT COUNT(*) as cnt, COALESCE(SUM(content_size), 0) as total_size "
            "FROM knowledge WHERE chunk_index = 0"
        )
        total = total_row["cnt"] if total_row else 0
        total_size = total_row["total_size"] if total_row else 0

        # Breakdown by category
        cat_rows = await self._fetchall(
            "SELECT category, COUNT(*) as cnt FROM knowledge "
            "WHERE chunk_index = 0 GROUP BY category ORDER BY cnt DESC"
        )
        by_category = {r["category"]: r["cnt"] for r in cat_rows}

        # Breakdown by source_tool
        src_rows = await self._fetchall(
            "SELECT source_tool, COUNT(*) as cnt FROM knowledge "
            "WHERE chunk_index = 0 GROUP BY source_tool ORDER BY cnt DESC"
        )
        by_source = {r["source_tool"]: r["cnt"] for r in src_rows}

        return {
            "total_entries": total,
            "total_size_bytes": total_size,
            "by_category": by_category,
            "by_source_tool": by_source,
        }

    # ------------------------------------------------------------------
    # Assets
    # ------------------------------------------------------------------

    async def add_asset(
        self,
        kind: str,
        name: str,
        metadata: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """Add an asset (domain, subdomain, IP, service, application).

        Returns the created asset as a dict.
        """
        if kind not in ASSET_KINDS:
            raise ValueError(f"Invalid asset kind '{kind}'. Must be one of: {', '.join(ASSET_KINDS)}")
        if not name:
            raise ValueError("Asset name is required")

        now = _now_iso()
        asset_id = _new_id()
        meta_json = _json_dumps(metadata or {})
        tags_json = _json_dumps(tags or [])

        await self._execute(
            "INSERT INTO assets (id, kind, name, metadata, tags, discovered_at, updated_at) "
            "VALUES (?, ?, ?, ?, ?, ?, ?)",
            (asset_id, kind, name, meta_json, tags_json, now, now),
        )

        return {
            "id": asset_id,
            "kind": kind,
            "name": name,
            "metadata": metadata or {},
            "tags": tags or [],
            "discovered_at": now,
            "updated_at": now,
        }

    # ------------------------------------------------------------------
    # Endpoints
    # ------------------------------------------------------------------

    async def add_endpoint(
        self,
        asset_id: str,
        method: str,
        path: str,
        parameters: Optional[Dict[str, Any]] = None,
        auth_required: bool = False,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Add an API endpoint linked to an asset.

        Returns the created endpoint as a dict.
        """
        if not asset_id:
            raise ValueError("asset_id is required")
        if not method:
            raise ValueError("method is required")
        if not path:
            raise ValueError("path is required")

        now = _now_iso()
        endpoint_id = _new_id()
        params_json = _json_dumps(parameters or {})
        meta_json = _json_dumps(metadata or {})

        await self._execute(
            "INSERT INTO endpoints "
            "(id, asset_id, method, path, parameters, auth_required, status, metadata, "
            "discovered_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
            (endpoint_id, asset_id, method, path, params_json,
             1 if auth_required else 0, "discovered", meta_json, now, now),
        )

        return {
            "id": endpoint_id,
            "asset_id": asset_id,
            "method": method,
            "path": path,
            "parameters": parameters or {},
            "auth_required": auth_required,
            "status": "discovered",
            "metadata": metadata or {},
            "discovered_at": now,
            "updated_at": now,
        }

    # ------------------------------------------------------------------
    # Identities
    # ------------------------------------------------------------------

    async def add_identity(
        self,
        name: str,
        auth_type: str,
        scope: str,
        permissions: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Add a test identity / credential.

        Returns the created identity as a dict.
        """
        if not name:
            raise ValueError("Identity name is required")
        if not auth_type:
            raise ValueError("auth_type is required")
        if not scope:
            raise ValueError("scope is required")

        now = _now_iso()
        identity_id = _new_id()
        perms_json = _json_dumps(permissions or [])
        meta_json = _json_dumps(metadata or {})

        await self._execute(
            "INSERT INTO identities "
            "(id, name, auth_type, scope, permissions, status, metadata, created_at, updated_at) "
            "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
            (identity_id, name, auth_type, scope, perms_json, "active", meta_json, now, now),
        )

        return {
            "id": identity_id,
            "name": name,
            "auth_type": auth_type,
            "scope": scope,
            "permissions": permissions or [],
            "status": "active",
            "metadata": metadata or {},
            "created_at": now,
            "updated_at": now,
        }

    # ------------------------------------------------------------------
    # Hypotheses
    # ------------------------------------------------------------------

    async def add_hypothesis(
        self,
        title: str,
        description: str,
        severity: str,
        target_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Create a security hypothesis to test.

        Returns the created hypothesis as a dict.
        """
        if not title:
            raise ValueError("Hypothesis title is required")
        if not description:
            raise ValueError("Hypothesis description is required")
        if severity not in HYPOTHESIS_SEVERITIES:
            raise ValueError(
                f"Invalid severity '{severity}'. Must be one of: {', '.join(HYPOTHESIS_SEVERITIES)}"
            )

        now = _now_iso()
        hyp_id = _new_id()

        await self._execute(
            "INSERT INTO hypotheses "
            "(id, title, description, severity, status, evidence, target_id, created_at, updated_at) "
            "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
            (hyp_id, title, description, severity, "proposed", "[]", target_id, now, now),
        )

        return {
            "id": hyp_id,
            "title": title,
            "description": description,
            "severity": severity,
            "status": "proposed",
            "evidence": [],
            "target_id": target_id,
            "created_at": now,
            "updated_at": now,
        }

    async def update_hypothesis(
        self,
        hypothesis_id: str,
        status: Optional[str] = None,
        evidence: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """Update a hypothesis status and/or evidence.

        Returns the updated hypothesis as a dict.
        """
        if not hypothesis_id:
            raise ValueError("hypothesis_id is required")

        # Fetch existing
        row = await self._fetchone("SELECT * FROM hypotheses WHERE id = ?", (hypothesis_id,))
        if row is None:
            raise ValueError(f"Hypothesis not found: {hypothesis_id}")

        existing = _deserialize_row("hypotheses", row)
        now = _now_iso()

        new_status = status if status is not None else existing["status"]
        if new_status not in HYPOTHESIS_STATUSES:
            raise ValueError(
                f"Invalid status '{new_status}'. Must be one of: {', '.join(HYPOTHESIS_STATUSES)}"
            )

        # For evidence, append new items to existing list if provided
        current_evidence = existing.get("evidence", [])
        if not isinstance(current_evidence, list):
            current_evidence = []
        if evidence is not None:
            current_evidence = current_evidence + evidence

        evidence_json = _json_dumps(current_evidence)

        await self._execute(
            "UPDATE hypotheses SET status = ?, evidence = ?, updated_at = ? WHERE id = ?",
            (new_status, evidence_json, now, hypothesis_id),
        )

        existing["status"] = new_status
        existing["evidence"] = current_evidence
        existing["updated_at"] = now
        return existing

    # ------------------------------------------------------------------
    # Findings
    # ------------------------------------------------------------------

    async def add_finding(
        self,
        hypothesis_id: str,
        title: str,
        severity: str,
        confidence: float,
        evidence_ids: Optional[List[str]] = None,
        remediation: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Record a confirmed finding.

        Returns the created finding as a dict.
        """
        if not hypothesis_id:
            raise ValueError("hypothesis_id is required")
        if not title:
            raise ValueError("Finding title is required")
        if severity not in HYPOTHESIS_SEVERITIES:
            raise ValueError(
                f"Invalid severity '{severity}'. Must be one of: {', '.join(HYPOTHESIS_SEVERITIES)}"
            )
        if not (0.0 <= confidence <= 1.0):
            raise ValueError("confidence must be between 0.0 and 1.0")

        now = _now_iso()
        finding_id = _new_id()
        eids_json = _json_dumps(evidence_ids or [])
        meta_json = _json_dumps(metadata or {})

        await self._execute(
            "INSERT INTO findings "
            "(id, hypothesis_id, title, severity, confidence, status, evidence_ids, "
            "remediation, metadata, created_at, updated_at) "
            "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
            (finding_id, hypothesis_id, title, severity, confidence, "draft",
             eids_json, remediation, meta_json, now, now),
        )

        return {
            "id": finding_id,
            "hypothesis_id": hypothesis_id,
            "title": title,
            "severity": severity,
            "confidence": confidence,
            "status": "draft",
            "evidence_ids": evidence_ids or [],
            "remediation": remediation,
            "metadata": metadata or {},
            "created_at": now,
            "updated_at": now,
        }

    async def update_finding(
        self,
        finding_id: str,
        status: Optional[str] = None,
        confidence: Optional[float] = None,
        remediation: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Update a finding's status, confidence, or remediation.

        Returns the updated finding as a dict.
        """
        if not finding_id:
            raise ValueError("finding_id is required")

        row = await self._fetchone("SELECT * FROM findings WHERE id = ?", (finding_id,))
        if row is None:
            raise ValueError(f"Finding not found: {finding_id}")

        existing = _deserialize_row("findings", row)
        now = _now_iso()

        new_status = status if status is not None else existing["status"]
        if new_status not in FINDING_STATUSES:
            raise ValueError(
                f"Invalid status '{new_status}'. Must be one of: {', '.join(FINDING_STATUSES)}"
            )

        new_confidence = confidence if confidence is not None else existing["confidence"]
        if not (0.0 <= new_confidence <= 1.0):
            raise ValueError("confidence must be between 0.0 and 1.0")

        new_remediation = remediation if remediation is not None else existing.get("remediation")

        await self._execute(
            "UPDATE findings SET status = ?, confidence = ?, remediation = ?, updated_at = ? "
            "WHERE id = ?",
            (new_status, new_confidence, new_remediation, now, finding_id),
        )

        existing["status"] = new_status
        existing["confidence"] = new_confidence
        existing["remediation"] = new_remediation
        existing["updated_at"] = now
        return existing

    # ------------------------------------------------------------------
    # Observations
    # ------------------------------------------------------------------

    async def add_observation(
        self,
        hypothesis_id: str,
        obs_type: str,
        content: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Add an observation to a hypothesis.

        Returns the created observation as a dict.
        """
        if not hypothesis_id:
            raise ValueError("hypothesis_id is required")
        if obs_type not in OBSERVATION_TYPES:
            raise ValueError(
                f"Invalid observation type '{obs_type}'. "
                f"Must be one of: {', '.join(OBSERVATION_TYPES)}"
            )
        if not content:
            raise ValueError("Observation content is required")

        now = _now_iso()
        obs_id = _new_id()
        meta_json = _json_dumps(metadata or {})

        await self._execute(
            "INSERT INTO observations "
            "(id, hypothesis_id, type, content, metadata, observed_at) "
            "VALUES (?, ?, ?, ?, ?, ?)",
            (obs_id, hypothesis_id, obs_type, content, meta_json, now),
        )

        return {
            "id": obs_id,
            "hypothesis_id": hypothesis_id,
            "type": obs_type,
            "content": content,
            "metadata": metadata or {},
            "observed_at": now,
        }

    # ------------------------------------------------------------------
    # Plans
    # ------------------------------------------------------------------

    async def add_plan(
        self,
        title: str,
        goal: str,
        steps: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """Create a testing plan with structured steps.

        Each step is ``{"description": str, "status": "pending", "result": ""}``.
        Returns the created plan as a dict.
        """
        if not title:
            raise ValueError("Plan title is required")
        if not goal:
            raise ValueError("Plan goal is required")
        if not steps or not isinstance(steps, list):
            raise ValueError("steps must be a non-empty list")

        now = _now_iso()
        plan_id = _new_id()

        # Normalize steps
        normalized: List[Dict[str, Any]] = []
        for s in steps:
            if isinstance(s, dict):
                normalized.append({
                    "description": s.get("description", ""),
                    "status": "pending",
                    "result": "",
                })
            elif isinstance(s, str):
                normalized.append({"description": s, "status": "pending", "result": ""})

        steps_json = _json_dumps(normalized)

        await self._execute(
            "INSERT INTO plans (id, title, goal, steps, status, reflection, created_at, updated_at) "
            "VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
            (plan_id, title, goal, steps_json, "active", "", now, now),
        )

        return {
            "id": plan_id,
            "title": title,
            "goal": goal,
            "steps": normalized,
            "status": "active",
            "reflection": "",
            "created_at": now,
            "updated_at": now,
        }

    async def update_plan(
        self,
        plan_id: str,
        step_index: Optional[int] = None,
        step_status: Optional[str] = None,
        step_result: Optional[str] = None,
        reflection: Optional[str] = None,
        status: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Update a plan's step status, reflection, or overall status.

        Returns the updated plan as a dict.
        """
        if not plan_id:
            raise ValueError("plan_id is required")

        row = await self._fetchone("SELECT * FROM plans WHERE id = ?", (plan_id,))
        if row is None:
            raise ValueError(f"Plan not found: {plan_id}")

        existing = _deserialize_row("plans", row)
        now = _now_iso()

        # Update step if index provided
        current_steps = existing.get("steps", [])
        if not isinstance(current_steps, list):
            current_steps = []

        if step_index is not None:
            if step_index < 0 or step_index >= len(current_steps):
                raise ValueError(
                    f"step_index {step_index} out of range (0-{len(current_steps) - 1})"
                )
            if step_status is not None:
                if step_status not in PLAN_STEP_STATUSES:
                    raise ValueError(
                        f"Invalid step_status '{step_status}'. Must be one of: {', '.join(PLAN_STEP_STATUSES)}"
                    )
                current_steps[step_index]["status"] = step_status
            if step_result is not None:
                current_steps[step_index]["result"] = step_result

        # Update overall status
        new_status = status if status is not None else existing["status"]
        if new_status not in PLAN_STATUSES:
            raise ValueError(
                f"Invalid plan status '{new_status}'. Must be one of: {', '.join(PLAN_STATUSES)}"
            )

        # Update reflection
        new_reflection = reflection if reflection is not None else existing.get("reflection", "")

        steps_json = _json_dumps(current_steps)

        await self._execute(
            "UPDATE plans SET steps = ?, status = ?, reflection = ?, updated_at = ? WHERE id = ?",
            (steps_json, new_status, new_reflection, now, plan_id),
        )

        existing["steps"] = current_steps
        existing["status"] = new_status
        existing["reflection"] = new_reflection
        existing["updated_at"] = now
        return existing

    # ------------------------------------------------------------------
    # Relationships (graph edges)
    # ------------------------------------------------------------------

    async def add_relationship(
        self,
        source_type: str,
        source_id: str,
        target_type: str,
        target_id: str,
        rel_type: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Add a typed edge between two entities.  Idempotent (INSERT OR IGNORE)."""
        if source_type not in VALID_ENTITY_TYPES:
            raise ValueError(f"Invalid source_type '{source_type}'. Must be one of: {', '.join(VALID_ENTITY_TYPES)}")
        if target_type not in VALID_ENTITY_TYPES:
            raise ValueError(f"Invalid target_type '{target_type}'. Must be one of: {', '.join(VALID_ENTITY_TYPES)}")
        if rel_type not in VALID_REL_TYPES:
            raise ValueError(f"Invalid rel_type '{rel_type}'. Must be one of: {', '.join(VALID_REL_TYPES)}")

        now = _now_iso()
        rel_id = _new_id()
        meta_json = _json_dumps(metadata or {})

        await self._execute(
            "INSERT OR IGNORE INTO relationships "
            "(id, source_type, source_id, target_type, target_id, rel_type, metadata, created_at) "
            "VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
            (rel_id, source_type, source_id, target_type, target_id, rel_type, meta_json, now),
        )

        return {
            "id": rel_id,
            "source_type": source_type,
            "source_id": source_id,
            "target_type": target_type,
            "target_id": target_id,
            "rel_type": rel_type,
            "metadata": metadata or {},
            "created_at": now,
        }

    async def query_relationships(
        self,
        source_type: Optional[str] = None,
        source_id: Optional[str] = None,
        target_type: Optional[str] = None,
        target_id: Optional[str] = None,
        rel_type: Optional[str] = None,
        limit: int = 100,
    ) -> List[Dict[str, Any]]:
        """Query relationships with optional filters."""
        where_parts: List[str] = []
        params: List[Any] = []

        if source_type:
            where_parts.append("source_type = ?")
            params.append(source_type)
        if source_id:
            where_parts.append("source_id = ?")
            params.append(source_id)
        if target_type:
            where_parts.append("target_type = ?")
            params.append(target_type)
        if target_id:
            where_parts.append("target_id = ?")
            params.append(target_id)
        if rel_type:
            where_parts.append("rel_type = ?")
            params.append(rel_type)

        sql = "SELECT * FROM relationships"
        if where_parts:
            sql += " WHERE " + " AND ".join(where_parts)
        sql += " ORDER BY created_at DESC LIMIT ?"
        params.append(limit)

        rows = await self._fetchall(sql, tuple(params))
        return [_deserialize_row("relationships", r) for r in rows]

    async def get_neighbors(
        self,
        entity_type: str,
        entity_id: str,
        rel_types: Optional[List[str]] = None,
        direction: str = "both",
        depth: int = 1,
    ) -> Dict[str, Any]:
        """BFS traversal from an entity. Returns connected nodes + edges.

        *direction*: ``outgoing``, ``incoming``, or ``both``.
        *depth*: BFS depth (max 4).
        """
        depth = min(max(depth, 1), 4)
        visited_nodes: set = set()
        visited_edges: set = set()
        nodes: List[Dict[str, Any]] = []
        edges: List[Dict[str, Any]] = []
        queue: List[tuple] = [(entity_type, entity_id, 0)]
        visited_nodes.add((entity_type, entity_id))

        while queue:
            cur_type, cur_id, cur_depth = queue.pop(0)
            if cur_depth >= depth:
                continue

            # Build SQL for outgoing + incoming edges
            rels: List[Dict[str, Any]] = []
            if direction in ("outgoing", "both"):
                sql = "SELECT * FROM relationships WHERE source_type = ? AND source_id = ?"
                params: List[Any] = [cur_type, cur_id]
                if rel_types:
                    placeholders = ",".join("?" * len(rel_types))
                    sql += f" AND rel_type IN ({placeholders})"
                    params.extend(rel_types)
                rows = await self._fetchall(sql, tuple(params))
                for r in rows:
                    rel = _deserialize_row("relationships", r)
                    rels.append(rel)

            if direction in ("incoming", "both"):
                sql = "SELECT * FROM relationships WHERE target_type = ? AND target_id = ?"
                params = [cur_type, cur_id]
                if rel_types:
                    placeholders = ",".join("?" * len(rel_types))
                    sql += f" AND rel_type IN ({placeholders})"
                    params.extend(rel_types)
                rows = await self._fetchall(sql, tuple(params))
                for r in rows:
                    rel = _deserialize_row("relationships", r)
                    rels.append(rel)

            for rel in rels:
                edge_key = (rel["source_type"], rel["source_id"],
                            rel["target_type"], rel["target_id"], rel["rel_type"])
                if edge_key not in visited_edges:
                    visited_edges.add(edge_key)
                    edges.append(rel)

                # Determine the neighbor
                if rel["source_type"] == cur_type and rel["source_id"] == cur_id:
                    neighbor = (rel["target_type"], rel["target_id"])
                else:
                    neighbor = (rel["source_type"], rel["source_id"])

                if neighbor not in visited_nodes:
                    visited_nodes.add(neighbor)
                    nodes.append({"type": neighbor[0], "id": neighbor[1]})
                    queue.append((neighbor[0], neighbor[1], cur_depth + 1))

        return {
            "root": {"type": entity_type, "id": entity_id},
            "nodes": nodes,
            "edges": edges,
            "depth": depth,
        }

    async def get_attack_path(
        self,
        from_type: str,
        from_id: str,
        to_type: str,
        to_id: str,
        max_depth: int = 8,
    ) -> Optional[List[Dict[str, Any]]]:
        """Find shortest path between two entities via BFS.

        Returns a list of relationship dicts forming the path, or ``None``
        if no path exists within *max_depth*.
        """
        max_depth = min(max(max_depth, 1), 8)
        target = (to_type, to_id)
        visited: set = set()
        visited.add((from_type, from_id))
        # queue items: (current_type, current_id, path_so_far)
        queue: List[tuple] = [(from_type, from_id, [])]

        while queue:
            cur_type, cur_id, path = queue.pop(0)
            if len(path) >= max_depth:
                continue

            # Get all edges from this node (both directions)
            rows = await self._fetchall(
                "SELECT * FROM relationships WHERE "
                "(source_type = ? AND source_id = ?) OR (target_type = ? AND target_id = ?)",
                (cur_type, cur_id, cur_type, cur_id),
            )

            for r in rows:
                rel = _deserialize_row("relationships", r)
                # Determine the neighbor
                if rel["source_type"] == cur_type and rel["source_id"] == cur_id:
                    neighbor = (rel["target_type"], rel["target_id"])
                else:
                    neighbor = (rel["source_type"], rel["source_id"])

                new_path = path + [rel]

                if neighbor == target:
                    return new_path

                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((neighbor[0], neighbor[1], new_path))

        return None

    # ------------------------------------------------------------------
    # Coverage matrix
    # ------------------------------------------------------------------

    async def coverage_init_rows(self, rows: List[Dict[str, Any]]) -> Dict[str, int]:
        """Batch INSERT OR IGNORE coverage cells.

        Each row dict should have: endpoint_id, vuln_class, tool_name,
        tool_args, priority, and optionally status.
        Returns ``{created, skipped_existing}``.
        """
        now = _now_iso()
        insert_sql = (
            "INSERT OR IGNORE INTO coverage_matrix "
            "(id, endpoint_id, vuln_class, status, tool_name, tool_args, "
            "priority, created_at, updated_at) "
            "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)"
        )
        statements = []
        for row in rows:
            cell_id = _new_id()
            statements.append((
                insert_sql,
                (
                    cell_id,
                    row["endpoint_id"],
                    row["vuln_class"],
                    row.get("status", "pending"),
                    row.get("tool_name", ""),
                    _json_dumps(row.get("tool_args", {})),
                    row.get("priority", 50),
                    now,
                    now,
                ),
            ))

        total = len(statements)
        await self._execute_batch(statements)

        # Count how many were actually inserted (total - existing)
        count_row = await self._fetchone(
            "SELECT COUNT(*) as cnt FROM coverage_matrix WHERE created_at = ?",
            (now,),
        )
        created = count_row["cnt"] if count_row else 0
        return {"created": created, "skipped_existing": total - created}

    async def coverage_next(
        self,
        vuln_class: Optional[str] = None,
        endpoint_id: Optional[str] = None,
        limit: int = 5,
    ) -> List[Dict[str, Any]]:
        """Return next pending coverage cells, joined with endpoint info.

        Ordered by priority DESC, then created_at ASC.
        """
        where_parts = ["cm.status = 'pending'"]
        params: List[Any] = []

        if vuln_class:
            where_parts.append("cm.vuln_class = ?")
            params.append(vuln_class)
        if endpoint_id:
            where_parts.append("cm.endpoint_id = ?")
            params.append(endpoint_id)

        sql = (
            "SELECT cm.*, e.method, e.path, e.parameters AS ep_parameters, "
            "e.auth_required, e.asset_id "
            "FROM coverage_matrix cm "
            "JOIN endpoints e ON e.id = cm.endpoint_id "
            f"WHERE {' AND '.join(where_parts)} "
            "ORDER BY cm.priority DESC, cm.created_at ASC "
            "LIMIT ?"
        )
        params.append(limit)

        rows = await self._fetchall(sql, tuple(params))
        results = []
        for r in rows:
            d = _deserialize_row("coverage_matrix", r)
            # Parse endpoint parameters from the join
            ep_params = d.pop("ep_parameters", "{}")
            if isinstance(ep_params, str):
                try:
                    ep_params = json.loads(ep_params)
                except (json.JSONDecodeError, TypeError):
                    ep_params = {}
            d["endpoint"] = {
                "method": d.pop("method", ""),
                "path": d.pop("path", ""),
                "parameters": ep_params,
                "auth_required": bool(d.pop("auth_required", 0)),
                "asset_id": d.pop("asset_id", ""),
            }
            results.append(d)
        return results

    async def coverage_mark(
        self,
        cell_id: str,
        status: str,
        finding_id: Optional[str] = None,
        result_summary: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Update a coverage cell's status and set completed_at."""
        if status not in COVERAGE_STATUSES:
            raise ValueError(f"Invalid status '{status}'. Must be one of: {', '.join(COVERAGE_STATUSES)}")

        now = _now_iso()
        completed = now if status in ("passed", "vulnerable", "skipped", "error") else None

        sets = ["status = ?", "updated_at = ?"]
        params: List[Any] = [status, now]

        if finding_id is not None:
            sets.append("finding_id = ?")
            params.append(finding_id)
        if result_summary is not None:
            sets.append("result_summary = ?")
            params.append(result_summary)
        if status == "in_progress":
            sets.append("attempted_at = ?")
            params.append(now)
        if completed:
            sets.append("completed_at = ?")
            params.append(completed)

        params.append(cell_id)
        await self._execute(
            f"UPDATE coverage_matrix SET {', '.join(sets)} WHERE id = ?",
            tuple(params),
        )

        row = await self._fetchone("SELECT * FROM coverage_matrix WHERE id = ?", (cell_id,))
        if row is None:
            raise ValueError(f"Coverage cell not found: {cell_id}")
        return _deserialize_row("coverage_matrix", row)

    async def coverage_report(
        self,
        endpoint_id: Optional[str] = None,
        vuln_class: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Aggregate coverage stats with GROUP BY."""
        where_parts: List[str] = []
        params: List[Any] = []

        if endpoint_id:
            where_parts.append("endpoint_id = ?")
            params.append(endpoint_id)
        if vuln_class:
            where_parts.append("vuln_class = ?")
            params.append(vuln_class)

        where_clause = (" WHERE " + " AND ".join(where_parts)) if where_parts else ""

        # Total cells
        total_row = await self._fetchone(
            f"SELECT COUNT(*) as cnt FROM coverage_matrix{where_clause}",
            tuple(params),
        )
        total = total_row["cnt"] if total_row else 0

        # By status
        status_rows = await self._fetchall(
            f"SELECT status, COUNT(*) as cnt FROM coverage_matrix{where_clause} "
            "GROUP BY status",
            tuple(params),
        )
        by_status = {r["status"]: r["cnt"] for r in status_rows}

        # By vuln_class
        vc_rows = await self._fetchall(
            f"SELECT vuln_class, COUNT(*) as cnt FROM coverage_matrix{where_clause} "
            "GROUP BY vuln_class ORDER BY cnt DESC",
            tuple(params),
        )
        by_vuln_class = {r["vuln_class"]: r["cnt"] for r in vc_rows}

        completed = sum(by_status.get(s, 0) for s in ("passed", "vulnerable", "skipped", "error"))
        coverage_pct = round((completed / total * 100), 1) if total > 0 else 0.0

        # Gaps: pending cells ordered by priority
        gap_rows = await self._fetchall(
            f"SELECT cm.*, e.method, e.path FROM coverage_matrix cm "
            f"JOIN endpoints e ON e.id = cm.endpoint_id "
            f"WHERE cm.status = 'pending'{' AND ' + ' AND '.join(['cm.' + w for w in where_parts]) if where_parts else ''} "
            f"ORDER BY cm.priority DESC LIMIT 20",
            tuple(params),
        )
        gaps = []
        for r in gap_rows:
            d = _deserialize_row("coverage_matrix", r)
            d["endpoint_method"] = d.pop("method", "")
            d["endpoint_path"] = d.pop("path", "")
            gaps.append(d)

        return {
            "total_cells": total,
            "by_status": by_status,
            "by_vuln_class": by_vuln_class,
            "coverage_pct": coverage_pct,
            "gaps": gaps,
        }

    async def coverage_query(
        self,
        query_type: str,
        filters: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """Pre-built SQL queries for structured coverage analysis.

        Supported query_type values:
        - untested_endpoints: endpoints with no coverage cells or all pending
        - coverage_by_vuln_class: aggregated stats per vuln class
        - findings_by_endpoint: endpoints with vulnerable cells
        - vulnerable_endpoints: endpoints that have at least one vulnerable cell
        - high_priority_gaps: pending cells with priority >= 70
        """
        filters = filters or {}

        if query_type == "untested_endpoints":
            rows = await self._fetchall(
                "SELECT e.id, e.method, e.path, e.asset_id, "
                "COUNT(cm.id) as total_cells, "
                "SUM(CASE WHEN cm.status = 'pending' THEN 1 ELSE 0 END) as pending_cells "
                "FROM endpoints e "
                "LEFT JOIN coverage_matrix cm ON cm.endpoint_id = e.id "
                "GROUP BY e.id "
                "HAVING total_cells = 0 OR total_cells = pending_cells "
                "ORDER BY e.rowid DESC LIMIT 50"
            )
        elif query_type == "coverage_by_vuln_class":
            rows = await self._fetchall(
                "SELECT vuln_class, "
                "COUNT(*) as total, "
                "SUM(CASE WHEN status = 'passed' THEN 1 ELSE 0 END) as passed, "
                "SUM(CASE WHEN status = 'vulnerable' THEN 1 ELSE 0 END) as vulnerable, "
                "SUM(CASE WHEN status = 'pending' THEN 1 ELSE 0 END) as pending, "
                "SUM(CASE WHEN status = 'error' THEN 1 ELSE 0 END) as errors "
                "FROM coverage_matrix GROUP BY vuln_class ORDER BY vuln_class"
            )
        elif query_type == "findings_by_endpoint":
            rows = await self._fetchall(
                "SELECT cm.endpoint_id, e.method, e.path, "
                "GROUP_CONCAT(cm.vuln_class) as vuln_classes, "
                "COUNT(*) as finding_count "
                "FROM coverage_matrix cm "
                "JOIN endpoints e ON e.id = cm.endpoint_id "
                "WHERE cm.status = 'vulnerable' "
                "GROUP BY cm.endpoint_id "
                "ORDER BY finding_count DESC LIMIT 50"
            )
        elif query_type == "vulnerable_endpoints":
            rows = await self._fetchall(
                "SELECT DISTINCT cm.endpoint_id, e.method, e.path, e.asset_id, "
                "cm.vuln_class, cm.finding_id, cm.result_summary "
                "FROM coverage_matrix cm "
                "JOIN endpoints e ON e.id = cm.endpoint_id "
                "WHERE cm.status = 'vulnerable' "
                "ORDER BY cm.completed_at DESC LIMIT 50"
            )
        elif query_type == "high_priority_gaps":
            rows = await self._fetchall(
                "SELECT cm.*, e.method, e.path "
                "FROM coverage_matrix cm "
                "JOIN endpoints e ON e.id = cm.endpoint_id "
                "WHERE cm.status = 'pending' AND cm.priority >= 70 "
                "ORDER BY cm.priority DESC LIMIT 50"
            )
        else:
            raise ValueError(
                f"Unknown query_type '{query_type}'. Must be one of: "
                "untested_endpoints, coverage_by_vuln_class, findings_by_endpoint, "
                "vulnerable_endpoints, high_priority_gaps"
            )

        return [dict(r) for r in rows]

    # ------------------------------------------------------------------
    # Generic query
    # ------------------------------------------------------------------

    async def query(
        self,
        table: str,
        filters: Optional[Dict[str, Any]] = None,
        limit: int = 100,
        offset: int = 0,
    ) -> List[Dict[str, Any]]:
        """Query any world-model table with optional filters.

        Filters are applied as ``column = value`` WHERE clauses combined
        with AND.  Returns a list of dicts.
        """
        if table not in _VALID_TABLES:
            raise ValueError(f"Invalid table '{table}'. Must be one of: {', '.join(sorted(_VALID_TABLES))}")

        filters = filters or {}

        where_clauses: List[str] = []
        params: List[Any] = []
        for col, val in filters.items():
            # Prevent SQL injection by validating column names
            if not col.isidentifier():
                raise ValueError(f"Invalid filter column name: {col}")
            where_clauses.append(f"{col} = ?")
            params.append(val)

        sql = f"SELECT * FROM {table}"
        if where_clauses:
            sql += " WHERE " + " AND ".join(where_clauses)
        sql += " ORDER BY rowid DESC LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        rows = await self._fetchall(sql, tuple(params))
        return [_deserialize_row(table, r) for r in rows]

    # ------------------------------------------------------------------
    # Get by ID
    # ------------------------------------------------------------------

    async def get_by_id(self, table: str, record_id: str) -> Optional[Dict[str, Any]]:
        """Fetch a single record by its ``id`` column.

        Returns a dict or ``None`` if not found.
        """
        if table not in _VALID_TABLES:
            raise ValueError(f"Invalid table '{table}'. Must be one of: {', '.join(sorted(_VALID_TABLES))}")

        row = await self._fetchone(f"SELECT * FROM {table} WHERE id = ?", (record_id,))
        if row is None:
            return None
        return _deserialize_row(table, row)

    # ------------------------------------------------------------------
    # Statistics
    # ------------------------------------------------------------------

    async def get_stats(self) -> Dict[str, Any]:
        """Return the row count for every table in the world model."""
        stats: Dict[str, Any] = {}
        for table in sorted(_VALID_TABLES):
            row = await self._fetchone(f"SELECT COUNT(*) as cnt FROM {table}")
            stats[table] = row["cnt"] if row else 0
        # Add knowledge store stats
        stats["knowledge_store"] = await self.knowledge_stats()
        return stats


# ---------------------------------------------------------------------------
# Per-assessment instance cache
# ---------------------------------------------------------------------------

_db_instances: Dict[int, WorldModelDatabase] = {}


def _resolve_db_path(assessment_id: int) -> str:
    """Compute the SQLite file path for a given assessment."""
    env_val = os.environ.get("WORLD_MODEL_DB", DEFAULT_DB_PATH)
    if env_val.endswith(".db"):
        base_dir = os.path.dirname(os.path.abspath(env_val))
    else:
        base_dir = os.path.abspath(env_val)
    return os.path.join(base_dir, f"world-model-{assessment_id}.db")


async def get_world_model_db(assessment_id: int) -> WorldModelDatabase:
    """Return (or create and initialise) the database for *assessment_id*."""
    if not isinstance(assessment_id, int) or assessment_id <= 0:
        raise ValueError(f"assessment_id must be a positive integer, got: {assessment_id!r}")
    if assessment_id not in _db_instances:
        db_path = _resolve_db_path(assessment_id)
        db = WorldModelDatabase(db_path=db_path)
        await db.init()
        _db_instances[assessment_id] = db
    return _db_instances[assessment_id]


async def reset_world_model_db(assessment_id: Optional[int] = None) -> None:
    """Close and remove database instance(s).

    If *assessment_id* is ``None``, close **all** instances.
    """
    if assessment_id is not None:
        db = _db_instances.pop(assessment_id, None)
        if db is not None:
            await db.close()
    else:
        for db in _db_instances.values():
            await db.close()
        _db_instances.clear()
