"""
World Model Database - PostgreSQL-backed persistence for the pentesting world model.

Migrated from SQLite to PostgreSQL with pgvector for vector similarity search
and PostgreSQL FTS (tsvector) for full-text search.

All world model tables use the ``wm_`` prefix and include an ``assessment_id``
column for per-assessment isolation within the shared PostgreSQL instance.
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Union

logger = logging.getLogger("autopentest-mcp")

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

# Valid enum values for validation
ASSET_KINDS = ("domain", "subdomain", "ip", "service", "application")
ENDPOINT_STATUSES = ("discovered", "tested", "vulnerable", "safe")
IDENTITY_STATUSES = ("active", "revoked", "expired")
HYPOTHESIS_SEVERITIES = ("info", "low", "medium", "high", "critical")
HYPOTHESIS_STATUSES = ("proposed", "testing", "confirmed", "rejected")
FINDING_STATUSES = ("draft", "confirmed", "reported", "remediated")
OBSERVATION_TYPES = ("request", "response", "behavior", "anomaly")
PLAN_STATUSES = ("active", "completed", "abandoned")
PLAN_STEP_STATUSES = ("pending", "in_progress", "done", "skipped")

# Graph / relationship constants
VALID_ENTITY_TYPES = ("asset", "endpoint", "identity", "hypothesis", "finding")
VALID_REL_TYPES = (
    "domain_has_subdomain", "resolves_to", "has_endpoint", "has_finding",
    "tested_by", "targets", "confirms",
    "leads_to", "authenticates_as",
)

# Coverage matrix constants
COVERAGE_STATUSES = ("pending", "in_progress", "passed", "vulnerable", "skipped", "error")
VULN_CLASSES = (
    "sqli", "xss", "idor", "injection", "auth_bypass", "ssrf",
    "path_traversal", "overflow", "type_confusion", "info_disclosure",
    "misconfig", "nuclei", "ssti",
)

# Knowledge store constants
CHUNK_SIZE = 65536  # 64KB
KNOWLEDGE_CATEGORIES = (
    "scan_output", "page_content", "http_exchange",
    "fuzz_result", "command_output", "error",
    "form_data", "navigation", "timing", "other",
)

# Tables with JSONB columns (kept for backward-compat reference; JSONB is
# auto-deserialized by asyncpg so this is mostly documentation now).
_JSON_COLUMNS: Dict[str, List[str]] = {
    "assets": ["metadata", "tags"],
    "endpoints": ["parameters", "metadata"],
    "identities": ["permissions", "metadata"],
    "hypotheses": ["evidence"],
    "observations": ["metadata"],
    "findings": ["evidence_ids", "metadata"],
    "knowledge": ["metadata", "tags"],
    "plans": ["steps"],
    "relationships": ["metadata"],
    "coverage_matrix": ["tool_args"],
}

_VALID_TABLES = frozenset(_JSON_COLUMNS.keys())

# Map logical table names (used by callers) to physical wm_ prefixed names
_TABLE_MAP = {t: f"wm_{t}" for t in _VALID_TABLES}


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _now_iso() -> datetime:
    """Return the current UTC time as a timezone-aware datetime.

    asyncpg requires real datetime objects for TIMESTAMPTZ columns
    (it does not auto-coerce ISO-8601 strings).
    """
    return datetime.now(timezone.utc)


def _new_id() -> str:
    """Generate a new UUID-4 string."""
    return str(uuid.uuid4())


def _json_dumps(obj: Any) -> str:
    """Serialize *obj* to a compact JSON string."""
    return json.dumps(obj, separators=(",", ":"), default=str)


def _deserialize_row(table: str, row: Any) -> Dict[str, Any]:
    """Convert an asyncpg.Record (or dict) into a plain dict.

    asyncpg returns JSONB columns as strings by default, so we must
    parse them back to Python objects using the ``_JSON_COLUMNS`` map.
    """
    d = dict(row)
    # Remove internal columns that callers don't need
    d.pop("assessment_id", None)
    d.pop("fts", None)
    # Parse JSONB string columns back to Python objects
    for col in _JSON_COLUMNS.get(table, []):
        val = d.get(col)
        if isinstance(val, str):
            try:
                d[col] = json.loads(val)
            except (json.JSONDecodeError, TypeError):
                pass
    return d


# ---------------------------------------------------------------------------
# WorldModelDatabase
# ---------------------------------------------------------------------------

class WorldModelDatabase:
    """Async PostgreSQL database for the pentesting world model.

    Uses asyncpg connection pool for all I/O.  Each instance is bound to a
    single ``assessment_id`` so every query is automatically scoped.
    """

    def __init__(self, pool: Any = None, assessment_id: int = 0) -> None:
        self._pool = pool
        self._assessment_id = assessment_id
        self._initialized: bool = False

    # ------------------------------------------------------------------
    # Lifecycle
    # ------------------------------------------------------------------

    async def init(self) -> None:
        """Mark the database as initialised.

        Schema creation is handled by ``database.create_wm_tables()``
        at application startup, not per-instance.
        """
        if self._initialized:
            return
        self._initialized = True
        logger.info(
            "WorldModelDatabase initialized (assessment_id=%s, pool=%s)",
            self._assessment_id,
            "connected" if self._pool else "none",
        )

    async def close(self) -> None:
        """Close this instance (no-op; pool is shared and managed externally)."""
        self._initialized = False

    # ------------------------------------------------------------------
    # Internal execute helpers
    # ------------------------------------------------------------------

    async def _execute(self, sql: str, params: tuple = ()) -> Any:
        """Execute a single SQL statement."""
        async with self._pool.acquire() as conn:
            return await conn.execute(sql, *params)

    async def _execute_batch(self, statements: List[tuple]) -> None:
        """Execute multiple SQL statements in a single transaction."""
        if not statements:
            return
        async with self._pool.acquire() as conn:
            async with conn.transaction():
                for sql, params in statements:
                    await conn.execute(sql, *params)

    async def _fetchone(self, sql: str, params: tuple = ()) -> Optional[Any]:
        """Fetch a single row."""
        async with self._pool.acquire() as conn:
            return await conn.fetchrow(sql, *params)

    async def _fetchall(self, sql: str, params: tuple = ()) -> List[Any]:
        """Fetch all rows."""
        async with self._pool.acquire() as conn:
            return await conn.fetch(sql, *params)

    # ------------------------------------------------------------------
    # Knowledge store
    # ------------------------------------------------------------------

    def _chunk_content(self, content: str) -> List[str]:
        """Split content into chunks of at most CHUNK_SIZE bytes at newline boundaries."""
        if len(content) <= CHUNK_SIZE:
            return [content]

        chunks: List[str] = []
        start = 0
        while start < len(content):
            end = start + CHUNK_SIZE
            if end >= len(content):
                chunks.append(content[start:])
                break
            nl_pos = content.rfind("\n", start, end)
            if nl_pos > start:
                chunks.append(content[start:nl_pos + 1])
                start = nl_pos + 1
            else:
                chunks.append(content[start:end])
                start = end
        return chunks

    async def store_knowledge(
        self,
        source_tool: str,
        category: str,
        title: str,
        content: str,
        target: str = "",
        metadata: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None,
        embedding: Optional[Any] = None,
    ) -> Dict[str, Any]:
        """Store knowledge with auto-chunking if content exceeds CHUNK_SIZE.

        Returns ``{"id": ..., "chunk_count": ..., "content_size": ...}``.
        """
        if category not in KNOWLEDGE_CATEGORIES:
            raise ValueError(
                f"Invalid category '{category}'. Must be one of: {', '.join(KNOWLEDGE_CATEGORIES)}"
            )
        if not title:
            raise ValueError("title is required")
        if not content:
            raise ValueError("content is required")

        now = _now_iso()
        meta = metadata or {}
        tags_val = tags or []
        content_size = len(content)

        chunks = self._chunk_content(content)
        chunk_total = len(chunks)
        head_id = _new_id()

        insert_sql = (
            "INSERT INTO wm_knowledge "
            "(id, assessment_id, source_tool, category, target, title, content, "
            "chunk_index, chunk_total, parent_id, content_size, embedding, "
            "metadata, tags, created_at) "
            "VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15)"
        )
        statements = []
        for i, chunk_text in enumerate(chunks):
            chunk_id = head_id if i == 0 else _new_id()
            parent = None if i == 0 else head_id
            emb = embedding if i == 0 else None
            statements.append((
                insert_sql,
                (chunk_id, self._assessment_id, source_tool, category, target,
                 title, chunk_text, i, chunk_total, parent, content_size,
                 emb, json.dumps(meta), json.dumps(tags_val), now),
            ))
        await self._execute_batch(statements)

        return {"id": head_id, "chunk_count": chunk_total, "content_size": content_size}

    async def recall_knowledge(
        self,
        query: Optional[str] = None,
        query_embedding: Optional[Any] = None,
        category: Optional[str] = None,
        target: Optional[str] = None,
        source_tool: Optional[str] = None,
        tags: Optional[List[str]] = None,
        since: Optional[str] = None,
        limit: int = 20,
        offset: int = 0,
        full_chunks: bool = False,
        include_content: bool = True,
    ) -> List[Dict[str, Any]]:
        """Hybrid search: PostgreSQL FTS + pgvector cosine similarity.

        Falls back to LIKE search if the query contains characters that
        break tsquery parsing.
        """
        results: List[Dict[str, Any]] = []

        # Build filter clauses
        where_parts: List[str] = [f"k.assessment_id = ${1}"]
        params: List[Any] = [self._assessment_id]
        param_idx = 2

        if category:
            where_parts.append(f"k.category = ${param_idx}")
            params.append(category)
            param_idx += 1
        if target:
            where_parts.append(f"k.target = ${param_idx}")
            params.append(target)
            param_idx += 1
        if source_tool:
            where_parts.append(f"k.source_tool = ${param_idx}")
            params.append(source_tool)
            param_idx += 1
        if since:
            where_parts.append(f"k.created_at >= ${param_idx}")
            params.append(since)
            param_idx += 1
        if not full_chunks:
            where_parts.append("k.chunk_index = 0")

        filter_clause = " AND ".join(where_parts)

        if query:
            # Try FTS first
            fts_query_param = f"${param_idx}"
            params.append(query)
            param_idx += 1

            fts_sql = (
                f"SELECT k.*, ts_rank_cd(k.fts, plainto_tsquery('english', {fts_query_param})) AS rank_score "
                f"FROM wm_knowledge k "
                f"WHERE {filter_clause} AND k.fts @@ plainto_tsquery('english', {fts_query_param}) "
                f"ORDER BY rank_score DESC "
                f"LIMIT 100"
            )
            try:
                rows = await self._fetchall(fts_sql, tuple(params))
            except Exception as fts_exc:
                logger.warning("FTS query failed, falling back to LIKE: %s", fts_exc)
                rows = await self._like_search(query, where_parts[:-0], params[:-1], param_idx - 1, limit)
        else:
            # Browse mode
            browse_sql = (
                f"SELECT k.* FROM wm_knowledge k "
                f"WHERE {filter_clause} "
                f"ORDER BY k.created_at DESC LIMIT ${param_idx} OFFSET ${param_idx + 1}"
            )
            params.extend([limit, offset])
            rows = await self._fetchall(browse_sql, tuple(params))

        for row in rows:
            entry = _deserialize_row("knowledge", row)
            entry.pop("embedding", None)

            rank = dict(row).get("rank_score")
            if rank is not None:
                entry["bm25_score"] = float(rank)

            # pgvector cosine similarity
            raw_emb = dict(row).get("embedding")
            if query_embedding and raw_emb:
                try:
                    # Use pgvector's <=> operator via a sub-query would be ideal,
                    # but for simplicity we compute in Python for hybrid re-ranking
                    entry["embedding_score"] = 0.0  # placeholder
                except Exception:
                    pass

            if not include_content:
                entry.pop("content", None)

            results.append(entry)

        # Hybrid re-ranking
        has_fts = any("bm25_score" in r for r in results)
        has_emb = any("embedding_score" in r and r["embedding_score"] > 0 for r in results)

        if has_fts and has_emb:
            fts_vals = [r.get("bm25_score", 0) for r in results]
            fts_min = min(fts_vals) if fts_vals else 0
            fts_max = max(fts_vals) if fts_vals else 0
            fts_range = fts_max - fts_min if fts_max != fts_min else 1.0

            for r in results:
                norm_fts = ((r.get("bm25_score", 0) - fts_min) / fts_range) if fts_range else 0.5
                emb_score = r.get("embedding_score", 0.0)
                r["combined_score"] = round(0.4 * norm_fts + 0.6 * emb_score, 4)

            results.sort(key=lambda r: r.get("combined_score", 0), reverse=True)
        elif has_fts:
            results.sort(key=lambda r: r.get("bm25_score", 0), reverse=True)
        elif has_emb:
            results.sort(key=lambda r: r.get("embedding_score", 0), reverse=True)

        if query:
            results = results[offset:offset + limit]

        if tags:
            tag_set = set(tags)
            results = [r for r in results if tag_set & set(r.get("tags", []))]

        return results

    async def _like_search(
        self,
        query: str,
        where_parts: List[str],
        params: List[Any],
        param_idx: int,
        limit: int,
    ) -> List[Any]:
        """Fallback LIKE-based search when FTS parsing fails."""
        like_param = f"%{query}%"
        like_clause = (
            f"(k.title LIKE ${param_idx} OR k.content LIKE ${param_idx + 1} "
            f"OR k.category LIKE ${param_idx + 2} OR k.target LIKE ${param_idx + 3})"
        )
        params_out = list(params) + [like_param] * 4
        param_idx += 4

        filter_clause = " AND ".join(where_parts) if where_parts else "TRUE"
        sql = (
            f"SELECT k.* FROM wm_knowledge k "
            f"WHERE {like_clause} AND {filter_clause} "
            f"ORDER BY k.created_at DESC LIMIT ${param_idx}"
        )
        params_out.append(limit)
        return await self._fetchall(sql, tuple(params_out))

    async def get_knowledge_chunks(self, parent_id: str) -> List[Dict[str, Any]]:
        """Reassemble all chunks for a parent knowledge entry."""
        rows = await self._fetchall(
            "SELECT * FROM wm_knowledge WHERE (id = $1 OR parent_id = $2) "
            "AND assessment_id = $3 ORDER BY chunk_index",
            (parent_id, parent_id, self._assessment_id),
        )
        return [_deserialize_row("knowledge", r) for r in rows]

    async def delete_knowledge(self, knowledge_id: str) -> bool:
        """Delete a knowledge entry and all its chunks atomically."""
        await self._execute_batch([
            ("DELETE FROM wm_knowledge WHERE parent_id = $1 AND assessment_id = $2",
             (knowledge_id, self._assessment_id)),
            ("DELETE FROM wm_knowledge WHERE id = $1 AND assessment_id = $2",
             (knowledge_id, self._assessment_id)),
        ])
        return True

    async def knowledge_stats(self) -> Dict[str, Any]:
        """Return knowledge store statistics."""
        total_row = await self._fetchone(
            "SELECT COUNT(*) as cnt, COALESCE(SUM(content_size), 0) as total_size "
            "FROM wm_knowledge WHERE chunk_index = 0 AND assessment_id = $1",
            (self._assessment_id,),
        )
        total = total_row["cnt"] if total_row else 0
        total_size = total_row["total_size"] if total_row else 0

        cat_rows = await self._fetchall(
            "SELECT category, COUNT(*) as cnt FROM wm_knowledge "
            "WHERE chunk_index = 0 AND assessment_id = $1 GROUP BY category ORDER BY cnt DESC",
            (self._assessment_id,),
        )
        by_category = {r["category"]: r["cnt"] for r in cat_rows}

        src_rows = await self._fetchall(
            "SELECT source_tool, COUNT(*) as cnt FROM wm_knowledge "
            "WHERE chunk_index = 0 AND assessment_id = $1 GROUP BY source_tool ORDER BY cnt DESC",
            (self._assessment_id,),
        )
        by_source = {r["source_tool"]: r["cnt"] for r in src_rows}

        return {
            "total_entries": total,
            "total_size_bytes": total_size,
            "by_category": by_category,
            "by_source_tool": by_source,
        }

    # ------------------------------------------------------------------
    # Assets
    # ------------------------------------------------------------------

    async def add_asset(
        self,
        kind: str,
        name: str,
        metadata: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """Add an asset (domain, subdomain, IP, service, application)."""
        if kind not in ASSET_KINDS:
            raise ValueError(f"Invalid asset kind '{kind}'. Must be one of: {', '.join(ASSET_KINDS)}")
        if not name:
            raise ValueError("Asset name is required")

        now = _now_iso()
        asset_id = _new_id()
        meta = metadata or {}
        tags_val = tags or []

        await self._execute(
            "INSERT INTO wm_assets (id, assessment_id, kind, name, metadata, tags, discovered_at, updated_at) "
            "VALUES ($1, $2, $3, $4, $5, $6, $7, $8)",
            (asset_id, self._assessment_id, kind, name,
             json.dumps(meta), json.dumps(tags_val), now, now),
        )

        return {
            "id": asset_id,
            "kind": kind,
            "name": name,
            "metadata": meta,
            "tags": tags_val,
            "discovered_at": now,
            "updated_at": now,
        }

    # ------------------------------------------------------------------
    # Endpoints
    # ------------------------------------------------------------------

    async def add_endpoint(
        self,
        asset_id: str,
        method: str,
        path: str,
        parameters: Optional[Dict[str, Any]] = None,
        auth_required: bool = False,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Add an API endpoint linked to an asset."""
        if not asset_id:
            raise ValueError("asset_id is required")
        if not method:
            raise ValueError("method is required")
        if not path:
            raise ValueError("path is required")

        now = _now_iso()
        endpoint_id = _new_id()
        params_val = parameters or {}
        meta = metadata or {}

        await self._execute(
            "INSERT INTO wm_endpoints "
            "(id, assessment_id, asset_id, method, path, parameters, auth_required, "
            "status, metadata, discovered_at, updated_at) "
            "VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)",
            (endpoint_id, self._assessment_id, asset_id, method, path,
             json.dumps(params_val), auth_required, "discovered",
             json.dumps(meta), now, now),
        )

        return {
            "id": endpoint_id,
            "asset_id": asset_id,
            "method": method,
            "path": path,
            "parameters": params_val,
            "auth_required": auth_required,
            "status": "discovered",
            "metadata": meta,
            "discovered_at": now,
            "updated_at": now,
        }

    async def update_endpoint(
        self,
        endpoint_id: str,
        parameters: Optional[Dict[str, Any]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        status: Optional[str] = None,
        auth_required: Optional[bool] = None,
    ) -> Dict[str, Any]:
        """Update an endpoint's parameters, metadata, status, or auth_required."""
        if not endpoint_id:
            raise ValueError("endpoint_id is required")

        row = await self._fetchone(
            "SELECT * FROM wm_endpoints WHERE id = $1 AND assessment_id = $2",
            (endpoint_id, self._assessment_id),
        )
        if row is None:
            raise ValueError(f"Endpoint not found: {endpoint_id}")

        existing = _deserialize_row("endpoints", row)
        now = _now_iso()

        sets: List[str] = ["updated_at = $1"]
        params: List[Any] = [now]
        idx = 2

        if parameters is not None:
            sets.append(f"parameters = ${idx}")
            params.append(json.dumps(parameters))
            idx += 1
        if metadata is not None:
            sets.append(f"metadata = ${idx}")
            params.append(json.dumps(metadata))
            idx += 1
        if status is not None:
            if status not in ENDPOINT_STATUSES:
                raise ValueError(
                    f"Invalid status '{status}'. Must be one of: {', '.join(ENDPOINT_STATUSES)}"
                )
            sets.append(f"status = ${idx}")
            params.append(status)
            idx += 1
        if auth_required is not None:
            sets.append(f"auth_required = ${idx}")
            params.append(auth_required)
            idx += 1

        params.append(endpoint_id)
        params.append(self._assessment_id)
        await self._execute(
            f"UPDATE wm_endpoints SET {', '.join(sets)} "
            f"WHERE id = ${idx} AND assessment_id = ${idx + 1}",
            tuple(params),
        )

        if parameters is not None:
            existing["parameters"] = parameters
        if metadata is not None:
            existing["metadata"] = metadata
        if status is not None:
            existing["status"] = status
        if auth_required is not None:
            existing["auth_required"] = auth_required
        existing["updated_at"] = now
        return existing

    # ------------------------------------------------------------------
    # Identities
    # ------------------------------------------------------------------

    async def add_identity(
        self,
        name: str,
        auth_type: str,
        scope: str,
        permissions: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Add a test identity / credential."""
        if not name:
            raise ValueError("Identity name is required")
        if not auth_type:
            raise ValueError("auth_type is required")
        if not scope:
            raise ValueError("scope is required")

        now = _now_iso()
        identity_id = _new_id()
        perms = permissions or []
        meta = metadata or {}

        await self._execute(
            "INSERT INTO wm_identities "
            "(id, assessment_id, name, auth_type, scope, permissions, status, metadata, "
            "created_at, updated_at) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)",
            (identity_id, self._assessment_id, name, auth_type, scope,
             json.dumps(perms), "active", json.dumps(meta), now, now),
        )

        return {
            "id": identity_id,
            "name": name,
            "auth_type": auth_type,
            "scope": scope,
            "permissions": perms,
            "status": "active",
            "metadata": meta,
            "created_at": now,
            "updated_at": now,
        }

    # ------------------------------------------------------------------
    # Hypotheses
    # ------------------------------------------------------------------

    async def add_hypothesis(
        self,
        title: str,
        description: str,
        severity: str,
        target_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Create a security hypothesis to test."""
        if not title:
            raise ValueError("Hypothesis title is required")
        if not description:
            raise ValueError("Hypothesis description is required")
        if severity not in HYPOTHESIS_SEVERITIES:
            raise ValueError(
                f"Invalid severity '{severity}'. Must be one of: {', '.join(HYPOTHESIS_SEVERITIES)}"
            )

        now = _now_iso()
        hyp_id = _new_id()

        await self._execute(
            "INSERT INTO wm_hypotheses "
            "(id, assessment_id, title, description, severity, status, evidence, "
            "target_id, created_at, updated_at) "
            "VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)",
            (hyp_id, self._assessment_id, title, description, severity,
             "proposed", "[]", target_id, now, now),
        )

        return {
            "id": hyp_id,
            "title": title,
            "description": description,
            "severity": severity,
            "status": "proposed",
            "evidence": [],
            "target_id": target_id,
            "created_at": now,
            "updated_at": now,
        }

    async def update_hypothesis(
        self,
        hypothesis_id: str,
        status: Optional[str] = None,
        evidence: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """Update a hypothesis status and/or evidence."""
        if not hypothesis_id:
            raise ValueError("hypothesis_id is required")

        row = await self._fetchone(
            "SELECT * FROM wm_hypotheses WHERE id = $1 AND assessment_id = $2",
            (hypothesis_id, self._assessment_id),
        )
        if row is None:
            raise ValueError(f"Hypothesis not found: {hypothesis_id}")

        existing = _deserialize_row("hypotheses", row)
        now = _now_iso()

        new_status = status if status is not None else existing["status"]
        if new_status not in HYPOTHESIS_STATUSES:
            raise ValueError(
                f"Invalid status '{new_status}'. Must be one of: {', '.join(HYPOTHESIS_STATUSES)}"
            )

        current_evidence = existing.get("evidence", [])
        if not isinstance(current_evidence, list):
            current_evidence = []
        if evidence is not None:
            current_evidence = current_evidence + evidence

        await self._execute(
            "UPDATE wm_hypotheses SET status = $1, evidence = $2, updated_at = $3 "
            "WHERE id = $4 AND assessment_id = $5",
            (new_status, json.dumps(current_evidence), now,
             hypothesis_id, self._assessment_id),
        )

        existing["status"] = new_status
        existing["evidence"] = current_evidence
        existing["updated_at"] = now
        return existing

    # ------------------------------------------------------------------
    # Findings
    # ------------------------------------------------------------------

    async def add_finding(
        self,
        hypothesis_id: str,
        title: str,
        severity: str,
        confidence: float,
        evidence_ids: Optional[List[str]] = None,
        remediation: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Record a confirmed finding."""
        if not hypothesis_id:
            raise ValueError("hypothesis_id is required")
        if not title:
            raise ValueError("Finding title is required")
        if severity not in HYPOTHESIS_SEVERITIES:
            raise ValueError(
                f"Invalid severity '{severity}'. Must be one of: {', '.join(HYPOTHESIS_SEVERITIES)}"
            )
        if not (0.0 <= confidence <= 1.0):
            raise ValueError("confidence must be between 0.0 and 1.0")

        now = _now_iso()
        finding_id = _new_id()
        eids = evidence_ids or []
        meta = metadata or {}

        await self._execute(
            "INSERT INTO wm_findings "
            "(id, assessment_id, hypothesis_id, title, severity, confidence, status, "
            "evidence_ids, remediation, metadata, created_at, updated_at) "
            "VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)",
            (finding_id, self._assessment_id, hypothesis_id, title, severity,
             confidence, "draft", json.dumps(eids), remediation,
             json.dumps(meta), now, now),
        )

        return {
            "id": finding_id,
            "hypothesis_id": hypothesis_id,
            "title": title,
            "severity": severity,
            "confidence": confidence,
            "status": "draft",
            "evidence_ids": eids,
            "remediation": remediation,
            "metadata": meta,
            "created_at": now,
            "updated_at": now,
        }

    async def update_finding(
        self,
        finding_id: str,
        status: Optional[str] = None,
        confidence: Optional[float] = None,
        remediation: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Update a finding's status, confidence, or remediation."""
        if not finding_id:
            raise ValueError("finding_id is required")

        row = await self._fetchone(
            "SELECT * FROM wm_findings WHERE id = $1 AND assessment_id = $2",
            (finding_id, self._assessment_id),
        )
        if row is None:
            raise ValueError(f"Finding not found: {finding_id}")

        existing = _deserialize_row("findings", row)
        now = _now_iso()

        new_status = status if status is not None else existing["status"]
        if new_status not in FINDING_STATUSES:
            raise ValueError(
                f"Invalid status '{new_status}'. Must be one of: {', '.join(FINDING_STATUSES)}"
            )

        new_confidence = confidence if confidence is not None else existing["confidence"]
        if not (0.0 <= new_confidence <= 1.0):
            raise ValueError("confidence must be between 0.0 and 1.0")

        new_remediation = remediation if remediation is not None else existing.get("remediation")

        await self._execute(
            "UPDATE wm_findings SET status = $1, confidence = $2, remediation = $3, "
            "updated_at = $4 WHERE id = $5 AND assessment_id = $6",
            (new_status, new_confidence, new_remediation, now,
             finding_id, self._assessment_id),
        )

        existing["status"] = new_status
        existing["confidence"] = new_confidence
        existing["remediation"] = new_remediation
        existing["updated_at"] = now
        return existing

    # ------------------------------------------------------------------
    # Observations
    # ------------------------------------------------------------------

    async def add_observation(
        self,
        hypothesis_id: str,
        obs_type: str,
        content: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Add an observation to a hypothesis."""
        if not hypothesis_id:
            raise ValueError("hypothesis_id is required")
        if obs_type not in OBSERVATION_TYPES:
            raise ValueError(
                f"Invalid observation type '{obs_type}'. "
                f"Must be one of: {', '.join(OBSERVATION_TYPES)}"
            )
        if not content:
            raise ValueError("Observation content is required")

        now = _now_iso()
        obs_id = _new_id()
        meta = metadata or {}

        await self._execute(
            "INSERT INTO wm_observations "
            "(id, assessment_id, hypothesis_id, type, content, metadata, observed_at) "
            "VALUES ($1, $2, $3, $4, $5, $6, $7)",
            (obs_id, self._assessment_id, hypothesis_id, obs_type, content,
             json.dumps(meta), now),
        )

        return {
            "id": obs_id,
            "hypothesis_id": hypothesis_id,
            "type": obs_type,
            "content": content,
            "metadata": meta,
            "observed_at": now,
        }

    # ------------------------------------------------------------------
    # Plans
    # ------------------------------------------------------------------

    async def add_plan(
        self,
        title: str,
        goal: str,
        steps: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """Create a testing plan with structured steps."""
        if not title:
            raise ValueError("Plan title is required")
        if not goal:
            raise ValueError("Plan goal is required")
        if not steps or not isinstance(steps, list):
            raise ValueError("steps must be a non-empty list")

        now = _now_iso()
        plan_id = _new_id()

        normalized: List[Dict[str, Any]] = []
        for s in steps:
            if isinstance(s, dict):
                normalized.append({
                    "description": s.get("description", ""),
                    "status": "pending",
                    "result": "",
                })
            elif isinstance(s, str):
                normalized.append({"description": s, "status": "pending", "result": ""})

        await self._execute(
            "INSERT INTO wm_plans "
            "(id, assessment_id, title, goal, steps, status, reflection, created_at, updated_at) "
            "VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)",
            (plan_id, self._assessment_id, title, goal,
             json.dumps(normalized), "active", "", now, now),
        )

        return {
            "id": plan_id,
            "title": title,
            "goal": goal,
            "steps": normalized,
            "status": "active",
            "reflection": "",
            "created_at": now,
            "updated_at": now,
        }

    async def update_plan(
        self,
        plan_id: str,
        step_index: Optional[int] = None,
        step_status: Optional[str] = None,
        step_result: Optional[str] = None,
        reflection: Optional[str] = None,
        status: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Update a plan's step status, reflection, or overall status."""
        if not plan_id:
            raise ValueError("plan_id is required")

        row = await self._fetchone(
            "SELECT * FROM wm_plans WHERE id = $1 AND assessment_id = $2",
            (plan_id, self._assessment_id),
        )
        if row is None:
            raise ValueError(f"Plan not found: {plan_id}")

        existing = _deserialize_row("plans", row)
        now = _now_iso()

        current_steps = existing.get("steps", [])
        if not isinstance(current_steps, list):
            current_steps = []

        if step_index is not None:
            if step_index < 0 or step_index >= len(current_steps):
                raise ValueError(
                    f"step_index {step_index} out of range (0-{len(current_steps) - 1})"
                )
            if step_status is not None:
                if step_status not in PLAN_STEP_STATUSES:
                    raise ValueError(
                        f"Invalid step_status '{step_status}'. Must be one of: {', '.join(PLAN_STEP_STATUSES)}"
                    )
                current_steps[step_index]["status"] = step_status
            if step_result is not None:
                current_steps[step_index]["result"] = step_result

        new_status = status if status is not None else existing["status"]
        if new_status not in PLAN_STATUSES:
            raise ValueError(
                f"Invalid plan status '{new_status}'. Must be one of: {', '.join(PLAN_STATUSES)}"
            )

        new_reflection = reflection if reflection is not None else existing.get("reflection", "")

        await self._execute(
            "UPDATE wm_plans SET steps = $1, status = $2, reflection = $3, updated_at = $4 "
            "WHERE id = $5 AND assessment_id = $6",
            (json.dumps(current_steps), new_status, new_reflection, now,
             plan_id, self._assessment_id),
        )

        existing["steps"] = current_steps
        existing["status"] = new_status
        existing["reflection"] = new_reflection
        existing["updated_at"] = now
        return existing

    # ------------------------------------------------------------------
    # Relationships (graph edges)
    # ------------------------------------------------------------------

    async def add_relationship(
        self,
        source_type: str,
        source_id: str,
        target_type: str,
        target_id: str,
        rel_type: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Add a typed edge between two entities.  Idempotent (ON CONFLICT DO NOTHING)."""
        if source_type not in VALID_ENTITY_TYPES:
            raise ValueError(f"Invalid source_type '{source_type}'. Must be one of: {', '.join(VALID_ENTITY_TYPES)}")
        if target_type not in VALID_ENTITY_TYPES:
            raise ValueError(f"Invalid target_type '{target_type}'. Must be one of: {', '.join(VALID_ENTITY_TYPES)}")
        if rel_type not in VALID_REL_TYPES:
            raise ValueError(f"Invalid rel_type '{rel_type}'. Must be one of: {', '.join(VALID_REL_TYPES)}")

        now = _now_iso()
        rel_id = _new_id()
        meta = metadata or {}

        await self._execute(
            "INSERT INTO wm_relationships "
            "(id, assessment_id, source_type, source_id, target_type, target_id, "
            "rel_type, metadata, created_at) "
            "VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9) "
            "ON CONFLICT (assessment_id, source_type, source_id, target_type, target_id, rel_type) "
            "DO NOTHING",
            (rel_id, self._assessment_id, source_type, source_id, target_type,
             target_id, rel_type, json.dumps(meta), now),
        )

        return {
            "id": rel_id,
            "source_type": source_type,
            "source_id": source_id,
            "target_type": target_type,
            "target_id": target_id,
            "rel_type": rel_type,
            "metadata": meta,
            "created_at": now,
        }

    async def query_relationships(
        self,
        source_type: Optional[str] = None,
        source_id: Optional[str] = None,
        target_type: Optional[str] = None,
        target_id: Optional[str] = None,
        rel_type: Optional[str] = None,
        limit: int = 100,
    ) -> List[Dict[str, Any]]:
        """Query relationships with optional filters."""
        where_parts: List[str] = [f"assessment_id = ${1}"]
        params: List[Any] = [self._assessment_id]
        idx = 2

        if source_type:
            where_parts.append(f"source_type = ${idx}")
            params.append(source_type)
            idx += 1
        if source_id:
            where_parts.append(f"source_id = ${idx}")
            params.append(source_id)
            idx += 1
        if target_type:
            where_parts.append(f"target_type = ${idx}")
            params.append(target_type)
            idx += 1
        if target_id:
            where_parts.append(f"target_id = ${idx}")
            params.append(target_id)
            idx += 1
        if rel_type:
            where_parts.append(f"rel_type = ${idx}")
            params.append(rel_type)
            idx += 1

        sql = f"SELECT * FROM wm_relationships WHERE {' AND '.join(where_parts)} "
        sql += f"ORDER BY created_at DESC LIMIT ${idx}"
        params.append(limit)

        rows = await self._fetchall(sql, tuple(params))
        return [_deserialize_row("relationships", r) for r in rows]

    async def get_neighbors(
        self,
        entity_type: str,
        entity_id: str,
        rel_types: Optional[List[str]] = None,
        direction: str = "both",
        depth: int = 1,
    ) -> Dict[str, Any]:
        """BFS traversal from an entity.  Returns connected nodes + edges."""
        depth = min(max(depth, 1), 4)
        visited_nodes: set = set()
        visited_edges: set = set()
        nodes: List[Dict[str, Any]] = []
        edges: List[Dict[str, Any]] = []
        queue: List[tuple] = [(entity_type, entity_id, 0)]
        visited_nodes.add((entity_type, entity_id))

        while queue:
            cur_type, cur_id, cur_depth = queue.pop(0)
            if cur_depth >= depth:
                continue

            rels: List[Dict[str, Any]] = []
            if direction in ("outgoing", "both"):
                sql = (
                    "SELECT * FROM wm_relationships "
                    "WHERE source_type = $1 AND source_id = $2 AND assessment_id = $3"
                )
                params: List[Any] = [cur_type, cur_id, self._assessment_id]
                if rel_types:
                    placeholders = ", ".join(f"${i}" for i in range(4, 4 + len(rel_types)))
                    sql += f" AND rel_type IN ({placeholders})"
                    params.extend(rel_types)
                rows = await self._fetchall(sql, tuple(params))
                for r in rows:
                    rels.append(_deserialize_row("relationships", r))

            if direction in ("incoming", "both"):
                sql = (
                    "SELECT * FROM wm_relationships "
                    "WHERE target_type = $1 AND target_id = $2 AND assessment_id = $3"
                )
                params = [cur_type, cur_id, self._assessment_id]
                if rel_types:
                    placeholders = ", ".join(f"${i}" for i in range(4, 4 + len(rel_types)))
                    sql += f" AND rel_type IN ({placeholders})"
                    params.extend(rel_types)
                rows = await self._fetchall(sql, tuple(params))
                for r in rows:
                    rels.append(_deserialize_row("relationships", r))

            for rel in rels:
                edge_key = (rel["source_type"], rel["source_id"],
                            rel["target_type"], rel["target_id"], rel["rel_type"])
                if edge_key not in visited_edges:
                    visited_edges.add(edge_key)
                    edges.append(rel)

                if rel["source_type"] == cur_type and rel["source_id"] == cur_id:
                    neighbor = (rel["target_type"], rel["target_id"])
                else:
                    neighbor = (rel["source_type"], rel["source_id"])

                if neighbor not in visited_nodes:
                    visited_nodes.add(neighbor)
                    nodes.append({"type": neighbor[0], "id": neighbor[1]})
                    queue.append((neighbor[0], neighbor[1], cur_depth + 1))

        return {
            "root": {"type": entity_type, "id": entity_id},
            "nodes": nodes,
            "edges": edges,
            "depth": depth,
        }

    async def get_attack_path(
        self,
        from_type: str,
        from_id: str,
        to_type: str,
        to_id: str,
        max_depth: int = 8,
    ) -> Optional[List[Dict[str, Any]]]:
        """Find shortest path between two entities via BFS."""
        max_depth = min(max(max_depth, 1), 8)
        target = (to_type, to_id)
        visited: set = set()
        visited.add((from_type, from_id))
        queue: List[tuple] = [(from_type, from_id, [])]

        while queue:
            cur_type, cur_id, path = queue.pop(0)
            if len(path) >= max_depth:
                continue

            rows = await self._fetchall(
                "SELECT * FROM wm_relationships WHERE "
                "((source_type = $1 AND source_id = $2) OR "
                "(target_type = $3 AND target_id = $4)) AND assessment_id = $5",
                (cur_type, cur_id, cur_type, cur_id, self._assessment_id),
            )

            for r in rows:
                rel = _deserialize_row("relationships", r)
                if rel["source_type"] == cur_type and rel["source_id"] == cur_id:
                    neighbor = (rel["target_type"], rel["target_id"])
                else:
                    neighbor = (rel["source_type"], rel["source_id"])

                new_path = path + [rel]

                if neighbor == target:
                    return new_path

                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((neighbor[0], neighbor[1], new_path))

        return None

    # ------------------------------------------------------------------
    # Coverage matrix
    # ------------------------------------------------------------------

    async def coverage_init_rows(self, rows: List[Dict[str, Any]]) -> Dict[str, int]:
        """Batch INSERT coverage cells (ON CONFLICT DO NOTHING).

        Returns ``{created, skipped_existing}``.
        """
        now = _now_iso()
        insert_sql = (
            "INSERT INTO wm_coverage_matrix "
            "(id, assessment_id, endpoint_id, vuln_class, parameter, status, "
            "tool_name, tool_args, priority, created_at, updated_at) "
            "VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11) "
            "ON CONFLICT (assessment_id, endpoint_id, vuln_class, parameter) DO NOTHING"
        )
        statements = []
        for row in rows:
            cell_id = _new_id()
            statements.append((
                insert_sql,
                (
                    cell_id,
                    self._assessment_id,
                    row["endpoint_id"],
                    row["vuln_class"],
                    row.get("parameter", ""),
                    row.get("status", "pending"),
                    row.get("tool_name", ""),
                    json.dumps(row.get("tool_args", {})),
                    row.get("priority", 50),
                    now,
                    now,
                ),
            ))

        total = len(statements)
        await self._execute_batch(statements)

        count_row = await self._fetchone(
            "SELECT COUNT(*) as cnt FROM wm_coverage_matrix "
            "WHERE created_at = $1 AND assessment_id = $2",
            (now, self._assessment_id),
        )
        created = count_row["cnt"] if count_row else 0
        return {"created": created, "skipped_existing": total - created}

    async def coverage_next(
        self,
        vuln_class: Optional[str] = None,
        endpoint_id: Optional[str] = None,
        limit: int = 5,
    ) -> List[Dict[str, Any]]:
        """Return next pending coverage cells, joined with endpoint info."""
        where_parts = ["cm.status = 'pending'", f"cm.assessment_id = ${1}"]
        params: List[Any] = [self._assessment_id]
        idx = 2

        if vuln_class:
            where_parts.append(f"cm.vuln_class = ${idx}")
            params.append(vuln_class)
            idx += 1
        if endpoint_id:
            where_parts.append(f"cm.endpoint_id = ${idx}")
            params.append(endpoint_id)
            idx += 1

        sql = (
            "SELECT cm.*, e.method, e.path, e.parameters AS ep_parameters, "
            "e.auth_required, e.asset_id "
            "FROM wm_coverage_matrix cm "
            "JOIN wm_endpoints e ON e.id = cm.endpoint_id "
            f"WHERE {' AND '.join(where_parts)} "
            f"ORDER BY cm.priority DESC, cm.created_at ASC "
            f"LIMIT ${idx}"
        )
        params.append(limit)

        rows = await self._fetchall(sql, tuple(params))
        results = []
        for r in rows:
            d = _deserialize_row("coverage_matrix", r)
            ep_params = d.pop("ep_parameters", {})
            # asyncpg auto-deserializes JSONB, but handle str just in case
            if isinstance(ep_params, str):
                try:
                    ep_params = json.loads(ep_params)
                except (json.JSONDecodeError, TypeError):
                    ep_params = {}
            d["endpoint"] = {
                "method": d.pop("method", ""),
                "path": d.pop("path", ""),
                "parameters": ep_params,
                "auth_required": d.pop("auth_required", False),
                "asset_id": d.pop("asset_id", ""),
            }
            results.append(d)
        return results

    async def coverage_mark(
        self,
        cell_id: str,
        status: str,
        finding_id: Optional[str] = None,
        result_summary: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Update a coverage cell's status and set completed_at."""
        if status not in COVERAGE_STATUSES:
            raise ValueError(f"Invalid status '{status}'. Must be one of: {', '.join(COVERAGE_STATUSES)}")

        now = _now_iso()
        completed = now if status in ("passed", "vulnerable", "skipped", "error") else None

        sets = ["status = $1", "updated_at = $2"]
        params: List[Any] = [status, now]
        idx = 3

        if finding_id is not None:
            sets.append(f"finding_id = ${idx}")
            params.append(finding_id)
            idx += 1
        if result_summary is not None:
            sets.append(f"result_summary = ${idx}")
            params.append(result_summary)
            idx += 1
        if status == "in_progress":
            sets.append(f"attempted_at = ${idx}")
            params.append(now)
            idx += 1
        if completed:
            sets.append(f"completed_at = ${idx}")
            params.append(completed)
            idx += 1

        params.append(cell_id)
        params.append(self._assessment_id)
        await self._execute(
            f"UPDATE wm_coverage_matrix SET {', '.join(sets)} "
            f"WHERE id = ${idx} AND assessment_id = ${idx + 1}",
            tuple(params),
        )

        row = await self._fetchone(
            "SELECT * FROM wm_coverage_matrix WHERE id = $1 AND assessment_id = $2",
            (cell_id, self._assessment_id),
        )
        if row is None:
            raise ValueError(f"Coverage cell not found: {cell_id}")
        return _deserialize_row("coverage_matrix", row)

    async def coverage_report(
        self,
        endpoint_id: Optional[str] = None,
        vuln_class: Optional[str] = None,
    ) -> Dict[str, Any]:
        """Aggregate coverage stats with GROUP BY."""
        where_parts: List[str] = [f"assessment_id = ${1}"]
        params: List[Any] = [self._assessment_id]
        idx = 2

        if endpoint_id:
            where_parts.append(f"endpoint_id = ${idx}")
            params.append(endpoint_id)
            idx += 1
        if vuln_class:
            where_parts.append(f"vuln_class = ${idx}")
            params.append(vuln_class)
            idx += 1

        where_clause = " WHERE " + " AND ".join(where_parts)

        total_row = await self._fetchone(
            f"SELECT COUNT(*) as cnt FROM wm_coverage_matrix{where_clause}",
            tuple(params),
        )
        total = total_row["cnt"] if total_row else 0

        status_rows = await self._fetchall(
            f"SELECT status, COUNT(*) as cnt FROM wm_coverage_matrix{where_clause} "
            "GROUP BY status",
            tuple(params),
        )
        by_status = {r["status"]: r["cnt"] for r in status_rows}

        vc_rows = await self._fetchall(
            f"SELECT vuln_class, COUNT(*) as cnt FROM wm_coverage_matrix{where_clause} "
            "GROUP BY vuln_class ORDER BY cnt DESC",
            tuple(params),
        )
        by_vuln_class = {r["vuln_class"]: r["cnt"] for r in vc_rows}

        completed = sum(by_status.get(s, 0) for s in ("passed", "vulnerable", "skipped", "error"))
        coverage_pct = round((completed / total * 100), 1) if total > 0 else 0.0

        # Gaps: pending cells ordered by priority
        gap_where = list(where_parts)
        gap_params = list(params)
        gap_where.append("cm.status = 'pending'")

        gap_sql = (
            f"SELECT cm.*, e.method, e.path FROM wm_coverage_matrix cm "
            f"JOIN wm_endpoints e ON e.id = cm.endpoint_id "
            f"WHERE {' AND '.join(['cm.' + w if not w.startswith('cm.') else w for w in gap_where])} "
            f"ORDER BY cm.priority DESC LIMIT 20"
        )
        # Fix: prefix assessment_id and endpoint_id correctly for joined query
        gap_sql_fixed = gap_sql.replace(
            "cm.assessment_id", "cm.assessment_id"
        ).replace(
            "cm.endpoint_id", "cm.endpoint_id"
        ).replace(
            "cm.vuln_class", "cm.vuln_class"
        )
        gap_rows = await self._fetchall(gap_sql_fixed, tuple(gap_params))
        gaps = []
        for r in gap_rows:
            d = _deserialize_row("coverage_matrix", r)
            d["endpoint_method"] = d.pop("method", "")
            d["endpoint_path"] = d.pop("path", "")
            gaps.append(d)

        return {
            "total_cells": total,
            "by_status": by_status,
            "by_vuln_class": by_vuln_class,
            "coverage_pct": coverage_pct,
            "gaps": gaps,
        }

    async def coverage_query(
        self,
        query_type: str,
        filters: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """Pre-built SQL queries for structured coverage analysis."""
        filters = filters or {}
        aid = self._assessment_id

        if query_type == "untested_endpoints":
            rows = await self._fetchall(
                "SELECT e.id, e.method, e.path, e.asset_id, "
                "COUNT(cm.id) as total_cells, "
                "SUM(CASE WHEN cm.status = 'pending' THEN 1 ELSE 0 END) as pending_cells "
                "FROM wm_endpoints e "
                "LEFT JOIN wm_coverage_matrix cm ON cm.endpoint_id = e.id AND cm.assessment_id = $1 "
                "WHERE e.assessment_id = $1 "
                "GROUP BY e.id, e.method, e.path, e.asset_id "
                "HAVING COUNT(cm.id) = 0 OR COUNT(cm.id) = SUM(CASE WHEN cm.status = 'pending' THEN 1 ELSE 0 END) "
                "LIMIT 50",
                (aid,),
            )
        elif query_type == "coverage_by_vuln_class":
            rows = await self._fetchall(
                "SELECT vuln_class, "
                "COUNT(*) as total, "
                "SUM(CASE WHEN status = 'passed' THEN 1 ELSE 0 END) as passed, "
                "SUM(CASE WHEN status = 'vulnerable' THEN 1 ELSE 0 END) as vulnerable, "
                "SUM(CASE WHEN status = 'pending' THEN 1 ELSE 0 END) as pending, "
                "SUM(CASE WHEN status = 'error' THEN 1 ELSE 0 END) as errors "
                "FROM wm_coverage_matrix WHERE assessment_id = $1 "
                "GROUP BY vuln_class ORDER BY vuln_class",
                (aid,),
            )
        elif query_type == "findings_by_endpoint":
            rows = await self._fetchall(
                "SELECT cm.endpoint_id, e.method, e.path, "
                "string_agg(cm.vuln_class, ',') as vuln_classes, "
                "COUNT(*) as finding_count "
                "FROM wm_coverage_matrix cm "
                "JOIN wm_endpoints e ON e.id = cm.endpoint_id "
                "WHERE cm.status = 'vulnerable' AND cm.assessment_id = $1 "
                "GROUP BY cm.endpoint_id, e.method, e.path "
                "ORDER BY finding_count DESC LIMIT 50",
                (aid,),
            )
        elif query_type == "vulnerable_endpoints":
            rows = await self._fetchall(
                "SELECT DISTINCT cm.endpoint_id, e.method, e.path, e.asset_id, "
                "cm.vuln_class, cm.finding_id, cm.result_summary "
                "FROM wm_coverage_matrix cm "
                "JOIN wm_endpoints e ON e.id = cm.endpoint_id "
                "WHERE cm.status = 'vulnerable' AND cm.assessment_id = $1 "
                "ORDER BY cm.completed_at DESC LIMIT 50",
                (aid,),
            )
        elif query_type == "high_priority_gaps":
            rows = await self._fetchall(
                "SELECT cm.*, e.method, e.path "
                "FROM wm_coverage_matrix cm "
                "JOIN wm_endpoints e ON e.id = cm.endpoint_id "
                "WHERE cm.status = 'pending' AND cm.priority >= 70 AND cm.assessment_id = $1 "
                "ORDER BY cm.priority DESC LIMIT 50",
                (aid,),
            )
        else:
            raise ValueError(
                f"Unknown query_type '{query_type}'. Must be one of: "
                "untested_endpoints, coverage_by_vuln_class, findings_by_endpoint, "
                "vulnerable_endpoints, high_priority_gaps"
            )

        return [dict(r) for r in rows]

    # ------------------------------------------------------------------
    # Generic query
    # ------------------------------------------------------------------

    async def query(
        self,
        table: str,
        filters: Optional[Dict[str, Any]] = None,
        limit: int = 100,
        offset: int = 0,
    ) -> List[Dict[str, Any]]:
        """Query any world-model table with optional filters."""
        if table not in _VALID_TABLES:
            raise ValueError(f"Invalid table '{table}'. Must be one of: {', '.join(sorted(_VALID_TABLES))}")

        pg_table = _TABLE_MAP[table]
        filters = filters or {}

        where_clauses: List[str] = [f"assessment_id = ${1}"]
        params: List[Any] = [self._assessment_id]
        idx = 2

        for col, val in filters.items():
            if not col.isidentifier():
                raise ValueError(f"Invalid filter column name: {col}")
            where_clauses.append(f"{col} = ${idx}")
            params.append(val)
            idx += 1

        sql = f"SELECT * FROM {pg_table} WHERE {' AND '.join(where_clauses)} "
        sql += f"ORDER BY created_at DESC LIMIT ${idx} OFFSET ${idx + 1}"
        params.extend([limit, offset])

        rows = await self._fetchall(sql, tuple(params))
        return [_deserialize_row(table, r) for r in rows]

    # ------------------------------------------------------------------
    # Get by ID
    # ------------------------------------------------------------------

    async def get_by_id(self, table: str, record_id: str) -> Optional[Dict[str, Any]]:
        """Fetch a single record by its ``id`` column."""
        if table not in _VALID_TABLES:
            raise ValueError(f"Invalid table '{table}'. Must be one of: {', '.join(sorted(_VALID_TABLES))}")

        pg_table = _TABLE_MAP[table]
        row = await self._fetchone(
            f"SELECT * FROM {pg_table} WHERE id = $1 AND assessment_id = $2",
            (record_id, self._assessment_id),
        )
        if row is None:
            return None
        return _deserialize_row(table, row)

    # ------------------------------------------------------------------
    # Statistics
    # ------------------------------------------------------------------

    async def get_stats(self) -> Dict[str, Any]:
        """Return the row count for every table in the world model."""
        stats: Dict[str, Any] = {}
        for table in sorted(_VALID_TABLES):
            pg_table = _TABLE_MAP[table]
            row = await self._fetchone(
                f"SELECT COUNT(*) as cnt FROM {pg_table} WHERE assessment_id = $1",
                (self._assessment_id,),
            )
            stats[table] = row["cnt"] if row else 0
        stats["knowledge_store"] = await self.knowledge_stats()
        return stats


# ---------------------------------------------------------------------------
# Per-assessment instance cache
# ---------------------------------------------------------------------------

_db_instances: Dict[int, WorldModelDatabase] = {}
_shared_pool: Any = None


def set_shared_pool(pool: Any) -> None:
    """Set the shared asyncpg connection pool (called once at startup)."""
    global _shared_pool
    _shared_pool = pool
    logger.info("WorldModelDatabase shared pool set")


async def get_world_model_db(assessment_id: int) -> WorldModelDatabase:
    """Return (or create and initialise) the database for *assessment_id*."""
    if not isinstance(assessment_id, int) or assessment_id <= 0:
        raise ValueError(f"assessment_id must be a positive integer, got: {assessment_id!r}")
    if assessment_id not in _db_instances:
        db = WorldModelDatabase(pool=_shared_pool, assessment_id=assessment_id)
        await db.init()
        _db_instances[assessment_id] = db
    return _db_instances[assessment_id]


async def reset_world_model_db(assessment_id: Optional[int] = None) -> None:
    """Close and remove database instance(s).

    If *assessment_id* is ``None``, close **all** instances.
    """
    if assessment_id is not None:
        db = _db_instances.pop(assessment_id, None)
        if db is not None:
            await db.close()
    else:
        for db in _db_instances.values():
            await db.close()
        _db_instances.clear()
