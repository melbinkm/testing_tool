"""LLM-Based Data-Flow Source-to-Sink Tracing.

Maps data flow from user input sources through sanitization steps
to dangerous sinks, using SAST findings and indexed code context.
"""

import uuid
from dataclasses import dataclass, field
from enum import Enum
from typing import List, Dict, Any, Optional


class InputType(Enum):
    USER_INPUT = "user_input"
    QUERY_PARAM = "query_param"
    FORM_FIELD = "form_field"
    HEADER = "header"
    COOKIE = "cookie"
    JSON_BODY = "json_body"
    FILE_UPLOAD = "file_upload"


class DangerLevel(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class DataFlowVerdict(Enum):
    SAFE = "safe"
    VULNERABLE = "vulnerable"
    NEEDS_REVIEW = "needs_review"


@dataclass
class DataFlowSource:
    file: str
    line: int
    param_name: str
    input_type: InputType = InputType.USER_INPUT


@dataclass
class DataFlowSink:
    file: str
    line: int
    function: str
    danger_level: DangerLevel = DangerLevel.HIGH


@dataclass
class SanitizationStep:
    file: str
    line: int
    transform_name: str
    effectiveness: float = 0.0  # 0.0 to 1.0


@dataclass
class DataFlowTrace:
    trace_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    source: Optional[DataFlowSource] = None
    sink: Optional[DataFlowSink] = None
    sanitization_chain: List[SanitizationStep] = field(default_factory=list)
    verdict: DataFlowVerdict = DataFlowVerdict.NEEDS_REVIEW
    confidence: float = 0.5

    def to_dict(self) -> Dict[str, Any]:
        return {
            "trace_id": self.trace_id,
            "source": {
                "file": self.source.file,
                "line": self.source.line,
                "param_name": self.source.param_name,
                "input_type": self.source.input_type.value,
            } if self.source else None,
            "sink": {
                "file": self.sink.file,
                "line": self.sink.line,
                "function": self.sink.function,
                "danger_level": self.sink.danger_level.value,
            } if self.sink else None,
            "sanitization_chain": [
                {
                    "file": s.file,
                    "line": s.line,
                    "transform_name": s.transform_name,
                    "effectiveness": s.effectiveness,
                }
                for s in self.sanitization_chain
            ],
            "verdict": self.verdict.value,
            "confidence": self.confidence,
        }


# Known dangerous sink patterns
_DANGEROUS_SINKS = {
    "exec": DangerLevel.CRITICAL,
    "eval": DangerLevel.CRITICAL,
    "system": DangerLevel.CRITICAL,
    "popen": DangerLevel.CRITICAL,
    "subprocess": DangerLevel.CRITICAL,
    "os.system": DangerLevel.CRITICAL,
    "cursor.execute": DangerLevel.HIGH,
    "query": DangerLevel.HIGH,
    "execute": DangerLevel.HIGH,
    "raw_sql": DangerLevel.HIGH,
    "innerHTML": DangerLevel.HIGH,
    "document.write": DangerLevel.HIGH,
    "render_template_string": DangerLevel.HIGH,
    "deserialize": DangerLevel.HIGH,
    "pickle.loads": DangerLevel.HIGH,
    "yaml.load": DangerLevel.MEDIUM,
    "open": DangerLevel.MEDIUM,
    "redirect": DangerLevel.MEDIUM,
    "send_file": DangerLevel.MEDIUM,
    "Response": DangerLevel.LOW,
}

# Known sanitization patterns
_SANITIZERS = {
    "escape": 0.8,
    "html_escape": 0.9,
    "sanitize": 0.7,
    "encode": 0.6,
    "parameterize": 0.95,
    "prepared_statement": 0.95,
    "bleach.clean": 0.85,
    "strip_tags": 0.7,
    "quote": 0.7,
    "validate": 0.5,
    "filter": 0.5,
    "DOMPurify": 0.9,
}

# Source detection patterns
_SOURCE_PATTERNS = {
    "request.args": InputType.QUERY_PARAM,
    "request.form": InputType.FORM_FIELD,
    "request.headers": InputType.HEADER,
    "request.cookies": InputType.COOKIE,
    "request.json": InputType.JSON_BODY,
    "request.files": InputType.FILE_UPLOAD,
    "request.GET": InputType.QUERY_PARAM,
    "request.POST": InputType.FORM_FIELD,
    "req.query": InputType.QUERY_PARAM,
    "req.body": InputType.JSON_BODY,
    "req.params": InputType.QUERY_PARAM,
    "req.headers": InputType.HEADER,
    "req.cookies": InputType.COOKIE,
    "$_GET": InputType.QUERY_PARAM,
    "$_POST": InputType.FORM_FIELD,
    "$_REQUEST": InputType.USER_INPUT,
    "$_COOKIE": InputType.COOKIE,
}


def trace_dataflow(
    sast_findings: List[Dict[str, Any]],
    indexed_code: Optional[Dict[str, Any]] = None,
) -> List[DataFlowTrace]:
    """Map SAST findings to source-sanitization-sink data flow traces.

    Parameters
    ----------
    sast_findings : list[dict]
        SAST findings from wm_knowledge (category='sast_finding').
    indexed_code : dict, optional
        Indexed code context (not used in heuristic mode, reserved for LLM).

    Returns
    -------
    list[DataFlowTrace]
        List of traced data flows with verdicts.
    """
    traces = []

    for finding in sast_findings:
        metadata = finding.get("metadata", {})
        if isinstance(metadata, str):
            import json
            try:
                metadata = json.loads(metadata)
            except (json.JSONDecodeError, TypeError):
                metadata = {}

        file_path = metadata.get("file", finding.get("title", ""))
        line_no = metadata.get("line", 0)
        content = finding.get("content", "")
        vuln_class = metadata.get("vuln_class", "")

        # Detect source
        source = _detect_source(content, file_path, line_no)

        # Detect sink
        sink = _detect_sink(content, vuln_class, file_path, line_no)

        # Detect sanitization
        sanitization = _detect_sanitization(content, file_path)

        # Calculate verdict
        verdict, confidence = _calculate_verdict(source, sink, sanitization)

        trace = DataFlowTrace(
            source=source,
            sink=sink,
            sanitization_chain=sanitization,
            verdict=verdict,
            confidence=confidence,
        )
        traces.append(trace)

    return traces


def _detect_source(
    content: str, file_path: str, line: int,
) -> Optional[DataFlowSource]:
    """Detect user input source in code content."""
    content_lower = content.lower()
    for pattern, input_type in _SOURCE_PATTERNS.items():
        if pattern.lower() in content_lower:
            # Try to extract param name
            param = pattern.split(".")[-1] if "." in pattern else pattern
            return DataFlowSource(
                file=file_path,
                line=line,
                param_name=param,
                input_type=input_type,
            )
    return None


def _detect_sink(
    content: str, vuln_class: str, file_path: str, line: int,
) -> Optional[DataFlowSink]:
    """Detect dangerous sink in code content."""
    content_lower = content.lower()
    for func, danger in _DANGEROUS_SINKS.items():
        if func.lower() in content_lower:
            return DataFlowSink(
                file=file_path,
                line=line,
                function=func,
                danger_level=danger,
            )
    # Default based on vuln class
    if vuln_class:
        danger = DangerLevel.HIGH if "injection" in vuln_class.lower() else DangerLevel.MEDIUM
        return DataFlowSink(
            file=file_path, line=line, function=vuln_class, danger_level=danger,
        )
    return None


def _detect_sanitization(
    content: str, file_path: str,
) -> List[SanitizationStep]:
    """Detect sanitization steps in code content."""
    steps = []
    content_lower = content.lower()
    for name, effectiveness in _SANITIZERS.items():
        if name.lower() in content_lower:
            steps.append(SanitizationStep(
                file=file_path, line=0, transform_name=name, effectiveness=effectiveness,
            ))
    return steps


def _calculate_verdict(
    source: Optional[DataFlowSource],
    sink: Optional[DataFlowSink],
    sanitization: List[SanitizationStep],
) -> tuple:
    """Calculate verdict and confidence from source, sink, and sanitization."""
    if not sink:
        return DataFlowVerdict.NEEDS_REVIEW, 0.3

    if not source:
        return DataFlowVerdict.NEEDS_REVIEW, 0.4

    # If there's effective sanitization, likely safe
    if sanitization:
        max_effectiveness = max(s.effectiveness for s in sanitization)
        if max_effectiveness >= 0.9:
            return DataFlowVerdict.SAFE, max_effectiveness
        elif max_effectiveness >= 0.6:
            return DataFlowVerdict.NEEDS_REVIEW, 0.5
        else:
            return DataFlowVerdict.VULNERABLE, 0.6

    # No sanitization: source directly reaches sink
    if sink.danger_level in (DangerLevel.CRITICAL, DangerLevel.HIGH):
        return DataFlowVerdict.VULNERABLE, 0.8
    else:
        return DataFlowVerdict.NEEDS_REVIEW, 0.5
