"""
Response Classifier - Deterministic vulnerability detection during test execution.

Classifies HTTP responses to determine if a payload successfully triggered
a vulnerability. Used by TestPlanExecutor to produce per-payload verdicts.
"""

from __future__ import annotations

import hashlib
import html as _html
import re
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from lib.signal_detector import ERROR_INDICATORS


# ---------------------------------------------------------------------------
# Classification result
# ---------------------------------------------------------------------------

@dataclass
class ClassificationResult:
    """Result of classifying a single response against a payload."""
    is_vulnerable: bool
    confidence: float  # 0.0 – 1.0
    severity: str  # info, low, medium, high, critical
    evidence: List[str] = field(default_factory=list)
    signal_type: str = ""  # error, timing, reflection, differential, content, header
    vuln_class: str = ""
    parameter: str = ""
    payload_used: str = ""


# ---------------------------------------------------------------------------
# SQL error patterns (extended from signal_detector)
# ---------------------------------------------------------------------------

_SQL_ERROR_PATTERNS: List[re.Pattern] = [
    re.compile(p, re.IGNORECASE) for p in [
        r"sql\s*syntax",
        r"mysql_fetch",
        r"ora-\d{5}",
        r"postgresql",
        r"sqlite3?\.\w+Error",
        r"jdbc\s*exception",
        r"odbc\s*(driver|error)",
        r"syntax error.*(?:near|at)",
        r"unclosed quotation",
        r"unterminated string",
        r"microsoft.*sql\s*server",
        r"sqlstate\[",
    ]
]

# ---------------------------------------------------------------------------
# XSS dangerous reflection patterns
# ---------------------------------------------------------------------------

_XSS_DANGEROUS_PATTERNS = [
    "<script", "onerror=", "onload=", "onclick=", "onmouseover=",
    "javascript:", "alert(", "prompt(", "confirm(",
    "<img src=", "<svg ", "<iframe ",
]

# ---------------------------------------------------------------------------
# SSTI computation markers
# ---------------------------------------------------------------------------

_SSTI_TEMPLATES = {
    "{{7*7}}": "49",
    "${7*7}": "49",
    "#{7*7}": "49",
    "{{7*'7'}}": "7777777",
    "<%= 7*7 %>": "49",
    "{{8*8}}": "64",
    "${8*8}": "64",
    "{{9*9}}": "81",
    "${9*9}": "81",
}

# SSTI error patterns (framework-specific error messages)
_SSTI_ERROR_PATTERNS: List[re.Pattern] = [
    re.compile(p, re.IGNORECASE) for p in [
        r"UndefinedError",
        r"TemplateSyntaxError",
        r"TemplateNotFound",
        r"freemarker\.core",
        r"jinja2\.exceptions",
        r"erb error",
        r"velocity\.exception",
    ]
]

# ---------------------------------------------------------------------------
# Path traversal file content patterns
# ---------------------------------------------------------------------------

_PATH_TRAVERSAL_PATTERNS: List[re.Pattern] = [
    re.compile(p, re.IGNORECASE) for p in [
        r"root:.*:0:0:",          # /etc/passwd
        r"\[extensions\]",        # win.ini
        r"\[fonts\]",             # win.ini
        r";\s*for 16-bit app",    # boot.ini
        r"\[boot\s*loader\]",     # boot.ini
        r"localhost",             # /etc/hosts common pattern
    ]
]

# ---------------------------------------------------------------------------
# SSRF internal/metadata content patterns
# ---------------------------------------------------------------------------

_SSRF_PATTERNS: List[re.Pattern] = [
    re.compile(p, re.IGNORECASE) for p in [
        r"ami-id",
        r"instance-id",
        r"iam/security-credentials",
        r"meta-data",
        r"computeMetadata",
        r"169\.254\.169\.254",
        r"metadata\.google\.internal",
    ]
]

# ---------------------------------------------------------------------------
# Command injection patterns
# ---------------------------------------------------------------------------

_CMDI_PATTERNS: List[re.Pattern] = [
    re.compile(p) for p in [
        r"uid=\d+",               # id command output
        r"drwx[r-][w-][x-]",     # ls -la output
        r"total\s+\d+\s",        # ls -la header
        r"root:x:0:0:",          # /etc/passwd read via cmdi
    ]
]


# ---------------------------------------------------------------------------
# ResponseClassifier
# ---------------------------------------------------------------------------

class ResponseClassifier:
    """Deterministic classifier for vulnerability detection per vuln class."""

    def classify(
        self,
        vuln_class: str,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Classify a single response against the baseline for a given vuln class.

        Parameters
        ----------
        vuln_class : str
            The vulnerability class being tested (sqli, xss, ssti, etc.).
        response : dict
            ``{status, headers, body, timing}`` of the test request.
        baseline : dict
            Fingerprint of the baseline response:
            ``{status, body_hash, body_length, timing_ms}``.
        payload : str
            The payload that was sent.
        parameter : str
            The parameter that was injected.

        Returns
        -------
        ClassificationResult
        """
        dispatcher = {
            # Legacy/grouped classes
            "sqli": self._classify_sqli,
            "xss": self._classify_xss,
            "ssti": self._classify_ssti,
            "path_traversal": self._classify_path_traversal,
            "ssrf": self._classify_ssrf,
            "injection": self._classify_injection,
            "overflow": self._classify_overflow,
            "type_confusion": self._classify_type_confusion,
            "info_disclosure": self._classify_info_disclosure,
            "misconfig": self._classify_misconfig,
            # Split SQL injection classes
            "sqli_error": self._classify_sqli,
            "sqli_blind_boolean": self._classify_sqli,
            "sqli_blind_time": self._classify_sqli,
            "sqli_union": self._classify_sqli,
            # Split XSS classes
            "xss_reflected": self._classify_xss,
            "xss_stored": self._classify_xss,
            "xss_dom": self._classify_xss,
            # New injection classes
            "command_injection": self._classify_cmdi,
            "nosql_injection": self._classify_nosql,
            "xml_injection": self._classify_xxe,
            "header_injection": self._classify_header_injection,
            # Auth/config classes
            "cors_misconfig": self._classify_cors,
            "csrf": self._classify_csrf,
            "cookie_security": self._classify_cookie_security,
            "jwt_manipulation": self._classify_jwt,
            "open_redirect": self._classify_open_redirect,
            "username_enumeration": self._classify_username_enum,
            # Batch B: Aliases to existing classifiers
            "info_leak_errors": self._classify_info_disclosure,
            "info_leak_comments": self._classify_info_disclosure,
            "info_leak_source_maps": self._classify_info_disclosure,
            "info_leak_headers": self._classify_info_disclosure,
            "clickjacking": self._classify_misconfig,
            "security_headers": self._classify_misconfig,
            "http_method_tampering": self._classify_misconfig,
            "tls_config": self._classify_misconfig,
            # Batch B: New classifier methods
            "auth_bypass": self._classify_auth_bypass,
            "broken_auth": self._classify_auth_bypass,
            "privilege_escalation": self._classify_auth_bypass,
            "idor": self._classify_idor,
            "session_fixation": self._classify_session,
            "session_expiry": self._classify_session,
            "directory_listing": self._classify_directory_listing,
            "ldap_injection": self._classify_ldap_injection,
            "mass_assignment": self._classify_mass_assignment,
            "graphql_introspection": self._classify_graphql_introspection,
            "param_pollution": self._classify_behavioral,
            "rate_limit_bypass": self._classify_behavioral,
            "race_condition": self._classify_behavioral,
            "price_manipulation": self._classify_behavioral,
        }

        classify_fn = dispatcher.get(vuln_class, self._classify_generic)
        result = classify_fn(response, baseline, payload, parameter)
        result.vuln_class = vuln_class
        result.parameter = parameter
        result.payload_used = payload
        return result

    # ------------------------------------------------------------------
    # Per-class classifiers
    # ------------------------------------------------------------------

    def _classify_sqli(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect SQL injection via error-based, time-based, and boolean-blind."""
        body = response.get("body") or ""
        body_lower = body.lower()
        evidence: List[str] = []
        confidence = 0.0

        # Error-based: check for SQL error patterns
        for pattern in _SQL_ERROR_PATTERNS:
            match = pattern.search(body_lower)
            if match:
                evidence.append(f"SQL error pattern matched: {match.group()}")
                confidence = max(confidence, 0.85)

        # Time-based: response significantly slower than baseline
        timing = response.get("timing") or {}
        resp_time = timing.get("duration_ms", 0)
        baseline_time = baseline.get("timing_ms", 0)

        if baseline_time > 0 and resp_time > 0:
            # Confirmed if > 3x baseline AND > 2500ms absolute
            if resp_time > (baseline_time * 3) and resp_time > 2500:
                evidence.append(
                    f"Timing anomaly: {resp_time:.0f}ms vs {baseline_time:.0f}ms baseline "
                    f"({resp_time / baseline_time:.1f}x)"
                )
                confidence = max(confidence, 0.8)
            # Suspicious if > 1.5x baseline but below threshold
            elif resp_time > (baseline_time * 1.5) and resp_time > 1000:
                evidence.append(
                    f"Slight timing increase: {resp_time:.0f}ms vs {baseline_time:.0f}ms baseline"
                )
                confidence = max(confidence, 0.4)

        # Boolean-blind: body hash differs for boolean payloads
        resp_hash = _hash_body(body)
        if baseline.get("body_hash") and resp_hash != baseline["body_hash"]:
            if any(kw in payload.lower() for kw in ("1=1", "1=2", "or 1", "and 1")):
                evidence.append("Response body changed for boolean SQL payload")
                confidence = max(confidence, 0.5)

        # Status code change to 500
        resp_status = response.get("status", 200)
        if resp_status == 500 and baseline.get("status", 200) != 500:
            evidence.append(f"Status changed to 500 (baseline: {baseline.get('status', 200)})")
            confidence = max(confidence, 0.45)

        is_vulnerable = confidence >= 0.6
        severity = "high" if is_vulnerable else "medium" if confidence >= 0.3 else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="error" if any("SQL error" in e for e in evidence) else "timing",
        )

    def _classify_xss(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect XSS via unencoded payload reflection."""
        body = response.get("body") or ""
        evidence: List[str] = []
        confidence = 0.0

        if payload and payload in body:
            # Check if dangerous patterns are reflected
            payload_lower = payload.lower()
            body_lower = body.lower()

            for pattern in _XSS_DANGEROUS_PATTERNS:
                if pattern.lower() in payload_lower and pattern.lower() in body_lower:
                    evidence.append(f"Payload reflected verbatim with dangerous pattern: {pattern}")
                    confidence = max(confidence, 0.9)
                    break

            if not evidence and payload in body:
                evidence.append("Payload reflected in response body")
                confidence = max(confidence, 0.5)

        # Check HTML-entity-encoded reflection (context-dependent XSS)
        elif payload and _html.escape(payload) in body:
            evidence.append("Payload reflected with HTML encoding (context-dependent XSS)")
            confidence = max(confidence, 0.3)

        is_vulnerable = confidence >= 0.6
        severity = "high" if is_vulnerable else "low"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="reflection",
        )

    def _classify_ssti(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect SSTI via computation marker presence, error patterns, or RCE indicators."""
        body = response.get("body") or ""
        evidence: List[str] = []
        confidence = 0.0

        # Check if template computation result appears in response
        expected = _SSTI_TEMPLATES.get(payload)
        if expected and expected in body:
            # Verify it wasn't already in the baseline
            baseline_body_has = False
            if baseline.get("body_hash"):
                # We can't check baseline body directly; use heuristic:
                # if baseline body_length is similar and status same, assume it's new
                pass
            evidence.append(f"Template computation result '{expected}' found in response for payload '{payload}'")
            confidence = 0.85

        # Check for template engine error messages
        if confidence < 0.6:
            for pattern in _SSTI_ERROR_PATTERNS:
                if pattern.search(body):
                    evidence.append(f"Template engine error detected: {pattern.pattern}")
                    confidence = max(confidence, 0.65)

        # Check for RCE indicators (uid=, root:x:, etc.)
        if confidence < 0.6:
            for pattern in _CMDI_PATTERNS:
                if pattern.search(body):
                    evidence.append(f"Command execution output detected via SSTI: {pattern.pattern}")
                    confidence = 0.95
                    break

        is_vulnerable = confidence >= 0.6
        severity = "critical" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="content",
        )

    def _classify_path_traversal(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect path traversal via file content patterns."""
        body = response.get("body") or ""
        evidence: List[str] = []
        confidence = 0.0

        for pattern in _PATH_TRAVERSAL_PATTERNS:
            match = pattern.search(body)
            if match:
                evidence.append(f"File content pattern detected: {match.group()}")
                confidence = max(confidence, 0.85)

        is_vulnerable = confidence >= 0.6
        severity = "high" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="content",
        )

    def _classify_ssrf(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect SSRF via internal/cloud metadata content."""
        body = response.get("body") or ""
        evidence: List[str] = []
        confidence = 0.0

        for pattern in _SSRF_PATTERNS:
            match = pattern.search(body)
            if match:
                evidence.append(f"Internal/metadata content detected: {match.group()}")
                confidence = max(confidence, 0.8)

        is_vulnerable = confidence >= 0.6
        severity = "high" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="content",
        )

    def _classify_injection(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Union of SQL + XSS + command injection patterns."""
        # Try SQL first
        sqli_result = self._classify_sqli(response, baseline, payload, parameter)
        if sqli_result.is_vulnerable:
            sqli_result.signal_type = "error"
            return sqli_result

        # Try XSS
        xss_result = self._classify_xss(response, baseline, payload, parameter)
        if xss_result.is_vulnerable:
            return xss_result

        # Command injection patterns
        body = response.get("body") or ""
        evidence: List[str] = []
        confidence = 0.0

        for pattern in _CMDI_PATTERNS:
            match = pattern.search(body)
            if match:
                evidence.append(f"Command injection output detected: {match.group()}")
                confidence = max(confidence, 0.8)

        if confidence >= 0.6:
            return ClassificationResult(
                is_vulnerable=True,
                confidence=confidence,
                severity="critical",
                evidence=evidence,
                signal_type="content",
            )

        # Return the highest-confidence non-vulnerable result
        best = max([sqli_result, xss_result], key=lambda r: r.confidence)
        return ClassificationResult(
            is_vulnerable=False,
            confidence=best.confidence,
            severity=best.severity,
            evidence=best.evidence,
            signal_type=best.signal_type,
        )

    def _classify_overflow(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect overflow via status 500 on large payload when baseline was 200."""
        resp_status = response.get("status", 200)
        base_status = baseline.get("status", 200)
        evidence: List[str] = []
        confidence = 0.0

        if resp_status == 500 and base_status == 200:
            evidence.append(f"Server error (500) triggered by oversized input (baseline: {base_status})")
            confidence = 0.7

        # Also check for significant body length change
        body_len = len(response.get("body") or "")
        baseline_len = baseline.get("body_length", 0) or 1
        if body_len < baseline_len * 0.3 and resp_status >= 400:
            evidence.append(f"Response truncated: {body_len} bytes vs {baseline_len} baseline")
            confidence = max(confidence, 0.5)

        is_vulnerable = confidence >= 0.6
        severity = "medium" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="differential",
        )

    def _classify_type_confusion(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect type confusion via differential response analysis."""
        body = response.get("body") or ""
        resp_status = response.get("status", 200)
        base_status = baseline.get("status", 200)
        evidence: List[str] = []
        confidence = 0.0

        # Status code changed
        if resp_status != base_status:
            evidence.append(f"Status changed: {base_status} -> {resp_status}")
            confidence = max(confidence, 0.5)

        # Body hash differs significantly
        resp_hash = _hash_body(body)
        if baseline.get("body_hash") and resp_hash != baseline["body_hash"]:
            body_len = len(body)
            baseline_len = baseline.get("body_length", 0) or 1
            ratio = body_len / baseline_len
            if ratio < 0.5 or ratio > 2.0:
                evidence.append(f"Significant body change: {body_len} vs {baseline_len} bytes")
                confidence = max(confidence, 0.6)

        is_vulnerable = confidence >= 0.6
        severity = "medium" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="differential",
        )

    def _classify_info_disclosure(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect information disclosure via server headers, debug info, stack traces."""
        headers = response.get("headers") or {}
        body_lower = (response.get("body") or "").lower()
        evidence: List[str] = []
        confidence = 0.0

        # Server version header
        server = ""
        for k, v in headers.items():
            if k.lower() == "server" and "/" in v:
                server = v
                evidence.append(f"Server version disclosed: {v}")
                confidence = max(confidence, 0.7)
            if k.lower() == "x-powered-by":
                evidence.append(f"X-Powered-By header: {v}")
                confidence = max(confidence, 0.7)

        # Debug info in body
        for indicator in ERROR_INDICATORS.get("debug", []):
            if indicator in body_lower:
                evidence.append(f"Debug information detected: {indicator}")
                confidence = max(confidence, 0.7)
                break

        # Stack traces
        for indicator in ERROR_INDICATORS.get("stackTrace", []):
            if indicator in body_lower:
                evidence.append(f"Stack trace detected: {indicator}")
                confidence = max(confidence, 0.75)
                break

        # Internal paths
        for indicator in ERROR_INDICATORS.get("path", []):
            if indicator in body_lower:
                evidence.append(f"Internal path disclosed: {indicator}")
                confidence = max(confidence, 0.65)
                break

        is_vulnerable = confidence >= 0.6
        severity = "medium" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="header" if server else "content",
        )

    def _classify_misconfig(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect misconfigurations via CORS, missing security headers, OPTIONS."""
        headers = response.get("headers") or {}
        lower_headers = {k.lower(): v for k, v in headers.items()}
        evidence: List[str] = []
        confidence = 0.0

        # Wildcard CORS
        acao = lower_headers.get("access-control-allow-origin", "")
        if acao == "*":
            evidence.append("Access-Control-Allow-Origin: * (wildcard CORS)")
            confidence = max(confidence, 0.75)

        # Missing security headers
        from lib.response_analyzer import _SECURITY_HEADERS
        missing_count = 0
        for header in _SECURITY_HEADERS:
            if header.lower() not in lower_headers:
                missing_count += 1
        if missing_count >= 4:
            evidence.append(f"{missing_count} security headers missing")
            confidence = max(confidence, 0.65)

        # OPTIONS reveals unexpected methods
        resp_status = response.get("status", 200)
        allow = lower_headers.get("allow", lower_headers.get("access-control-allow-methods", ""))
        if allow and resp_status < 400:
            methods = [m.strip().upper() for m in allow.split(",")]
            dangerous = {"DELETE", "PUT", "PATCH", "TRACE"}
            found_dangerous = dangerous & set(methods)
            if found_dangerous:
                evidence.append(f"Unexpected HTTP methods allowed: {', '.join(found_dangerous)}")
                confidence = max(confidence, 0.7)

        is_vulnerable = confidence >= 0.6
        severity = "medium" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="header",
        )

    def _classify_cmdi(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect OS command injection."""
        body = response.get("body") or ""
        evidence: List[str] = []
        confidence = 0.0

        # Command output patterns
        cmdi_patterns = [
            (r"uid=\d+.*gid=\d+", "Unix id command output", 0.9),
            (r"root:x:0:0:", "Unix /etc/passwd content", 0.95),
            (r"Microsoft Windows \[Version", "Windows command output", 0.9),
            (r"Directory of [A-Z]:\\", "Windows dir command", 0.9),
            (r"Volume Serial Number", "Windows volume info", 0.85),
        ]

        for pattern, desc, conf in cmdi_patterns:
            if re.search(pattern, body, re.IGNORECASE):
                evidence.append(f"{desc} detected")
                confidence = max(confidence, conf)

        # Timing-based (for blind command injection)
        timing = response.get("timing") or {}
        resp_time = timing.get("duration_ms", 0)
        baseline_time = baseline.get("timing_ms", 0)

        if "sleep" in payload.lower() or "timeout" in payload.lower():
            if resp_time > 4000 and resp_time > (baseline_time * 2):
                evidence.append(f"Timing delay: {resp_time:.0f}ms (expected ~5s for sleep payload)")
                confidence = max(confidence, 0.75)

        is_vulnerable = confidence >= 0.6
        severity = "critical" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="content" if evidence else "timing",
        )

    def _classify_nosql(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect NoSQL injection."""
        body = response.get("body") or ""
        evidence: List[str] = []
        confidence = 0.0

        # NoSQL error patterns
        nosql_errors = [
            (r"MongoError", "MongoDB error"),
            (r"CastError.*ObjectId", "MongoDB ObjectId error"),
            (r"mongodb.*exception", "MongoDB exception"),
            (r"\$where.*function", "MongoDB $where syntax"),
        ]

        for pattern, desc in nosql_errors:
            if re.search(pattern, body, re.IGNORECASE):
                evidence.append(f"{desc} detected")
                confidence = max(confidence, 0.85)

        # Authentication bypass indicators
        resp_status = response.get("status", 200)
        if "$ne" in payload or "$gt" in payload:
            if resp_status == 200 and baseline.get("status") in (401, 403):
                evidence.append("Auth bypass: 401/403 baseline -> 200 with NoSQL operator")
                confidence = max(confidence, 0.9)

        is_vulnerable = confidence >= 0.6
        severity = "high" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="error",
        )

    def _classify_xxe(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect XXE/XML injection."""
        body = response.get("body") or ""
        evidence: List[str] = []
        confidence = 0.0

        # File content from XXE
        xxe_indicators = [
            (r"root:x:0:0:", "/etc/passwd read via XXE", 0.95),
            (r"\[fonts\]", "win.ini read via XXE", 0.95),
            (r"ami-id|instance-id", "AWS metadata via XXE", 0.9),
        ]

        for pattern, desc, conf in xxe_indicators:
            if re.search(pattern, body, re.IGNORECASE):
                evidence.append(desc)
                confidence = max(confidence, conf)

        # XML parsing errors
        xml_errors = [
            r"XML.*(?:parse|parsing).*error",
            r"SAXParseException",
            r"DOCTYPE.*not allowed",
            r"ENTITY.*declaration",
        ]

        for pattern in xml_errors:
            if re.search(pattern, body, re.IGNORECASE):
                evidence.append("XML parsing error detected")
                confidence = max(confidence, 0.5)

        is_vulnerable = confidence >= 0.6
        severity = "high" if is_vulnerable else "medium"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="content",
        )

    def _classify_header_injection(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect HTTP header injection (CRLF)."""
        headers = response.get("headers") or {}
        evidence: List[str] = []
        confidence = 0.0

        # Check for injected headers
        if "X-Injected-Header" in headers:
            evidence.append("Injected custom header detected")
            confidence = 0.95

        # Check Set-Cookie injection
        set_cookie = headers.get("Set-Cookie", "")
        if "injected" in set_cookie.lower():
            evidence.append("Injected cookie detected")
            confidence = 0.95

        # Check for response splitting
        body = response.get("body") or ""
        if "\r\n\r\n" in payload and "<script>" in body:
            evidence.append("Response splitting with XSS payload")
            confidence = 0.9

        is_vulnerable = confidence >= 0.6
        severity = "medium" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="header",
        )

    def _classify_cors(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect CORS misconfiguration."""
        headers = {k.lower(): v for k, v in response.get("headers", {}).items()}
        evidence: List[str] = []
        confidence = 0.0

        acao = headers.get("access-control-allow-origin", "")
        acac = headers.get("access-control-allow-credentials", "")

        # Wildcard CORS
        if acao == "*":
            evidence.append("Wildcard CORS: Access-Control-Allow-Origin: *")
            confidence = 0.7

        # Null origin allowed
        if acao == "null":
            evidence.append("Null origin allowed")
            confidence = 0.8

        # Reflected origin (check if payload origin is reflected)
        if "evil.com" in payload and "evil.com" in acao:
            evidence.append("Arbitrary origin reflected")
            confidence = 0.85

        # Wildcard + credentials (critical)
        if acao == "*" and acac.lower() == "true":
            evidence.append("Critical: Wildcard CORS with credentials enabled")
            confidence = 1.0

        # Check Access-Control-Expose-Headers for sensitive header leakage
        aceh = headers.get("access-control-expose-headers", "")
        if aceh:
            _SENSITIVE_HEADERS = {"authorization", "x-api-key", "set-cookie", "cookie", "x-csrf-token"}
            exposed = {h.strip().lower() for h in aceh.split(",")}
            leaked = _SENSITIVE_HEADERS & exposed
            if leaked:
                evidence.append(f"Sensitive headers exposed via CORS: {', '.join(sorted(leaked))}")
                confidence = max(confidence, 0.75)

        is_vulnerable = confidence >= 0.6
        severity = "high" if confidence >= 0.8 else "medium"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="header",
        )

    def _classify_csrf(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect missing CSRF protection via differential analysis."""
        resp_status = response.get("status", 200)
        baseline_status = baseline.get("status", 0)
        evidence: List[str] = []
        confidence = 0.0

        if resp_status in (200, 201, 204):
            if baseline_status not in (200, 201, 204) and resp_status in (200, 201, 204):
                # Baseline fails but token-less request succeeds — strong signal
                evidence.append("State-changing succeeds without token but baseline required it")
                confidence = 0.8
            else:
                # Generic success — not enough evidence on its own
                evidence.append("State-changing operation returned success status")
                confidence = 0.3  # Below 0.6 threshold — NOT marked vulnerable

        is_vulnerable = confidence >= 0.6
        severity = "medium" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="differential",
        )

    def _classify_cookie_security(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect insecure cookie attributes (per-cookie analysis)."""
        headers = response.get("headers") or {}
        # Support multi-cookie: prefer Set-Cookie-All list, fall back to single
        cookies = headers.get("Set-Cookie-All", [])
        if not cookies:
            single = headers.get("Set-Cookie", "")
            cookies = [single] if single else []

        evidence: List[str] = []
        confidence = 0.0

        for cookie in cookies:
            name = cookie.split("=")[0].strip() if "=" in cookie else "unknown"
            cl = cookie.lower()
            if "httponly" not in cl:
                evidence.append(f"Cookie '{name}' missing HttpOnly flag")
                confidence = max(confidence, 0.6)
            if "secure" not in cl:
                evidence.append(f"Cookie '{name}' missing Secure flag")
                confidence = max(confidence, 0.6)
            if "samesite" not in cl:
                evidence.append(f"Cookie '{name}' missing SameSite attribute")
                confidence = max(confidence, 0.5)

        is_vulnerable = confidence >= 0.6  # Bug 16 threshold
        severity = "low" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="header",
        )

    def _classify_jwt(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect JWT vulnerabilities."""
        import base64
        import json as json_module

        resp_status = response.get("status", 200)
        evidence: List[str] = []
        confidence = 0.0

        # Decode JWT header to check alg field
        parts = payload.split(".")
        if len(parts) >= 2:
            try:
                # Base64 decode the header segment
                header_b64 = parts[0]
                # Add padding if needed
                padding = len(header_b64) % 4
                if padding:
                    header_b64 += "=" * (4 - padding)
                header_json = base64.urlsafe_b64decode(header_b64).decode()
                header = json_module.loads(header_json)

                # Algorithm:none accepted
                if header.get("alg", "").lower() == "none":
                    if resp_status == 200:
                        evidence.append("Algorithm 'none' accepted - signature verification bypassed")
                        confidence = 1.0
            except Exception:
                # Fallback to plaintext check for non-JWT payloads
                if "alg.*none" in payload.lower() or '"alg":"none"' in payload.lower():
                    if resp_status == 200:
                        evidence.append("Algorithm 'none' accepted - signature verification bypassed")
                        confidence = 1.0

        # Signature stripping: check for <=2 parts (header.payload or header.payload.)
        if len(parts) <= 2 or (len(parts) == 3 and parts[2] == ""):
            if resp_status == 200:
                evidence.append("JWT accepted without signature")
                confidence = 0.95

        # Modified claims accepted
        if resp_status == 200 and baseline.get("status") in (401, 403):
            if "admin" in payload.lower() or "role" in payload.lower():
                evidence.append("Modified JWT claims accepted (potential privilege escalation)")
                confidence = 0.9

        is_vulnerable = confidence >= 0.6
        severity = "critical" if confidence >= 0.9 else "high"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="differential",
        )

    def _classify_open_redirect(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect open redirect vulnerabilities."""
        resp_status = response.get("status", 200)
        headers = response.get("headers") or {}
        location = headers.get("Location", "")
        evidence: List[str] = []
        confidence = 0.0

        # Check for redirect to external domain
        if resp_status in (301, 302, 303, 307, 308):
            if "evil.com" in location or "attacker.com" in location:
                evidence.append(f"Redirect to untrusted domain: {location}")
                confidence = 0.95

            # Check for payload reflection in Location header
            if payload in location:
                evidence.append(f"Payload reflected in Location header: {location}")
                confidence = max(confidence, 0.85)

        is_vulnerable = confidence >= 0.6
        severity = "medium" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="header",
        )

    def _classify_username_enum(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect username enumeration via timing or error differences."""
        body = response.get("body") or ""
        timing = response.get("timing") or {}
        resp_time = timing.get("duration_ms", 0)
        baseline_time = baseline.get("timing_ms", 0)
        evidence: List[str] = []
        confidence = 0.0

        # Error message differences
        if "user not found" in body.lower() or "invalid username" in body.lower():
            evidence.append("Verbose error message discloses username validity")
            confidence = 0.8
        elif "incorrect password" in body.lower() or "wrong password" in body.lower():
            evidence.append("Error message confirms username exists")
            confidence = 0.75

        # Timing differences
        if baseline_time > 0 and resp_time > 0:
            time_diff = abs(resp_time - baseline_time)
            if time_diff > 200:  # 200ms difference
                evidence.append(f"Timing difference: {time_diff:.0f}ms (may indicate username validity)")
                confidence = max(confidence, 0.6)

        is_vulnerable = confidence >= 0.6
        severity = "low" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="timing" if "Timing" in str(evidence) else "content",
        )

    def _classify_auth_bypass(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect auth bypass, broken auth, privilege escalation."""
        resp_status = response.get("status", 200)
        baseline_status = baseline.get("status", 200)
        body = response.get("body") or ""
        body_lower = body.lower()
        evidence: List[str] = []
        confidence = 0.0

        # Response 200 when baseline was 401/403
        if resp_status == 200 and baseline_status in (401, 403):
            evidence.append(f"Auth bypass: {baseline_status} baseline -> 200 with payload")
            confidence = 0.9

        # Admin/privilege keywords in body
        admin_keywords = ["admin", "administrator", "privilege", "elevated", "superuser"]
        if any(kw in body_lower for kw in admin_keywords):
            evidence.append("Admin/privilege keywords detected in response")
            confidence = max(confidence, 0.7)

        is_vulnerable = confidence >= 0.6
        severity = "critical" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="differential",
        )

    def _classify_idor(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect IDOR vulnerabilities."""
        body = response.get("body") or ""
        resp_status = response.get("status", 200)
        baseline_status = baseline.get("status", 200)
        evidence: List[str] = []
        confidence = 0.0

        # Body hash differs from baseline for different resource ID
        resp_hash = _hash_body(body)
        if baseline.get("body_hash") and resp_hash != baseline["body_hash"]:
            evidence.append("Response body changed for different resource ID")
            confidence = 0.6

        # 200 when baseline was 403
        if resp_status == 200 and baseline_status == 403:
            evidence.append("Unauthorized access: 403 baseline -> 200 with different ID")
            confidence = max(confidence, 0.85)

        is_vulnerable = confidence >= 0.6
        severity = "high" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="differential",
        )

    def _classify_session(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect session fixation and session expiry issues."""
        headers = response.get("headers") or {}
        resp_status = response.get("status", 200)
        baseline_status = baseline.get("status", 200)
        evidence: List[str] = []
        confidence = 0.0

        # Payload session ID reflected in Set-Cookie
        set_cookie = headers.get("Set-Cookie", "")
        if payload and payload in set_cookie:
            evidence.append("Session ID from payload reflected in Set-Cookie (fixation)")
            confidence = 0.8

        # Expired token still gets 200
        if "expired" in payload.lower() or "old" in payload.lower():
            if resp_status == 200 and baseline_status == 200:
                evidence.append("Expired token still accepted")
                confidence = max(confidence, 0.7)

        is_vulnerable = confidence >= 0.6
        severity = "medium" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="header",
        )

    def _classify_directory_listing(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect directory listing enabled."""
        body = response.get("body") or ""
        evidence: List[str] = []
        confidence = 0.0

        # Directory listing patterns
        patterns = [
            (r"Index of /", "Apache-style directory listing"),
            (r"<DIR>", "IIS-style directory listing"),
            (r"Parent Directory", "Directory navigation"),
            (r"Last modified", "Directory listing headers"),
        ]

        for pattern, desc in patterns:
            if re.search(pattern, body, re.IGNORECASE):
                evidence.append(desc)
                confidence = 0.8
                break

        is_vulnerable = confidence >= 0.6
        severity = "low" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="content",
        )

    def _classify_ldap_injection(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect LDAP injection."""
        body = response.get("body") or ""
        evidence: List[str] = []
        confidence = 0.0

        # LDAP error patterns
        patterns = [
            (r"javax\.naming", "JNDI naming error"),
            (r"LDAPException", "LDAP exception"),
            (r"Invalid DN syntax", "LDAP DN syntax error"),
            (r"ldap_search", "LDAP search function"),
            (r"Bad search filter", "LDAP filter error"),
        ]

        for pattern, desc in patterns:
            if re.search(pattern, body, re.IGNORECASE):
                evidence.append(desc)
                confidence = 0.85
                break

        is_vulnerable = confidence >= 0.6
        severity = "high" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="error",
        )

    def _classify_mass_assignment(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect mass assignment vulnerabilities."""
        body = response.get("body") or ""
        body_lower = body.lower()
        evidence: List[str] = []
        confidence = 0.0

        # Extra role/admin fields reflected in response
        mass_assign_fields = ["role", "admin", "isadmin", "is_admin", "privilege"]
        if any(field in body_lower for field in mass_assign_fields):
            if any(field in payload.lower() for field in mass_assign_fields):
                evidence.append("Mass assignment field reflected in response")
                confidence = 0.7

        # Body hash changed with extra fields
        resp_hash = _hash_body(body)
        if baseline.get("body_hash") and resp_hash != baseline["body_hash"]:
            if "role" in payload.lower() or "admin" in payload.lower():
                evidence.append("Response changed after adding privilege fields")
                confidence = max(confidence, 0.75)

        is_vulnerable = confidence >= 0.6
        severity = "high" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="differential",
        )

    def _classify_graphql_introspection(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect GraphQL introspection enabled."""
        body = response.get("body") or ""
        resp_status = response.get("status", 200)
        evidence: List[str] = []
        confidence = 0.0

        # __schema or __type+fields in 200 response body
        if resp_status == 200:
            if "__schema" in body:
                evidence.append("GraphQL __schema introspection exposed")
                confidence = 0.9
            elif "__type" in body and "fields" in body:
                evidence.append("GraphQL __type introspection exposed")
                confidence = 0.85

        is_vulnerable = confidence >= 0.6
        severity = "medium" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="content",
        )

    def _classify_behavioral(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Detect behavioral vulnerabilities (param pollution, rate limit bypass, race condition, price manipulation)."""
        resp_status = response.get("status", 200)
        baseline_status = baseline.get("status", 200)
        body = response.get("body") or ""
        baseline_body_len = baseline.get("body_length", 0) or 1
        evidence: List[str] = []
        confidence = 0.0

        # Status change differential
        if resp_status != baseline_status:
            evidence.append(f"Status changed: {baseline_status} -> {resp_status}")
            confidence = 0.5

        # Body size ratio >2x or <0.5x
        body_len = len(body)
        if baseline_body_len > 0:
            ratio = body_len / baseline_body_len
            if ratio > 2.0 or ratio < 0.5:
                evidence.append(f"Significant body size change: {body_len} vs {baseline_body_len} bytes")
                confidence = max(confidence, 0.6)

        # 200 when baseline was 429/403 (rate limit bypass)
        if resp_status == 200 and baseline_status in (429, 403):
            evidence.append("Rate limit bypass: 429/403 baseline -> 200")
            confidence = max(confidence, 0.75)

        is_vulnerable = confidence >= 0.6
        severity = "medium" if is_vulnerable else "info"
        return ClassificationResult(
            is_vulnerable=is_vulnerable,
            confidence=confidence,
            severity=severity,
            evidence=evidence,
            signal_type="differential",
        )

    def _classify_generic(
        self,
        response: Dict[str, Any],
        baseline: Dict[str, Any],
        payload: str,
        parameter: str,
    ) -> ClassificationResult:
        """Fallback classifier: differential analysis."""
        resp_status = response.get("status", 200)
        base_status = baseline.get("status", 200)
        evidence: List[str] = []
        confidence = 0.0

        if resp_status != base_status:
            evidence.append(f"Status changed: {base_status} -> {resp_status}")
            confidence = 0.3

        return ClassificationResult(
            is_vulnerable=False,
            confidence=confidence,
            severity="info",
            evidence=evidence,
            signal_type="differential",
        )


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _hash_body(body: str) -> str:
    """SHA-256 hash of body, truncated to 16 hex chars."""
    return hashlib.sha256(body.encode("utf-8", errors="replace")).hexdigest()[:16]
