"""
Code Indexer

Indexes source code files into wm_knowledge for RAG-based semantic search.
Enables the LLM to query code by meaning, not just grep patterns.
"""
from __future__ import annotations

import asyncio
import hashlib
import os
import re
import time
from typing import Any, Dict, List, Optional

from lib.embedder import get_embedder
from lib.world_model_db import WorldModelDatabase


class CodeIndexer:
    """Index source code into wm_knowledge for RAG-based code search."""

    # File extensions to index
    CODE_EXTENSIONS = {
        ".py", ".js", ".ts", ".jsx", ".tsx",
        ".java", ".go", ".rb", ".php",
        ".c", ".cpp", ".cs", ".rs", ".swift", ".kt",
        ".html", ".css", ".scss", ".yaml", ".yml", ".json", ".xml",
    }

    # Directories to exclude (vendor code, dependencies)
    EXCLUDE_PATTERNS = [
        ".git",
        "node_modules",
        "vendor",
        "__pycache__",
        ".venv",
        "venv",
        ".pytest_cache",
        "build",
        "dist",
        ".next",
        ".nuxt",
        "target",  # Rust
        "bin",
        "obj",  # C#
    ]

    def __init__(self, db: WorldModelDatabase):
        """Initialize code indexer.

        Parameters
        ----------
        db : WorldModelDatabase
            World model database for storing indexed code
        """
        self.db = db
        self.embedder = get_embedder()

    async def index_repo(
        self,
        repo_path: str,
        sast_runner: Any,
        languages: Optional[List[str]] = None,
        exclude_patterns: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """Index all source files into wm_knowledge.

        Strategy:
        - List all source files (exclude vendor, node_modules, .git, etc.)
        - For each file: read content, detect language, generate embedding
        - Store as wm_knowledge(category="source_code") with metadata
        - Tags: [language, risk_level based on file type]

        Parameters
        ----------
        repo_path : str
            Path to cloned repository
        sast_runner : SASTRunner
            SAST runner instance for file operations
        languages : list of str, optional
            Language filter (only index these languages)
        exclude_patterns : list of str, optional
            Additional patterns to exclude

        Returns
        -------
        dict
            {files_indexed, total_size_kb, chunks_created, languages, index_time_ms}
        """
        start_time = time.monotonic()

        exclude_list = self.EXCLUDE_PATTERNS + (exclude_patterns or [])

        # Build find command (exclude patterns)
        find_args = [repo_path, "-type", "f"]
        for pattern in exclude_list:
            find_args.extend(["-not", "-path", f"*/{pattern}/*"])

        # Execute find
        result = await sast_runner._exec("find", find_args, timeout=60)
        if result["exit_code"] != 0:
            return {
                "files_indexed": 0,
                "total_size_kb": 0,
                "chunks_created": 0,
                "languages": [],
                "index_time_ms": int((time.monotonic() - start_time) * 1000),
                "error": "Failed to list files",
            }

        all_files = result["stdout"].splitlines()

        # Filter by extension
        source_files = [
            f for f in all_files
            if any(f.endswith(ext) for ext in self.CODE_EXTENSIONS)
        ]

        # Language filter
        if languages:
            lang_exts = {
                "python": [".py"],
                "javascript": [".js", ".jsx"],
                "typescript": [".ts", ".tsx"],
                "java": [".java"],
                "go": [".go"],
                "ruby": [".rb"],
                "php": [".php"],
                "c": [".c"],
                "cpp": [".cpp"],
                "csharp": [".cs"],
                "rust": [".rs"],
                "swift": [".swift"],
                "kotlin": [".kt"],
            }
            allowed_exts = []
            for lang in languages:
                allowed_exts.extend(lang_exts.get(lang.lower(), []))

            source_files = [f for f in source_files if any(f.endswith(ext) for ext in allowed_exts)]

        # Index files
        indexed_count = 0
        total_size = 0
        chunks_created = 0
        detected_languages = set()

        for file_path in source_files:
            # Read file content
            cat_result = await sast_runner._exec("cat", [file_path], timeout=10)
            if cat_result["exit_code"] != 0:
                continue

            content = cat_result["stdout"]
            if not content.strip():
                continue

            # Detect language
            language = self._detect_language(file_path)
            detected_languages.add(language)

            # Index file
            try:
                await self.index_file(repo_path, file_path, content, language)
                indexed_count += 1
                total_size += len(content)
                chunks_created += 1  # Simplified: 1 chunk per file
            except Exception as exc:
                # Log error but continue indexing
                print(f"Failed to index {file_path}: {exc}")
                continue

        return {
            "files_indexed": indexed_count,
            "total_size_kb": total_size // 1024,
            "chunks_created": chunks_created,
            "languages": sorted(list(detected_languages)),
            "index_time_ms": int((time.monotonic() - start_time) * 1000),
        }

    async def index_file(
        self,
        repo_path: str,
        file_path: str,
        content: str,
        language: str,
    ) -> str:
        """Index single file into wm_knowledge.

        - title: file_path (relative)
        - target: repo_path
        - source_tool: "sast_indexer"
        - category: "source_code"
        - content: file source code
        - embedding: generated from first 512 chars (function signatures, imports)
        - tags: [language, risk_category]
        - metadata: {language, line_count, file_size, priority_score,
                     functions_defined (basic regex), imports}

        Parameters
        ----------
        repo_path : str
            Repository root path
        file_path : str
            Absolute file path
        content : str
            File content
        language : str
            Detected language

        Returns
        -------
        str
            Knowledge entry ID
        """
        # Make path relative to repo
        if file_path.startswith(repo_path):
            relative_path = file_path[len(repo_path):].lstrip("/")
        else:
            relative_path = file_path

        # Extract metadata
        metadata = self._extract_metadata(content, language)
        metadata["language"] = language
        metadata["file_size"] = len(content)
        metadata["line_count"] = content.count("\n") + 1

        # Calculate priority score
        priority_score, priority_reasons = self._calculate_file_priority(relative_path)
        metadata["priority_score"] = priority_score

        # Classify risk level
        risk_level = self._classify_risk(relative_path, content)

        # Generate embedding from head (function signatures, imports)
        head_content = content[:512]
        embedding = None
        if self.embedder.available:
            embedding = self.embedder.embed(head_content)

        # Store in world model
        entry_id = await self.db.store_knowledge(
            title=relative_path,
            content=content,
            target=repo_path,
            source_tool="sast_indexer",
            category="source_code",
            tags=[language, risk_level],
            metadata=metadata,
            embedding=embedding,
        )

        return entry_id

    def _detect_language(self, file_path: str) -> str:
        """Detect language from file extension."""
        ext_map = {
            ".py": "python",
            ".js": "javascript",
            ".ts": "typescript",
            ".jsx": "javascript",
            ".tsx": "typescript",
            ".java": "java",
            ".go": "go",
            ".rb": "ruby",
            ".php": "php",
            ".c": "c",
            ".cpp": "cpp",
            ".cs": "csharp",
            ".rs": "rust",
            ".swift": "swift",
            ".kt": "kotlin",
            ".html": "html",
            ".css": "css",
            ".scss": "scss",
            ".yaml": "yaml",
            ".yml": "yaml",
            ".json": "json",
            ".xml": "xml",
        }

        for ext, lang in ext_map.items():
            if file_path.endswith(ext):
                return lang

        return "unknown"

    def _classify_risk(self, file_path: str, content: str) -> str:
        """Classify file risk level by path and content patterns.

        Returns: "critical" | "high" | "medium" | "low"
        """
        path_lower = file_path.lower()
        content_lower = content.lower()

        # Critical: auth, crypto, payment
        if any(keyword in path_lower for keyword in ["auth", "login", "password", "crypto", "payment", "billing"]):
            return "critical"
        if any(keyword in content_lower for keyword in ["password", "secret", "token", "api_key"]):
            return "critical"

        # High: database, API routes, session
        if any(keyword in path_lower for keyword in ["database", "db", "api", "route", "session", "middleware"]):
            return "high"
        if any(keyword in content_lower for keyword in ["select ", "insert ", "update ", "delete ", "query"]):
            return "high"

        # Medium: config, validators
        if any(keyword in path_lower for keyword in ["config", "settings", "validator", "middleware"]):
            return "medium"

        # Low: tests, docs, static
        if any(keyword in path_lower for keyword in ["test", "spec", "doc", "readme", "static", "public"]):
            return "low"

        return "medium"

    def _extract_metadata(self, content: str, language: str) -> Dict[str, Any]:
        """Extract basic metadata from source code.

        - function/method names (regex: def/function/public patterns)
        - import statements
        - Suspicious patterns: eval(), exec(), SQL string concat, etc.

        Returns: {functions: [...], imports: [...], suspicious_patterns: [...]}
        """
        metadata: Dict[str, Any] = {
            "functions": [],
            "imports": [],
            "suspicious_patterns": [],
        }

        # Extract functions (basic regex for common patterns)
        if language == "python":
            func_pattern = r"def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\("
            metadata["functions"] = re.findall(func_pattern, content)

            import_pattern = r"(?:from\s+[\w.]+\s+)?import\s+([\w.,\s]+)"
            metadata["imports"] = re.findall(import_pattern, content)

            # Suspicious patterns
            if "eval(" in content:
                metadata["suspicious_patterns"].append("eval()")
            if "exec(" in content:
                metadata["suspicious_patterns"].append("exec()")
            if re.search(r"execute\s*\(\s*['\"]", content):
                metadata["suspicious_patterns"].append("SQL string concat")

        elif language in ["javascript", "typescript"]:
            func_pattern = r"(?:function\s+|const\s+|let\s+|var\s+)([a-zA-Z_][a-zA-Z0-9_]*)\s*(?:=\s*(?:async\s*)?\(|=\s*(?:async\s+)?function|\()"
            metadata["functions"] = re.findall(func_pattern, content)

            import_pattern = r"import\s+.*\s+from\s+['\"](.+?)['\"]"
            metadata["imports"] = re.findall(import_pattern, content)

            # Suspicious patterns
            if "eval(" in content:
                metadata["suspicious_patterns"].append("eval()")
            if "innerHTML" in content:
                metadata["suspicious_patterns"].append("innerHTML (XSS risk)")
            if re.search(r"dangerouslySetInnerHTML", content):
                metadata["suspicious_patterns"].append("dangerouslySetInnerHTML")

        elif language == "java":
            func_pattern = r"(?:public|private|protected)\s+(?:static\s+)?(?:\w+\s+)+(\w+)\s*\("
            metadata["functions"] = re.findall(func_pattern, content)

            import_pattern = r"import\s+([\w.]+);"
            metadata["imports"] = re.findall(import_pattern, content)

            # Suspicious patterns
            if "Runtime.getRuntime().exec" in content:
                metadata["suspicious_patterns"].append("Runtime.exec (command injection risk)")

        elif language == "go":
            func_pattern = r"func\s+(\w+)\s*\("
            metadata["functions"] = re.findall(func_pattern, content)

            import_pattern = r'import\s+(?:"(.+?)"|`(.+?)`)'
            metadata["imports"] = [m[0] or m[1] for m in re.findall(import_pattern, content)]

            # Suspicious patterns
            if "os/exec" in content:
                metadata["suspicious_patterns"].append("os/exec")

        # Limit list sizes
        metadata["functions"] = metadata["functions"][:50]
        metadata["imports"] = metadata["imports"][:30]

        return metadata

    def _calculate_file_priority(self, file_path: str) -> tuple:
        """Calculate file priority score (0-100) and reasons.

        Returns: (score, reasons)
        """
        path_lower = file_path.lower()
        score = 50  # Base score
        reasons = []

        # Routes, controllers, handlers, views (+30)
        if any(keyword in path_lower for keyword in ["route", "controller", "handler", "view", "endpoint"]):
            score += 30
            reasons.append("Route/Controller")

        # Auth, login, session (+25)
        if any(keyword in path_lower for keyword in ["auth", "login", "session", "token", "password"]):
            score += 25
            reasons.append("Authentication")

        # Database, ORM, query, model (+20)
        if any(keyword in path_lower for keyword in ["database", "db", "orm", "query", "model"]):
            score += 20
            reasons.append("Database")

        # API definitions (+15)
        if any(keyword in path_lower for keyword in ["api", "graphql", "rest"]):
            score += 15
            reasons.append("API")

        # Config, settings (+10)
        if any(keyword in path_lower for keyword in ["config", "settings", "env"]):
            score += 10
            reasons.append("Config")

        # Test files (-10)
        if any(keyword in path_lower for keyword in ["test", "spec", "_test.", ".test."]):
            score -= 10
            reasons.append("Test file")

        # Generated, vendor (-20)
        if any(keyword in path_lower for keyword in ["generated", "vendor", "node_modules", ".min.", "bundle"]):
            score -= 20
            reasons.append("Generated/Vendor")

        return (max(0, min(100, score)), reasons)

    def extract_function_at_line(
        self,
        content: str,
        language: str,
        line_number: int,
    ) -> Dict[str, Any]:
        """Extract function containing the specified line number.

        Parameters
        ----------
        content : str
            Full file content
        language : str
            Programming language
        line_number : int
            Line number (1-indexed) to find function for

        Returns
        -------
        dict
            {
                "function_name": str or None,
                "start_line": int,
                "end_line": int,
                "code": str
            }
        """
        lines = content.splitlines()
        if line_number < 1 or line_number > len(lines):
            # Out of bounds - return context window
            return self._context_window(lines, line_number)

        if language == "python":
            return self._extract_python_function(lines, line_number)
        elif language in ["javascript", "typescript", "java", "go", "c", "cpp", "csharp", "rust", "swift", "kotlin"]:
            return self._extract_brace_function(lines, line_number, language)
        else:
            # Fallback to context window
            return self._context_window(lines, line_number)

    def _extract_python_function(self, lines: List[str], line_number: int) -> Dict[str, Any]:
        """Extract Python function by indentation."""
        idx = line_number - 1  # Convert to 0-indexed

        # Walk backwards to find def/class at same or lower indentation
        target_indent = len(lines[idx]) - len(lines[idx].lstrip())
        start_idx = idx
        function_name = None

        for i in range(idx, -1, -1):
            line = lines[i]
            if not line.strip():
                continue  # Skip blank lines

            indent = len(line) - len(line.lstrip())

            # Found def or class at lower or equal indentation
            if indent <= target_indent and (line.lstrip().startswith("def ") or line.lstrip().startswith("class ")):
                start_idx = i
                # Extract function name
                match = re.search(r"(?:def|class)\s+([a-zA-Z_][a-zA-Z0-9_]*)", line)
                if match:
                    function_name = match.group(1)
                target_indent = indent
                break

        # Walk forward to find end (return to same or lower indentation)
        end_idx = len(lines)
        for i in range(start_idx + 1, len(lines)):
            line = lines[i]
            if not line.strip():
                continue  # Skip blank lines

            indent = len(line) - len(line.lstrip())
            if indent <= target_indent:
                end_idx = i
                break

        return {
            "function_name": function_name,
            "start_line": start_idx + 1,
            "end_line": end_idx,
            "code": "\n".join(lines[start_idx:end_idx]),
        }

    def _extract_brace_function(self, lines: List[str], line_number: int, language: str) -> Dict[str, Any]:
        """Extract function from brace-delimited language (JS/TS/Java/Go/C/etc.)."""
        idx = line_number - 1

        # Function signature patterns by language
        # Prioritize 'function' keyword to avoid matching variable declarations
        patterns = {
            "javascript": r"function\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*\(",
            "typescript": r"function\s+([a-zA-Z_$][a-zA-Z0-9_$]*)\s*\(",
            "java": r"(?:public|private|protected|static).*\s+(\w+)\s*\(",
            "go": r"func\s+(?:\(.*\)\s+)?(\w+)\s*\(",
            "c": r"\w+\s+(\w+)\s*\(",
            "cpp": r"\w+\s+(\w+)\s*\(",
            "csharp": r"(?:public|private|protected|internal).*\s+(\w+)\s*\(",
            "rust": r"fn\s+(\w+)\s*[<(]",
            "swift": r"func\s+(\w+)\s*[<(]",
            "kotlin": r"fun\s+(\w+)\s*[<(]",
        }
        pattern = patterns.get(language, r"function\s+(\w+)\s*\(")

        # Walk backwards to find function signature
        start_idx = idx
        function_name = None
        brace_start = None

        for i in range(idx, max(-1, idx - 50), -1):
            line = lines[i]
            match = re.search(pattern, line)
            if match:
                start_idx = i
                function_name = match.group(1)
                # Find opening brace
                for j in range(i, min(len(lines), i + 10)):
                    if "{" in lines[j]:
                        brace_start = j
                        break
                break

        if brace_start is None:
            # No function found, return context window
            return self._context_window(lines, line_number)

        # Track brace depth to find closing brace
        depth = 0
        end_idx = len(lines)

        for i in range(brace_start, len(lines)):
            line = lines[i]
            depth += line.count("{")
            depth -= line.count("}")

            if depth == 0:
                end_idx = i + 1
                break

        return {
            "function_name": function_name,
            "start_line": start_idx + 1,
            "end_line": end_idx,
            "code": "\n".join(lines[start_idx:end_idx]),
        }

    def _context_window(self, lines: List[str], line_number: int, window: int = 15) -> Dict[str, Any]:
        """Return Â±window lines around line_number as fallback."""
        idx = line_number - 1
        start_idx = max(0, idx - window)
        end_idx = min(len(lines), idx + window + 1)

        return {
            "function_name": None,
            "start_line": start_idx + 1,
            "end_line": end_idx,
            "code": "\n".join(lines[start_idx:end_idx]),
        }

    def enumerate_functions_with_lines(
        self, content: str, language: str, include_tests: bool = False
    ) -> List[Dict[str, Any]]:
        """Extract all function definitions with line numbers.

        Parameters
        ----------
        content : str
            Full file content
        language : str
            Programming language
        include_tests : bool, optional
            Include test functions (default: False)

        Returns
        -------
        list of dict
            [{"name": str, "start_line": int, "end_line": int, "signature": str}, ...]
            Capped at 500 functions per file
        """
        lines = content.splitlines()
        functions = []

        if language == "python":
            # Find all function definitions
            for match in re.finditer(r"^([ \t]*)def\s+([a-zA-Z_]\w*)\s*\(", content, re.MULTILINE):
                func_name = match.group(2)

                # Skip test functions unless include_tests=True
                if not include_tests and (func_name.startswith("test_") or func_name.endswith("_test")):
                    continue

                # Get line number from match position
                line_num = content[:match.start()].count("\n") + 1

                # Extract function details using existing method
                func_info = self._extract_python_function(lines, line_num)

                # Build signature (first line)
                sig_start = match.start()
                sig_end = content.find(":", sig_start) + 1
                signature = content[sig_start:sig_end].strip() if sig_end > sig_start else match.group(0).strip()

                functions.append({
                    "name": func_name,
                    "start_line": func_info["start_line"],
                    "end_line": func_info["end_line"],
                    "signature": signature,
                })

        elif language in ["javascript", "typescript"]:
            # Function declarations + arrow functions
            patterns = [
                r"function\s+([a-zA-Z_$][\w$]*)\s*\(",  # function foo()
                r"(?:const|let|var)\s+([a-zA-Z_$][\w$]*)\s*=\s*(?:async\s*)?\(",  # const foo = (
                r"(?:const|let|var)\s+([a-zA-Z_$][\w$]*)\s*=\s*(?:async\s+)?function",  # const foo = function
            ]

            for pattern in patterns:
                for match in re.finditer(pattern, content, re.MULTILINE):
                    func_name = match.group(1)

                    # Skip test functions unless include_tests=True
                    if not include_tests and (func_name.startswith("test") or func_name.endswith("Test") or "test" in func_name.lower()):
                        continue

                    line_num = content[:match.start()].count("\n") + 1
                    func_info = self._extract_brace_function(lines, line_num, language)

                    # Build signature (first line containing function def)
                    sig_start = match.start()
                    sig_end = content.find("{", sig_start)
                    signature = content[sig_start:sig_end].strip() if sig_end > sig_start else match.group(0).strip()

                    functions.append({
                        "name": func_name,
                        "start_line": func_info["start_line"],
                        "end_line": func_info["end_line"],
                        "signature": signature,
                    })

        elif language in ["java", "go", "c", "cpp", "csharp", "rust", "swift", "kotlin"]:
            # Language-specific function patterns
            patterns_map = {
                "java": r"(?:public|private|protected)\s+(?:static\s+)?(?:\w+\s+)+(\w+)\s*\(",
                "go": r"func\s+(?:\(.*\)\s+)?(\w+)\s*\(",
                "c": r"\w+\s+(\w+)\s*\(",
                "cpp": r"\w+\s+(\w+)\s*\(",
                "csharp": r"(?:public|private|protected|internal)\s+(?:static\s+)?(?:\w+\s+)+(\w+)\s*\(",
                "rust": r"fn\s+(\w+)\s*[<(]",
                "swift": r"func\s+(\w+)\s*[<(]",
                "kotlin": r"fun\s+(\w+)\s*[<(]",
            }

            pattern = patterns_map.get(language, r"function\s+(\w+)\s*\(")

            for match in re.finditer(pattern, content, re.MULTILINE):
                func_name = match.group(1)

                # Skip test functions unless include_tests=True
                if not include_tests and ("test" in func_name.lower()):
                    continue

                line_num = content[:match.start()].count("\n") + 1
                func_info = self._extract_brace_function(lines, line_num, language)

                # Build signature
                sig_start = match.start()
                sig_end = content.find("{", sig_start)
                signature = content[sig_start:sig_end].strip() if sig_end > sig_start else match.group(0).strip()

                functions.append({
                    "name": func_name,
                    "start_line": func_info["start_line"],
                    "end_line": func_info["end_line"],
                    "signature": signature,
                })

        # Remove duplicates (same function name at same line) and cap at 500
        seen = set()
        unique_functions = []
        for func in functions:
            key = (func["name"], func["start_line"])
            if key not in seen:
                seen.add(key)
                unique_functions.append(func)
                if len(unique_functions) >= 500:
                    break

        return unique_functions


# -- Module-level helper --

def get_code_indexer(db: WorldModelDatabase) -> CodeIndexer:
    """Factory function for CodeIndexer."""
    return CodeIndexer(db)
