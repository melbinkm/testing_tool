"""
Embedder - Lazy singleton wrapper for sentence-transformers embedding model.

Generates 384-dimensional embeddings using all-MiniLM-L6-v2 with ONNX backend.
Falls back gracefully if sentence-transformers is not installed.
"""

from __future__ import annotations

import logging
from typing import List, Optional

logger = logging.getLogger("autopentest-mcp")


class Embedder:
    """Lazy-loaded embedding model wrapper."""

    MODEL_NAME = "all-MiniLM-L6-v2"
    DIMENSIONS = 384

    def __init__(self) -> None:
        self._model = None
        self._available: Optional[bool] = None

    def _load(self) -> None:
        """Load model on first use. Sets _available flag."""
        if self._available is not None:
            return
        try:
            from sentence_transformers import SentenceTransformer
            self._model = SentenceTransformer(self.MODEL_NAME, backend="onnx")
            self._available = True
            logger.info("Embedder loaded: %s (ONNX)", self.MODEL_NAME)
        except Exception as exc:
            self._available = False
            logger.info("Embedder not available (FTS5-only mode): %s", exc)

    @property
    def available(self) -> bool:
        """Whether the embedding model is loaded and usable."""
        if self._available is None:
            self._load()
        return self._available  # type: ignore[return-value]

    def embed(self, text: str) -> Optional[bytes]:
        """Return embedding as raw float32 bytes, or None if unavailable."""
        if not self.available or not text:
            return None
        try:
            import numpy as np
            vec = self._model.encode(text, convert_to_numpy=True)
            return vec.astype(np.float32).tobytes()
        except Exception as exc:
            logger.warning("Embedding failed: %s", exc)
            return None

    def embed_batch(self, texts: List[str]) -> List[Optional[bytes]]:
        """Batch embedding for chunked content."""
        if not self.available or not texts:
            return [None] * len(texts)
        try:
            import numpy as np
            vecs = self._model.encode(texts, convert_to_numpy=True, batch_size=32)
            return [v.astype(np.float32).tobytes() for v in vecs]
        except Exception as exc:
            logger.warning("Batch embedding failed: %s", exc)
            return [None] * len(texts)

    @staticmethod
    def cosine_similarity(a: bytes, b: bytes) -> float:
        """Compute cosine similarity between two float32 embedding byte arrays."""
        import struct
        import math

        n = len(a) // 4
        va = struct.unpack(f"{n}f", a)
        vb = struct.unpack(f"{n}f", b)

        dot = sum(x * y for x, y in zip(va, vb))
        norm_a = math.sqrt(sum(x * x for x in va))
        norm_b = math.sqrt(sum(x * x for x in vb))
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return dot / (norm_a * norm_b)


# ---------------------------------------------------------------------------
# Module-level singleton
# ---------------------------------------------------------------------------

_embedder: Optional[Embedder] = None


def get_embedder() -> Embedder:
    """Return the global Embedder singleton."""
    global _embedder
    if _embedder is None:
        _embedder = Embedder()
    return _embedder
