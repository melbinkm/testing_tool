"""
Code Audit Tools - LLM-powered full code audit

Provides 4 tools for systematic function-level code review:
1. code_audit_enumerate - Parse all functions, create audit queue
2. code_audit_get_next - Get next unreviewed function (prioritized by risk)
3. code_audit_mark_reviewed - Record LLM verdict
4. code_audit_progress - Show audit completion stats
"""
from __future__ import annotations

import os
from typing import Any, Dict, List

from mcp.types import Tool, TextContent

from lib.code_indexer import get_code_indexer
from lib.sast_runner import get_sast_runner
from lib.world_model_db import WorldModelDatabase


# -- Helper: Get DB instance --

def _get_db(mcp_service: Any) -> WorldModelDatabase:
    """Get WorldModelDatabase instance for current assessment."""
    if not mcp_service.current_assessment_id:
        raise ValueError("No assessment loaded. Call load_assessment first.")

    from lib.world_model_db import get_shared_pool
    pool = get_shared_pool()
    if not pool:
        raise RuntimeError("Database pool not initialized")

    return WorldModelDatabase(pool=pool, assessment_id=mcp_service.current_assessment_id)


# -- Tool 1: Enumerate Functions --

async def _handle_code_audit_enumerate(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Enumerate all functions in indexed source code, create audit queue.

    Strategy:
    1. Query wm_knowledge(category="source_code") for all indexed files
    2. For each file: extract functions with line numbers via code_indexer
    3. Compute risk score per function (0-100) based on:
       - File priority score (existing _calculate_file_priority)
       - Function name keywords (auth/sql/admin/eval/crypto/upload)
    4. Map score to tier: ≥80=critical, ≥60=high, ≥40=medium, <40=low
    5. Check SAST overlap: if SAST finding exists within function's line range, auto-mark reviewed
    6. Store as wm_knowledge(category="code_audit_queue") with metadata
    7. Return summary stats
    """
    db = _get_db(mcp_service)
    code_indexer = get_code_indexer(db)

    risk_filter = args.get("risk_filter")  # Optional: "critical"|"high"|"medium"|"low"
    max_functions = args.get("max_functions", 0)  # 0 = unlimited
    include_tests = args.get("include_tests", False)

    # Query all indexed source files
    source_files = await db.recall_knowledge(
        category="source_code",
        limit=10000  # Cap at 10k files
    )

    if not source_files:
        return [TextContent(type="text", text="No source code indexed. Call sast_index_repo first.")]

    # Query all SAST findings for overlap detection
    sast_findings = await db.recall_knowledge(
        category="sast_scan_result",
        limit=10000
    )

    # Build SAST line range index: {file_path: [(start_line, end_line), ...]}
    sast_ranges = {}
    for finding in sast_findings:
        file_path = finding.get("title", "")  # SAST findings store file path in title
        line = finding.get("metadata", {}).get("line", 0)
        if file_path and line:
            if file_path not in sast_ranges:
                sast_ranges[file_path] = []
            sast_ranges[file_path].append(line)

    # Process each source file
    functions_queued = 0
    by_risk_tier = {"critical": 0, "high": 0, "medium": 0, "low": 0}
    covered_by_sast = 0
    files_processed = 0

    for source_file in source_files:
        file_path = source_file.get("title", "")  # Relative path
        content = source_file.get("content", "")
        language = source_file.get("metadata", {}).get("language", "unknown")
        file_priority = source_file.get("metadata", {}).get("priority_score", 50)

        if not content:
            continue

        # Extract functions with line numbers
        functions = code_indexer.enumerate_functions_with_lines(content, language, include_tests)

        for func in functions:
            # Compute risk score
            risk_score = _calculate_function_risk(func["name"], file_priority)
            risk_tier = _score_to_tier(risk_score)

            # Apply risk filter
            if risk_filter:
                tier_order = {"low": 0, "medium": 1, "high": 2, "critical": 3}
                if tier_order.get(risk_tier, 0) < tier_order.get(risk_filter, 0):
                    continue  # Skip lower-risk functions

            # Check SAST overlap
            reviewed = False
            verdict = None
            sast_lines = sast_ranges.get(file_path, [])
            for sast_line in sast_lines:
                if func["start_line"] <= sast_line <= func["end_line"]:
                    reviewed = True
                    verdict = "covered_by_sast"
                    covered_by_sast += 1
                    break

            # Store in audit queue
            await db.store_knowledge(
                title=f"{file_path}::{func['name']}",
                content=func["signature"],
                target=file_path,
                source_tool="code_audit",
                category="code_audit_queue",
                tags=[language, risk_tier],
                metadata={
                    "file": file_path,
                    "function_name": func["name"],
                    "start_line": func["start_line"],
                    "end_line": func["end_line"],
                    "language": language,
                    "risk_score": risk_score,
                    "risk_tier": risk_tier,
                    "reviewed": reviewed,
                    "verdict": verdict,
                    "signature": func["signature"],
                },
            )

            functions_queued += 1
            by_risk_tier[risk_tier] += 1

            # Check max_functions cap
            if max_functions > 0 and functions_queued >= max_functions:
                break

        files_processed += 1

        # Break outer loop if max reached
        if max_functions > 0 and functions_queued >= max_functions:
            break

    # Return summary
    result = {
        "status": "ok",
        "functions_queued": functions_queued,
        "by_risk_tier": by_risk_tier,
        "covered_by_sast": covered_by_sast,
        "files_processed": files_processed,
    }

    return [TextContent(type="text", text=f"Code audit queue created:\n{_format_dict(result)}")]


def _calculate_function_risk(function_name: str, file_priority: int) -> int:
    """Calculate function risk score (0-100) from name keywords + file priority.

    Base: file_priority (0-100)
    Bonuses:
    - auth/login/password: +20
    - query/execute/sql: +15
    - admin/role/permission: +10
    - eval/exec/subprocess: +25
    - encrypt/decrypt/hash: +15
    - upload/file/path: +10
    """
    score = file_priority
    name_lower = function_name.lower()

    # Authentication keywords
    if any(kw in name_lower for kw in ["auth", "login", "password", "session", "token"]):
        score += 20

    # Database/SQL keywords
    if any(kw in name_lower for kw in ["query", "execute", "sql", "database", "db"]):
        score += 15

    # Authorization keywords
    if any(kw in name_lower for kw in ["admin", "role", "permission", "access", "authorize"]):
        score += 10

    # Code execution keywords
    if any(kw in name_lower for kw in ["eval", "exec", "subprocess", "command", "run"]):
        score += 25

    # Cryptography keywords
    if any(kw in name_lower for kw in ["encrypt", "decrypt", "hash", "crypto", "cipher"]):
        score += 15

    # File operations keywords
    if any(kw in name_lower for kw in ["upload", "file", "path", "directory", "read", "write"]):
        score += 10

    return min(100, max(0, score))


def _score_to_tier(score: int) -> str:
    """Map risk score to tier: ≥80=critical, ≥60=high, ≥40=medium, <40=low."""
    if score >= 80:
        return "critical"
    elif score >= 60:
        return "high"
    elif score >= 40:
        return "medium"
    else:
        return "low"


# -- Tool 2: Get Next Unreviewed Function --

async def _handle_code_audit_get_next(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Get next unreviewed function, prioritized by risk score (highest first).

    Returns:
    - queue_item_id, function_name, file, risk_score, risk_tier
    - function_code (extracted from source file)
    - local_file_path (for reading full source)
    - investigation_guide (5-step review checklist)
    - remaining_unreviewed count
    - remaining_by_tier breakdown

    If all reviewed: {"status": "all_reviewed", "total_reviewed": N}
    """
    db = _get_db(mcp_service)
    sast_runner = get_sast_runner()
    code_indexer = get_code_indexer(db)

    risk_tier = args.get("risk_tier")  # Optional filter: "critical"|"high"|"medium"|"low"

    # Query all audit queue items
    queue_items = await db.recall_knowledge(
        category="code_audit_queue",
        limit=500
    )

    # Filter: unreviewed + optional risk_tier
    unreviewed = []
    for item in queue_items:
        metadata = item.get("metadata", {})
        if not metadata.get("reviewed", False):
            if risk_tier and metadata.get("risk_tier") != risk_tier:
                continue
            unreviewed.append(item)

    # Check if all reviewed
    if not unreviewed:
        total_reviewed = len([item for item in queue_items if item.get("metadata", {}).get("reviewed", False)])
        return [TextContent(
            type="text",
            text=f"All functions reviewed! Total reviewed: {total_reviewed}"
        )]

    # Sort by risk_score descending
    unreviewed.sort(key=lambda x: x.get("metadata", {}).get("risk_score", 0), reverse=True)

    # Take first (highest risk)
    next_item = unreviewed[0]
    metadata = next_item["metadata"]

    # Read source file content
    file_path = metadata["file"]
    repo_path = f"/mnt/d/testing_tool/AutoPentest/sast_repos/{mcp_service.current_assessment_id}"
    local_file_path = os.path.join(repo_path, file_path)

    # Read file via sast_runner
    cat_result = await sast_runner._exec("cat", [local_file_path], timeout=10)
    if cat_result["exit_code"] != 0:
        return [TextContent(
            type="text",
            text=f"Error reading file {local_file_path}: {cat_result.get('stderr', 'Unknown error')}"
        )]

    content = cat_result["stdout"]

    # Extract function code
    func_info = code_indexer.extract_function_at_line(
        content=content,
        language=metadata["language"],
        line_number=metadata["start_line"]
    )

    # Build remaining stats
    remaining_by_tier = {"critical": 0, "high": 0, "medium": 0, "low": 0}
    for item in unreviewed:
        tier = item.get("metadata", {}).get("risk_tier", "low")
        remaining_by_tier[tier] += 1

    # Investigation guide
    investigation_guide = _get_investigation_guide(metadata["function_name"], metadata["risk_tier"])

    result = {
        "status": "ok",
        "queue_item_id": next_item["id"],
        "function_name": metadata["function_name"],
        "file": metadata["file"],
        "risk_score": metadata["risk_score"],
        "risk_tier": metadata["risk_tier"],
        "start_line": metadata["start_line"],
        "end_line": metadata["end_line"],
        "function_code": func_info["code"],
        "local_file_path": local_file_path,
        "investigation_guide": investigation_guide,
        "remaining_unreviewed": len(unreviewed),
        "remaining_by_tier": remaining_by_tier,
    }

    return [TextContent(type="text", text=_format_dict(result))]


def _get_investigation_guide(function_name: str, risk_tier: str) -> str:
    """Return security review checklist for function."""
    return f"""
Security Review Checklist for {function_name} ({risk_tier} risk):

1. Input Validation
   - Are all inputs validated before use?
   - Are there checks for type, length, format?
   - Is there a whitelist of allowed values?

2. SQL Injection (if database access)
   - Are parameterized queries used?
   - Is input sanitized before SQL execution?
   - Are there string concatenations in SQL?

3. Authentication/Authorization
   - Are permissions checked before sensitive operations?
   - Is user identity verified?
   - Are there hardcoded credentials?

4. Code Injection
   - Are there eval(), exec(), or subprocess calls?
   - Is user input passed to code execution?
   - Are template injections possible?

5. Data Exposure
   - Is sensitive data logged?
   - Are error messages verbose?
   - Is data encrypted where necessary?

Verdict Options:
- safe: No security issues found
- suspicious: Potential issue, needs dynamic testing
- vulnerable: Confirmed security vulnerability
""".strip()


# -- Tool 3: Mark Reviewed --

async def _handle_code_audit_mark_reviewed(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Record LLM verdict for a function.

    Parameters:
    - queue_item_id (required): ID from code_audit_get_next
    - verdict (required): "safe" | "suspicious" | "vulnerable"
    - vuln_class, severity, description, code_snippet, remediation, cwe (optional, for vulnerable)

    Actions:
    - Update metadata: reviewed=True, verdict=<verdict>
    - If vulnerable: Create finding card via safe_add_card
    - If suspicious: Store suspicious_reason in metadata (for follow-up testing)
    """
    db = _get_db(mcp_service)

    queue_item_id = args.get("queue_item_id")
    verdict = args.get("verdict")  # "safe" | "suspicious" | "vulnerable"

    if not queue_item_id or not verdict:
        return [TextContent(type="text", text="Error: queue_item_id and verdict are required")]

    if verdict not in ["safe", "suspicious", "vulnerable"]:
        return [TextContent(type="text", text=f"Error: verdict must be 'safe', 'suspicious', or 'vulnerable', got '{verdict}'")]

    # Load entry
    entries = await db.recall_knowledge(category="code_audit_queue", limit=500)
    entry = next((e for e in entries if e["id"] == queue_item_id), None)

    if not entry:
        return [TextContent(type="text", text=f"Error: Queue item {queue_item_id} not found")]

    # Update metadata
    metadata = entry.get("metadata", {})
    metadata["reviewed"] = True
    metadata["verdict"] = verdict

    # Handle verdict-specific actions
    finding_created = False

    if verdict == "vulnerable":
        # Create finding card
        vuln_class = args.get("vuln_class", "code_vulnerability")
        severity = args.get("severity", "medium")
        description = args.get("description", f"Security vulnerability in {metadata['function_name']}")
        code_snippet = args.get("code_snippet", "")
        remediation = args.get("remediation", "")
        cwe = args.get("cwe", "")

        # Store vuln details in metadata
        metadata["vuln_class"] = vuln_class
        metadata["severity"] = severity
        metadata["description"] = description
        metadata["remediation"] = remediation
        metadata["cwe"] = cwe

        # Create finding card
        card_data = {
            "card_type": "finding",
            "title": f"Code Audit: {vuln_class} in {metadata['function_name']}",
            "description": description,
            "severity": severity,
            "tags": [vuln_class, metadata["language"], "code_audit"],
            "metadata": {
                "file": metadata["file"],
                "function": metadata["function_name"],
                "line_range": f"{metadata['start_line']}-{metadata['end_line']}",
                "code_snippet": code_snippet,
                "remediation": remediation,
                "cwe": cwe,
                "source": "code_audit",
            },
        }

        await mcp_service.safe_add_card(**card_data)
        finding_created = True

    elif verdict == "suspicious":
        # Store reason for follow-up
        suspicious_reason = args.get("suspicious_reason", "Needs dynamic testing")
        metadata["suspicious_reason"] = suspicious_reason

    # Update entry
    await db.update_knowledge(queue_item_id, metadata=metadata)

    result = {
        "status": "ok",
        "queue_item_id": queue_item_id,
        "verdict": verdict,
        "finding_created": finding_created,
    }

    return [TextContent(type="text", text=_format_dict(result))]


# -- Tool 4: Progress --

async def _handle_code_audit_progress(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Show audit completion stats.

    Returns:
    - total functions in queue
    - reviewed count
    - unreviewed count
    - completion_pct
    - by_verdict breakdown (safe, suspicious, vulnerable, covered_by_sast, unreviewed)
    - by_risk_tier breakdown (critical, high, medium, low)
    - findings_created count
    """
    db = _get_db(mcp_service)

    # Query all queue items
    queue_items = await db.recall_knowledge(
        category="code_audit_queue",
        limit=10000
    )

    total = len(queue_items)
    reviewed = 0
    by_verdict = {
        "safe": 0,
        "suspicious": 0,
        "vulnerable": 0,
        "covered_by_sast": 0,
        "unreviewed": 0,
    }
    by_risk_tier = {"critical": 0, "high": 0, "medium": 0, "low": 0}
    findings_created = 0

    for item in queue_items:
        metadata = item.get("metadata", {})
        tier = metadata.get("risk_tier", "low")
        by_risk_tier[tier] += 1

        if metadata.get("reviewed", False):
            reviewed += 1
            verdict = metadata.get("verdict", "safe")
            by_verdict[verdict] += 1

            if verdict == "vulnerable":
                findings_created += 1
        else:
            by_verdict["unreviewed"] += 1

    completion_pct = round((reviewed / total) * 100, 1) if total > 0 else 100.0

    result = {
        "status": "ok",
        "total": total,
        "reviewed": reviewed,
        "unreviewed": total - reviewed,
        "completion_pct": completion_pct,
        "by_verdict": by_verdict,
        "by_risk_tier": by_risk_tier,
        "findings_created": findings_created,
    }

    return [TextContent(type="text", text=_format_dict(result))]


# -- Tool Registry --

def get_code_audit_tools() -> List[Tool]:
    """Return code audit tool definitions."""
    return [
        Tool(
            name="code_audit_enumerate",
            description="""Parse all indexed source files, extract ALL functions with line numbers, create audit queue.

            **When to use:** After sast_index_repo to enumerate all functions for comprehensive security review.

            **Parameters:**
            - risk_filter (optional): "critical"|"high"|"medium"|"low" - only enumerate files at/above this tier
            - max_functions (optional, default 0=unlimited): cap on queue size
            - include_tests (optional, default false): include test functions

            **Returns:** {functions_queued, by_risk_tier, covered_by_sast, files_processed}

            **Budget impact:** Low (local processing, no HTTP requests)
            **Dependencies:** sast_index_repo (must have source_code in wm_knowledge)
            **Risk level:** safe
            """,
            inputSchema={
                "type": "object",
                "properties": {
                    "risk_filter": {
                        "type": "string",
                        "enum": ["critical", "high", "medium", "low"],
                        "description": "Only enumerate files at/above this risk tier",
                    },
                    "max_functions": {
                        "type": "integer",
                        "description": "Cap on queue size (0=unlimited)",
                        "default": 0,
                    },
                    "include_tests": {
                        "type": "boolean",
                        "description": "Include test functions in audit queue",
                        "default": False,
                    },
                },
            },
        ),
        Tool(
            name="code_audit_get_next",
            description="""Get next unreviewed function, prioritized by risk score (highest first).

            **When to use:** After code_audit_enumerate to get the next function for LLM security review.

            **Parameters:**
            - risk_tier (optional): filter to specific tier ("critical"|"high"|"medium"|"low")

            **Returns:** {queue_item_id, function_name, file, risk_score, risk_tier, function_code, local_file_path, investigation_guide, remaining_unreviewed, remaining_by_tier}

            **Budget impact:** Low (local file read)
            **Dependencies:** code_audit_enumerate
            **Risk level:** safe
            """,
            inputSchema={
                "type": "object",
                "properties": {
                    "risk_tier": {
                        "type": "string",
                        "enum": ["critical", "high", "medium", "low"],
                        "description": "Filter to specific risk tier",
                    },
                },
            },
        ),
        Tool(
            name="code_audit_mark_reviewed",
            description="""Record LLM verdict for a function after security review.

            **When to use:** After reviewing code from code_audit_get_next.

            **Parameters:**
            - queue_item_id (required): ID from code_audit_get_next
            - verdict (required): "safe"|"suspicious"|"vulnerable"
            - vuln_class, severity, description, code_snippet, remediation, cwe (optional, for vulnerable)
            - suspicious_reason (optional, for suspicious)

            **Actions:**
            - Updates metadata: reviewed=True, verdict=<verdict>
            - If vulnerable: Creates finding card
            - If suspicious: Stores reason for follow-up testing

            **Budget impact:** Low (local DB write)
            **Dependencies:** code_audit_get_next
            **Risk level:** safe
            """,
            inputSchema={
                "type": "object",
                "properties": {
                    "queue_item_id": {
                        "type": "string",
                        "description": "Queue item ID from code_audit_get_next",
                    },
                    "verdict": {
                        "type": "string",
                        "enum": ["safe", "suspicious", "vulnerable"],
                        "description": "Security review verdict",
                    },
                    "vuln_class": {
                        "type": "string",
                        "description": "Vulnerability class (e.g., sql_injection, xss)",
                    },
                    "severity": {
                        "type": "string",
                        "enum": ["info", "low", "medium", "high", "critical"],
                        "description": "Severity level",
                    },
                    "description": {
                        "type": "string",
                        "description": "Detailed vulnerability description",
                    },
                    "code_snippet": {
                        "type": "string",
                        "description": "Vulnerable code snippet",
                    },
                    "remediation": {
                        "type": "string",
                        "description": "Remediation guidance",
                    },
                    "cwe": {
                        "type": "string",
                        "description": "CWE ID (e.g., CWE-89)",
                    },
                    "suspicious_reason": {
                        "type": "string",
                        "description": "Reason for suspicious verdict",
                    },
                },
                "required": ["queue_item_id", "verdict"],
            },
        ),
        Tool(
            name="code_audit_progress",
            description="""Show audit completion stats.

            **When to use:** To check progress on code audit queue.

            **Returns:** {total, reviewed, unreviewed, completion_pct, by_verdict, by_risk_tier, findings_created}

            **Budget impact:** Low (local DB query)
            **Dependencies:** code_audit_enumerate
            **Risk level:** safe
            """,
            inputSchema={
                "type": "object",
                "properties": {},
            },
        ),
    ]


async def handle_code_audit_tool(name: str, args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Dispatch code audit tool calls to handlers."""
    if name == "code_audit_enumerate":
        return await _handle_code_audit_enumerate(args, mcp_service)
    elif name == "code_audit_get_next":
        return await _handle_code_audit_get_next(args, mcp_service)
    elif name == "code_audit_mark_reviewed":
        return await _handle_code_audit_mark_reviewed(args, mcp_service)
    elif name == "code_audit_progress":
        return await _handle_code_audit_progress(args, mcp_service)
    else:
        return [TextContent(type="text", text=f"Unknown code audit tool: {name}")]


# -- Helper: Format dict for output --

def _format_dict(d: Dict[str, Any], indent: int = 0) -> str:
    """Format dict as readable text."""
    lines = []
    for key, value in d.items():
        if isinstance(value, dict):
            lines.append(f"{'  ' * indent}{key}:")
            lines.append(_format_dict(value, indent + 1))
        elif isinstance(value, list):
            lines.append(f"{'  ' * indent}{key}: {value}")
        else:
            lines.append(f"{'  ' * indent}{key}: {value}")
    return "\n".join(lines)
