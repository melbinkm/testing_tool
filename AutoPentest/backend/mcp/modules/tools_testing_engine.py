"""
Testing Engine MCP Tools

Provides 7 new tools for the comprehensive vulnerability testing engine:
- checklist_get: List all vulnerability test specifications
- checklist_analyze_exchange: Analyze request/response for test recommendations
- testing_build_matrix: Build extended 42-class coverage matrix
- testing_next: Get next N test cells with pre-loaded payloads
- testing_should_continue: Ask if enough testing done for a spec/endpoint/param
- testing_record_auth_change: Notify that auth state changed (triggers re-crawl)
- testing_status: Get comprehensive testing progress
"""

from typing import List, Dict, Any, Optional
import json
import logging

from mcp.types import Tool, TextContent

from lib.vuln_checklist import get_vuln_checklist
from lib.exchange_analyzer import get_exchange_analyzer
from lib.payload_registry import get_payload_registry
from lib.test_orchestrator import get_orchestrator
from lib.coverage_tracker import CoverageTracker

logger = logging.getLogger(__name__)


def get_testing_engine_tools() -> List[Tool]:
    """Get all testing engine tools."""
    return [
        Tool(
            name="checklist_get",
            description="""List all 42+ vulnerability test specifications, optionally filtered by category or endpoint.

**When to use:**
- At the start of testing to understand available test categories
- To check which tests apply to a specific endpoint
- To get payload counts for test planning

**Input:**
- category (optional): Filter by category (injection|auth|config|client_side|api|business_logic|info_leak)
- endpoint_id (optional): Filter to tests applicable for specific endpoint
- include_payload_counts (optional): Include number of available payloads per spec

**Returns:**
List of vulnerability specs with metadata:
- id, name, category, description
- tool_name, requires_auth, parameter_bound
- owasp_category, severity_range
- payload_count (if requested)

**Budget impact:** Minimal (read-only)
**Risk level:** None""",
            inputSchema={
                "type": "object",
                "properties": {
                    "category": {"type": "string", "description": "Filter by category"},
                    "endpoint_id": {"type": "string", "description": "Filter to applicable tests for endpoint"},
                    "include_payload_counts": {"type": "boolean", "description": "Include payload counts"}
                },
                "additionalProperties": False
            }
        ),

        Tool(
            name="checklist_analyze_exchange",
            description="""Analyze a request/response pair to recommend additional vulnerability tests beyond the static checklist.

**When to use:**
- After probing an endpoint to get personalized test recommendations
- To identify which parameters are highest risk
- To detect technologies and configuration issues

**Dependencies:**
- Requires a request/response pair (from endpoint_probe or http_send)

**Input:**
- request: {method, url, headers, body, parameters}
- response: {status, headers, body, time_ms}

**Returns:**
ExchangeAnalysis with:
- recommended_tests: List of vuln spec IDs to run
- parameter_recommendations: Which tests to run on which params
- priority_adjustments: Tests that should be prioritized
- detected_technologies: Tech stack detected
- risk_signals: Notable security observations

**Budget impact:** Minimal (analysis only)
**Risk level:** None
**Failure modes:** Invalid request/response format""",
            inputSchema={
                "type": "object",
                "properties": {
                    "request": {
                        "type": "object",
                        "description": "HTTP request to analyze",
                        "properties": {
                            "method": {"type": "string"},
                            "url": {"type": "string"},
                            "headers": {"type": "object"},
                            "body": {"type": "string"},
                            "parameters": {"type": "array"}
                        },
                        "required": ["method", "url"]
                    },
                    "response": {
                        "type": "object",
                        "description": "HTTP response to analyze",
                        "properties": {
                            "status": {"type": "integer"},
                            "headers": {"type": "object"},
                            "body": {"type": "string"},
                            "time_ms": {"type": "number"}
                        },
                        "required": ["status", "headers", "body"]
                    }
                },
                "required": ["request", "response"],
                "additionalProperties": False
            }
        ),

        Tool(
            name="testing_build_matrix",
            description="""Build extended 42-class coverage matrix for discovered endpoints.

**When to use:**
- After crawling completes to build the full test matrix
- After re-crawl to extend matrix with new endpoints
- To get cell count estimates for planning

**Dependencies:**
- Requires prior crawl (crawler_start + crawler_results)
- Endpoint data in world model

**Input:**
- base_url: Target base URL
- endpoint_ids (optional): Specific endpoints (default: all)
- categories (optional): Limit to specific vuln categories

**Returns:**
Matrix summary:
- total_cells: Total test cells in matrix
- cells_by_category: Breakdown per vuln category
- cells_by_endpoint: Breakdown per endpoint
- estimated_duration: Rough time estimate

**Budget impact:** Moderate (builds in-memory matrix)
**Risk level:** None
**Failure modes:** No endpoints found, database connection""",
            inputSchema={
                "type": "object",
                "properties": {
                    "base_url": {"type": "string", "description": "Target base URL"},
                    "endpoint_ids": {"type": "array", "items": {"type": "string"}},
                    "categories": {"type": "array", "items": {"type": "string"}}
                },
                "required": ["base_url"],
                "additionalProperties": False
            }
        ),

        Tool(
            name="testing_next",
            description="""Query untested cells from coverage matrix with metadata.

**When to use:**
- To discover what cells remain untested
- To get endpoint URLs and vulnerability specs for planning tests
- To see payload counts and priorities

**What this returns:**
This tool returns DATA about untested cells. You (the LLM) decide how to test them.

**Dependencies:**
- Requires coverage matrix (testing_build_matrix)

**Input:**
- limit: Maximum number of cells to return (default: 5)
- vuln_spec_id (optional): Filter to specific vuln class

**Returns:**
List of untested cells, each with:
- cell_id: Unique identifier
- endpoint_id: Endpoint ID
- endpoint_url: Full endpoint URL (for HTTP requests)
- endpoint_method: HTTP method (GET, POST, etc.)
- vuln_spec_id: Vulnerability class to test
- parameter: Parameter name (if parameter-bound)
- payload_count: Number of available payloads for this vuln class
- priority: Test priority score (0-100)
- spec_name: Human-readable vuln class name
- spec_category: Category (injection, auth, config, etc.)
- spec_description: What this test checks for
- severity_range: Expected severity if vulnerable

Plus coverage_summary:
- total_cells: Total cells in matrix
- tested_cells: Cells already tested
- pending_cells: Cells remaining
- coverage_pct: Percentage complete

**Decision making:**
Use this data to decide which cells to test and in what order.

**Budget impact:** Low
**Risk level:** None
**Failure modes:** Matrix not built, no untested cells remaining""",
            inputSchema={
                "type": "object",
                "properties": {
                    "limit": {"type": "integer", "default": 5, "description": "Max cells to return"},
                    "vuln_spec_id": {"type": "string", "description": "Filter to specific vuln class"}
                },
                "additionalProperties": False
            }
        ),

        Tool(
            name="testing_should_continue",
            description="""Return factual data about a cell's testing state (attempts, payloads tried, results).

**When to use:**
- After running test payloads to check testing progress
- To get raw state data and make your own decisions
- To see how many attempts remain before max_attempts

**What this returns:**
This tool returns FACTUAL DATA, not recommendations. You (the LLM) decide what to do.

**Returns:**
CellTestingState with factual fields:
- cell_id: Unique identifier for this test cell
- attempt_count: Number of test attempts so far
- max_attempts: Maximum attempts allowed (5)
- payload_types_tried: List of payload types already tried (["hardcoded"], ["llm_generated"], etc.)
- suspicious_signal_count: Count of suspicious signals detected
- suspicious_signals: Array of suspicious signal details
- vulnerability_confirmed: Boolean - was vulnerability confirmed?
- max_attempts_reached: Boolean - hit the 5-attempt limit?
- available_payload_count: Number of payloads in registry for this vuln class
- results_summary: {"clean": N, "suspicious": N, "vulnerable": N}

**Decision making:**
Use this data to decide:
- Continue testing? (attempts < 5, no confirmed vuln, payloads available)
- Try different payload types? (suspicious signals, hardcoded exhausted)
- Move to next cell? (confirmed vuln OR max_attempts OR all clean)

**Budget impact:** Minimal
**Risk level:** None""",
            inputSchema={
                "type": "object",
                "properties": {
                    "vuln_spec_id": {"type": "string", "description": "Vuln spec ID"},
                    "endpoint_id": {"type": "string", "description": "Endpoint ID"},
                    "parameter": {"type": "string", "description": "Parameter (if parameter-bound)"},
                    "results_so_far": {"type": "array", "description": "Test results"}
                },
                "required": ["vuln_spec_id", "endpoint_id"],
                "additionalProperties": False
            }
        ),

        Tool(
            name="testing_record_auth_change",
            description="""Notify that authentication state changed, triggering a re-crawl workflow.

**When to use:**
- After discovering new credentials (credentials_add)
- After privilege escalation
- After obtaining a new session token

**Dependencies:**
- Identity must exist (auth_get_identities)

**Input:**
- identity_id: Identity that changed
- change_type: new_credential | privilege_escalation | session_token

**Returns:**
- status: recorded
- recrawl_scheduled: bool
- next_actions: What to do next (crawler_start, coverage_discover, etc.)

**Budget impact:** Low (schedules re-crawl)
**Risk level:** None
**Failure modes:** Invalid identity_id""",
            inputSchema={
                "type": "object",
                "properties": {
                    "identity_id": {"type": "string", "description": "Identity ID"},
                    "change_type": {
                        "type": "string",
                        "enum": ["new_credential", "privilege_escalation", "session_token"],
                        "description": "Type of auth change"
                    },
                    "details": {"type": "object", "description": "Additional context"}
                },
                "required": ["identity_id", "change_type"],
                "additionalProperties": False
            }
        ),

        Tool(
            name="testing_status",
            description="""Get comprehensive testing progress and orchestrator status.

**When to use:**
- Periodically during testing to check progress
- To get guidance on next actions
- To check if testing is complete

**Dependencies:** None

**Input:** None (uses current assessment context)

**Returns:**
Status with:
- state: Current orchestrator state
- progress: Coverage %, cells tested, vulns found
- crawl_count: Number of crawls performed
- pending_recrawls: Re-crawls scheduled
- auth_changes: Recent auth state changes
- recommendations: Next 5 recommended actions

**Budget impact:** Minimal
**Risk level:** None""",
            inputSchema={
                "type": "object",
                "properties": {},
                "additionalProperties": False
            }
        )
    ]


async def handle_testing_engine_tool(name: str, args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """
    Handle testing engine tool calls.

    Args:
        name: Tool name
        args: Tool arguments
        mcp_service: MCP service instance

    Returns:
        List of TextContent responses
    """
    try:
        if name == "checklist_get":
            return await _handle_checklist_get(args, mcp_service)
        elif name == "checklist_analyze_exchange":
            return await _handle_analyze_exchange(args, mcp_service)
        elif name == "testing_build_matrix":
            return await _handle_build_matrix(args, mcp_service)
        elif name == "testing_next":
            return await _handle_testing_next(args, mcp_service)
        elif name == "testing_should_continue":
            return await _handle_should_continue(args, mcp_service)
        elif name == "testing_record_auth_change":
            return await _handle_record_auth_change(args, mcp_service)
        elif name == "testing_status":
            return await _handle_testing_status(args, mcp_service)
        else:
            return [TextContent(type="text", text=f"Unknown testing engine tool: {name}")]

    except Exception as e:
        logger.error(f"Error handling {name}: {e}", exc_info=True)
        return [TextContent(type="text", text=f"Error: {str(e)}")]


async def _handle_checklist_get(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle checklist_get tool."""
    checklist = get_vuln_checklist()
    category = args.get('category')
    endpoint_id = args.get('endpoint_id')
    include_counts = args.get('include_payload_counts', False)

    # Get specs
    if category:
        specs = checklist.get_specs_by_category(category)
    elif endpoint_id:
        # Would need endpoint metadata from world model
        # For now, return all specs
        specs = checklist.get_all_specs()
    else:
        specs = checklist.get_all_specs()

    # Get payload counts if requested
    payload_counts = {}
    if include_counts:
        registry = get_payload_registry()
        payload_counts = registry.get_all_payload_counts()

    # Build response
    specs_data = []
    for spec in specs:
        spec_dict = spec.to_dict()
        if include_counts:
            spec_dict['payload_count'] = payload_counts.get(spec.id, 0)
        specs_data.append(spec_dict)

    result = {
        'total_specs': len(specs_data),
        'specs': specs_data,
        'summary': checklist.get_summary()
    }

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_analyze_exchange(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle checklist_analyze_exchange tool."""
    request = args['request']
    response = args['response']

    analyzer = get_exchange_analyzer()
    analysis = analyzer.analyze(request, response)

    # Convert to dict
    result = {
        'recommended_tests': analysis.recommended_tests,
        'parameter_recommendations': analysis.parameter_recommendations,
        'priority_adjustments': analysis.priority_adjustments,
        'detected_technologies': analysis.detected_technologies,
        'risk_signals': analysis.risk_signals,
        'top_recommendations': analyzer.get_top_recommendations(analysis, limit=10)
    }

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_build_matrix(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle testing_build_matrix tool."""
    base_url = args['base_url']
    endpoint_ids = args.get('endpoint_ids')
    categories = args.get('categories')

    # Get assessment ID from context
    assessment_id = mcp_service.current_assessment_id
    if not assessment_id:
        return [TextContent(type="text", text="Error: No active assessment context")]

    # Fetch endpoints from world model
    from lib.world_model_db import get_world_model_db
    db = await get_world_model_db(assessment_id)

    if endpoint_ids:
        placeholders = ", ".join(f"${i+2}" for i in range(len(endpoint_ids)))
        rows = await db._fetchall(
            f"SELECT * FROM wm_endpoints WHERE assessment_id = $1 AND id IN ({placeholders})",
            (db._assessment_id, *endpoint_ids),
        )
    else:
        rows = await db._fetchall(
            "SELECT * FROM wm_endpoints WHERE assessment_id = $1",
            (db._assessment_id,),
        )
    endpoints = [dict(r) for r in rows]

    if not endpoints:
        return [TextContent(type="text", text=json.dumps({
            'base_url': base_url, 'total_cells': 0,
            'message': 'No endpoints found - run crawler first'
        }, indent=2))]

    # Build matrix rows using CoverageTracker static methods
    vuln_classes = categories  # may be None
    matrix_rows = CoverageTracker.build_matrix(endpoints, base_url, vuln_classes)

    # Persist to wm_coverage_matrix
    inserted = 0
    created_ids = []
    import uuid
    from datetime import datetime, timezone
    for row in matrix_rows:
        try:
            cell_id = str(uuid.uuid4())
            now = datetime.now(timezone.utc)
            rows_result = await db._fetchall(
                "INSERT INTO wm_coverage_matrix (id, assessment_id, endpoint_id, vuln_class, parameter, "
                "tool_name, tool_args, priority, status, created_at, updated_at) "
                "VALUES ($1, $2, $3, $4, $5, $6, $7, $8, 'pending', $9, $10) "
                "ON CONFLICT DO NOTHING RETURNING id",
                (cell_id, db._assessment_id, row["endpoint_id"], row["vuln_class"],
                 row.get("parameter", ""), row.get("tool_name", ""),
                 json.dumps(row.get("tool_args", {})), row.get("priority", 50), now, now),
            )
            if rows_result:
                inserted += 1
                created_ids.append(dict(rows_result[0])["id"])
        except Exception as exc:
            logger.debug("Failed to insert coverage row: %s", exc)

    # Compute stats from DB
    stat_rows = await db._fetchall(
        "SELECT vuln_class, COUNT(*) as cnt FROM wm_coverage_matrix "
        "WHERE assessment_id = $1 GROUP BY vuln_class",
        (db._assessment_id,),
    )
    cells_by_category = {dict(r)["vuln_class"]: dict(r)["cnt"] for r in stat_rows}

    ep_stat_rows = await db._fetchall(
        "SELECT endpoint_id, COUNT(*) as cnt FROM wm_coverage_matrix "
        "WHERE assessment_id = $1 GROUP BY endpoint_id",
        (db._assessment_id,),
    )
    cells_by_endpoint = {dict(r)["endpoint_id"]: dict(r)["cnt"] for r in ep_stat_rows}

    result = {
        'base_url': base_url,
        'total_cells': sum(cells_by_category.values()),
        'cells_by_category': cells_by_category,
        'cells_by_endpoint': cells_by_endpoint,
        'matrix_built': True,
        'rows_inserted': inserted,
        'created_cell_ids': created_ids,
    }

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_testing_next(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle testing_next tool - returns untested cells with metadata."""
    limit = args.get('limit', 5)
    vuln_spec_id = args.get('vuln_spec_id')

    assessment_id = mcp_service.current_assessment_id
    if not assessment_id:
        return [TextContent(type="text", text="Error: No active assessment context")]

    registry = get_payload_registry()

    # Get untested cells from coverage matrix via DB query with endpoint data
    from lib.world_model_db import get_world_model_db
    db = await get_world_model_db(assessment_id)

    if vuln_spec_id:
        rows = await db._fetchall(
            "SELECT cm.*, ep.url as endpoint_url, ep.method as endpoint_method "
            "FROM wm_coverage_matrix cm "
            "JOIN wm_endpoints ep ON cm.endpoint_id = ep.id AND cm.assessment_id = ep.assessment_id "
            "WHERE cm.assessment_id = $1 AND cm.status = 'pending' AND cm.vuln_class = $2 "
            "ORDER BY cm.priority DESC LIMIT $3",
            (db._assessment_id, vuln_spec_id, limit),
        )
    else:
        rows = await db._fetchall(
            "SELECT cm.*, ep.url as endpoint_url, ep.method as endpoint_method "
            "FROM wm_coverage_matrix cm "
            "JOIN wm_endpoints ep ON cm.endpoint_id = ep.id AND cm.assessment_id = ep.assessment_id "
            "WHERE cm.assessment_id = $1 AND cm.status = 'pending' "
            "ORDER BY cm.priority DESC LIMIT $2",
            (db._assessment_id, limit),
        )
    untested_cells = [dict(r) for r in rows]

    # Get coverage summary
    coverage_rows = await db._fetchall(
        "SELECT status, COUNT(*) as cnt FROM wm_coverage_matrix "
        "WHERE assessment_id = $1 GROUP BY status",
        (db._assessment_id,),
    )
    status_counts = {dict(r)["status"]: dict(r)["cnt"] for r in coverage_rows}
    total_cells = sum(status_counts.values())
    pending_cells = status_counts.get("pending", 0)
    tested_cells = total_cells - pending_cells
    coverage_pct = round(tested_cells / max(total_cells, 1) * 100, 1) if total_cells > 0 else 0

    # Load spec details and payload count for each cell
    checklist = get_vuln_checklist()
    cells_data = []
    for cell in untested_cells:
        spec_id = cell['vuln_class']
        payloads = registry.get_payloads(spec_id)

        # Include checklist spec details for context
        spec = checklist.get_spec_by_id(spec_id)

        cell_data = {
            'cell_id': cell['id'],
            'endpoint_id': cell['endpoint_id'],
            'endpoint_url': cell.get('endpoint_url', ''),
            'endpoint_method': cell.get('endpoint_method', 'GET'),
            'vuln_spec_id': spec_id,
            'parameter': cell.get('parameter'),
            'payload_count': len(payloads),
            'priority': cell.get('priority', 0),
        }

        if spec:
            cell_data['spec_name'] = spec.name
            cell_data['spec_category'] = spec.category
            cell_data['spec_description'] = spec.description
            cell_data['severity_range'] = spec.severity_range

        cells_data.append(cell_data)

    result = {
        'cells_count': len(cells_data),
        'cells': cells_data,
        'coverage_summary': {
            'total_cells': total_cells,
            'tested_cells': tested_cells,
            'pending_cells': pending_cells,
            'coverage_pct': coverage_pct
        }
    }

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_should_continue(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle testing_should_continue tool - returns factual state data."""
    vuln_spec_id = args['vuln_spec_id']
    endpoint_id = args['endpoint_id']
    parameter = args.get('parameter')
    results_so_far = args.get('results_so_far', [])

    assessment_id = mcp_service.current_assessment_id
    if not assessment_id:
        return [TextContent(type="text", text="Error: No active assessment context")]

    base_url = mcp_service.current_base_url or "unknown"
    orchestrator = get_orchestrator(assessment_id, base_url)

    state = orchestrator.get_cell_testing_state(
        vuln_spec_id,
        endpoint_id,
        parameter,
        results_so_far
    )

    result = {
        'cell_id': state.cell_id,
        'vuln_spec_id': state.vuln_spec_id,
        'endpoint_id': state.endpoint_id,
        'parameter': state.parameter,
        'attempt_count': state.attempt_count,
        'max_attempts': state.max_attempts,
        'payload_types_tried': state.payload_types_tried,
        'suspicious_signal_count': state.suspicious_signal_count,
        'suspicious_signals': state.suspicious_signals,
        'vulnerability_confirmed': state.vulnerability_confirmed,
        'max_attempts_reached': state.max_attempts_reached,
        'available_payload_count': state.available_payload_count,
        'results_summary': state.results_summary
    }

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_record_auth_change(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle testing_record_auth_change tool."""
    identity_id = args['identity_id']
    change_type = args['change_type']
    details = args.get('details', {})

    assessment_id = mcp_service.current_assessment_id
    if not assessment_id:
        return [TextContent(type="text", text="Error: No active assessment context")]

    base_url = mcp_service.current_base_url or "unknown"
    orchestrator = get_orchestrator(assessment_id, base_url)

    result = orchestrator.record_auth_change(identity_id, change_type, details)

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_testing_status(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle testing_status tool."""
    assessment_id = mcp_service.current_assessment_id
    if not assessment_id:
        return [TextContent(type="text", text="Error: No active assessment context")]

    base_url = mcp_service.current_base_url or "unknown"
    try:
        orchestrator = get_orchestrator(assessment_id, base_url)

        # Sync progress from coverage matrix before reporting
        matrix_count = 0
        try:
            from lib.world_model_db import get_world_model_db
            db = await get_world_model_db(assessment_id)
            progress_rows = await db._fetchall(
                "SELECT status, COUNT(*) as cnt FROM wm_coverage_matrix "
                "WHERE assessment_id = $1 GROUP BY status",
                (db._assessment_id,),
            )
            counts = {dict(r)["status"]: dict(r)["cnt"] for r in progress_rows}
            total = sum(counts.values())
            matrix_count = total
            pending = counts.get("pending", 0)
            orchestrator.update_progress({
                "total_cells": total,
                "tested_cells": total - pending,
                "confirmed_vulns": counts.get("vulnerable", 0),
                "suspicious_cells": counts.get("in_progress", 0),  # suspicious uses in_progress for now
            })
        except Exception:
            pass  # Return stale progress if DB unavailable

        status = orchestrator.get_status()
    except ValueError:
        # No orchestrator yet, return initial state
        status = {
            'state': 'init',
            'progress': {'total_cells': 0, 'tested_cells': 0, 'confirmed_vulns': 0},
            'message': 'Testing not started - run testing_build_matrix to begin'
        }
        matrix_count = 0

    # Add endpoint completeness check (Gap 6)
    try:
        from lib.world_model_db import get_world_model_db
        db = await get_world_model_db(assessment_id)

        all_endpoints = await db._fetchall(
            "SELECT id, method, path FROM wm_endpoints WHERE assessment_id = $1", (db._assessment_id,))
        covered_eps = await db._fetchall(
            "SELECT DISTINCT endpoint_id FROM wm_coverage_matrix WHERE assessment_id = $1", (db._assessment_id,))

        all_ep_ids = {dict(e)["id"] for e in all_endpoints}
        covered_ep_ids = {dict(e)["endpoint_id"] for e in covered_eps}
        uncovered = all_ep_ids - covered_ep_ids

        uncovered_details = []
        if uncovered:
            for ep in all_endpoints:
                ep_dict = dict(ep)
                if ep_dict["id"] in uncovered:
                    uncovered_details.append({
                        "endpoint_id": ep_dict["id"],
                        "method": ep_dict["method"],
                        "path": ep_dict["path"]
                    })

        coverage_pct = round(len(covered_ep_ids) / max(len(all_ep_ids), 1) * 100, 1) if all_ep_ids else 0

        status["endpoint_completeness"] = {
            "total_known_endpoints": len(all_ep_ids),
            "endpoints_in_matrix": len(covered_ep_ids),
            "uncovered_endpoints": len(uncovered),
            "coverage_percentage": coverage_pct,
            "uncovered_details": uncovered_details[:15],  # Limit to first 15
            "recommendation": (
                f"Run testing_build_matrix to add {len(uncovered)} uncovered endpoints"
                if uncovered else "All endpoints covered in matrix"
            ),
        }
    except Exception as exc:
        logger.debug(f"Failed to compute endpoint completeness: {exc}")
        status["endpoint_completeness"] = {"error": str(exc)}

    # Add checklist compliance check (Gap 3)
    try:
        checklist = get_vuln_checklist()
        all_spec_ids = {spec.id for spec in checklist.get_all_specs()}

        if matrix_count > 0:
            from lib.world_model_db import get_world_model_db
            db = await get_world_model_db(assessment_id)
            tested_classes = await db._fetchall(
                "SELECT DISTINCT vuln_class FROM wm_coverage_matrix "
                "WHERE assessment_id = $1 AND status != 'pending'", (db._assessment_id,))
            tested_class_ids = {dict(r)["vuln_class"] for r in tested_classes}
        else:
            tested_class_ids = set()

        untested = all_spec_ids - tested_class_ids
        compliance_pct = round(len(tested_class_ids) / max(len(all_spec_ids), 1) * 100, 1) if all_spec_ids else 0

        status["checklist_compliance"] = {
            "matrix_built": matrix_count > 0,
            "total_vuln_classes": len(all_spec_ids),
            "classes_with_test_activity": len(tested_class_ids),
            "untested_vuln_classes": sorted(list(untested))[:15],  # First 15
            "compliance_percentage": compliance_pct,
            "warning": (
                "COVERAGE MATRIX NOT BUILT - Run testing_build_matrix first"
                if matrix_count == 0 else None
            ),
        }
    except Exception as exc:
        logger.debug(f"Failed to compute checklist compliance: {exc}")
        status["checklist_compliance"] = {"error": str(exc)}

    return [TextContent(type="text", text=json.dumps(status, indent=2))]
