"""
Testing Engine MCP Tools

Provides 7 new tools for the comprehensive vulnerability testing engine:
- checklist_get: List all vulnerability test specifications
- checklist_analyze_exchange: Analyze request/response for test recommendations
- testing_build_matrix: Build extended 42-class coverage matrix
- testing_next: Get next N test cells with pre-loaded payloads
- testing_should_continue: Ask if enough testing done for a spec/endpoint/param
- testing_record_auth_change: Notify that auth state changed (triggers re-crawl)
- testing_status: Get comprehensive testing progress
"""

from typing import List, Dict, Any, Optional
import json
import logging

from mcp.types import Tool, TextContent

from lib.vuln_checklist import get_vuln_checklist
from lib.exchange_analyzer import get_exchange_analyzer
from lib.payload_registry import get_payload_registry
from lib.test_orchestrator import get_orchestrator
from lib.coverage_tracker import CoverageTracker

logger = logging.getLogger(__name__)


def get_testing_engine_tools() -> List[Tool]:
    """Get all testing engine tools."""
    return [
        Tool(
            name="checklist_get",
            description="""List all 42+ vulnerability test specifications, optionally filtered by category or endpoint.

**When to use:**
- At the start of testing to understand available test categories
- To check which tests apply to a specific endpoint
- To get payload counts for test planning

**Input:**
- category (optional): Filter by category (injection|auth|config|client_side|api|business_logic|info_leak)
- endpoint_id (optional): Filter to tests applicable for specific endpoint
- include_payload_counts (optional): Include number of available payloads per spec

**Returns:**
List of vulnerability specs with metadata:
- id, name, category, description
- tool_name, requires_auth, parameter_bound
- owasp_category, severity_range
- payload_count (if requested)

**Budget impact:** Minimal (read-only)
**Risk level:** None""",
            inputSchema={
                "type": "object",
                "properties": {
                    "category": {"type": "string", "description": "Filter by category"},
                    "endpoint_id": {"type": "string", "description": "Filter to applicable tests for endpoint"},
                    "include_payload_counts": {"type": "boolean", "description": "Include payload counts"}
                },
                "additionalProperties": False
            }
        ),

        Tool(
            name="checklist_analyze_exchange",
            description="""Analyze a request/response pair to recommend additional vulnerability tests beyond the static checklist.

**When to use:**
- After probing an endpoint to get personalized test recommendations
- To identify which parameters are highest risk
- To detect technologies and configuration issues

**Dependencies:**
- Requires a request/response pair (from endpoint_probe or http_send)

**Input:**
- request: {method, url, headers, body, parameters}
- response: {status, headers, body, time_ms}

**Returns:**
ExchangeAnalysis with:
- recommended_tests: List of vuln spec IDs to run
- parameter_recommendations: Which tests to run on which params
- priority_adjustments: Tests that should be prioritized
- detected_technologies: Tech stack detected
- risk_signals: Notable security observations

**Budget impact:** Minimal (analysis only)
**Risk level:** None
**Failure modes:** Invalid request/response format""",
            inputSchema={
                "type": "object",
                "properties": {
                    "request": {
                        "type": "object",
                        "description": "HTTP request to analyze",
                        "properties": {
                            "method": {"type": "string"},
                            "url": {"type": "string"},
                            "headers": {"type": "object"},
                            "body": {"type": "string"},
                            "parameters": {"type": "array"}
                        },
                        "required": ["method", "url"]
                    },
                    "response": {
                        "type": "object",
                        "description": "HTTP response to analyze",
                        "properties": {
                            "status": {"type": "integer"},
                            "headers": {"type": "object"},
                            "body": {"type": "string"},
                            "time_ms": {"type": "number"}
                        },
                        "required": ["status", "headers", "body"]
                    }
                },
                "required": ["request", "response"],
                "additionalProperties": False
            }
        ),

        Tool(
            name="testing_build_matrix",
            description="""Build extended 42-class coverage matrix for discovered endpoints.

**When to use:**
- After crawling completes to build the full test matrix
- After re-crawl to extend matrix with new endpoints
- To get cell count estimates for planning

**Dependencies:**
- Requires prior crawl (crawler_start + crawler_results)
- Endpoint data in world model

**Input:**
- base_url: Target base URL
- endpoint_ids (optional): Specific endpoints (default: all)
- categories (optional): Limit to specific vuln categories

**Returns:**
Matrix summary:
- total_cells: Total test cells in matrix
- cells_by_category: Breakdown per vuln category
- cells_by_endpoint: Breakdown per endpoint
- estimated_duration: Rough time estimate

**Budget impact:** Moderate (builds in-memory matrix)
**Risk level:** None
**Failure modes:** No endpoints found, database connection""",
            inputSchema={
                "type": "object",
                "properties": {
                    "base_url": {"type": "string", "description": "Target base URL"},
                    "endpoint_ids": {"type": "array", "items": {"type": "string"}},
                    "categories": {"type": "array", "items": {"type": "string"}}
                },
                "required": ["base_url"],
                "additionalProperties": False
            }
        ),

        Tool(
            name="testing_next",
            description="""Get next N test cells with pre-loaded payloads, ready for execution.

**When to use:**
- In the main testing loop to get next batch of tests
- Use this instead of manually querying coverage matrix + loading payloads

**Dependencies:**
- Requires coverage matrix (testing_build_matrix)

**Input:**
- limit: Maximum number of cells to return (default: 5)
- vuln_spec_id (optional): Filter to specific vuln class
- strategy: Payload selection strategy (default: "hardcoded_first")

**Returns:**
List of test cells, each with:
- cell_id, endpoint_id, vuln_spec_id, parameter
- payloads: Pre-loaded payload list (ready to use)
- tool_call: Suggested MCP tool invocation
- priority: Test priority score

**Budget impact:** Low
**Risk level:** None
**Failure modes:** Matrix not built, no untested cells remaining""",
            inputSchema={
                "type": "object",
                "properties": {
                    "limit": {"type": "integer", "default": 5, "description": "Max cells to return"},
                    "vuln_spec_id": {"type": "string", "description": "Filter to specific vuln class"},
                    "strategy": {
                        "type": "string",
                        "enum": ["hardcoded_first", "llm_first", "mixed"],
                        "default": "hardcoded_first"
                    }
                },
                "additionalProperties": False
            }
        ),

        Tool(
            name="testing_should_continue",
            description="""Ask if enough testing has been done for a specific vuln/endpoint/parameter combination.

**When to use:**
- After running hardcoded payloads to decide next steps
- To get structured guidance on whether to generate LLM payloads
- To avoid over-testing clean endpoints

**Dependencies:**
- Requires prior test results for the target

**Input:**
- vuln_spec_id: Vulnerability spec being tested
- endpoint_id: Endpoint being tested
- parameter (optional): Parameter being tested
- results_so_far (optional): Test results to analyze

**Returns:**
ContinueDecision with:
- should_continue: bool
- recommendation: continue_hardcoded | generate_llm_payloads | move_to_next | log_suspicious
- reasoning: Why this decision
- confidence: Confidence score (0.0-1.0)
- next_payload_type: What type of payloads to use next

**Budget impact:** Minimal
**Risk level:** None""",
            inputSchema={
                "type": "object",
                "properties": {
                    "vuln_spec_id": {"type": "string", "description": "Vuln spec ID"},
                    "endpoint_id": {"type": "string", "description": "Endpoint ID"},
                    "parameter": {"type": "string", "description": "Parameter (if parameter-bound)"},
                    "results_so_far": {"type": "array", "description": "Test results"}
                },
                "required": ["vuln_spec_id", "endpoint_id"],
                "additionalProperties": False
            }
        ),

        Tool(
            name="testing_record_auth_change",
            description="""Notify that authentication state changed, triggering a re-crawl workflow.

**When to use:**
- After discovering new credentials (credentials_add)
- After privilege escalation
- After obtaining a new session token

**Dependencies:**
- Identity must exist (auth_get_identities)

**Input:**
- identity_id: Identity that changed
- change_type: new_credential | privilege_escalation | session_token

**Returns:**
- status: recorded
- recrawl_scheduled: bool
- next_actions: What to do next (crawler_start, coverage_discover, etc.)

**Budget impact:** Low (schedules re-crawl)
**Risk level:** None
**Failure modes:** Invalid identity_id""",
            inputSchema={
                "type": "object",
                "properties": {
                    "identity_id": {"type": "string", "description": "Identity ID"},
                    "change_type": {
                        "type": "string",
                        "enum": ["new_credential", "privilege_escalation", "session_token"],
                        "description": "Type of auth change"
                    },
                    "details": {"type": "object", "description": "Additional context"}
                },
                "required": ["identity_id", "change_type"],
                "additionalProperties": False
            }
        ),

        Tool(
            name="testing_status",
            description="""Get comprehensive testing progress and orchestrator status.

**When to use:**
- Periodically during testing to check progress
- To get guidance on next actions
- To check if testing is complete

**Dependencies:** None

**Input:** None (uses current assessment context)

**Returns:**
Status with:
- state: Current orchestrator state
- progress: Coverage %, cells tested, vulns found
- crawl_count: Number of crawls performed
- pending_recrawls: Re-crawls scheduled
- auth_changes: Recent auth state changes
- recommendations: Next 5 recommended actions

**Budget impact:** Minimal
**Risk level:** None""",
            inputSchema={
                "type": "object",
                "properties": {},
                "additionalProperties": False
            }
        )
    ]


async def handle_testing_engine_tool(name: str, args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """
    Handle testing engine tool calls.

    Args:
        name: Tool name
        args: Tool arguments
        mcp_service: MCP service instance

    Returns:
        List of TextContent responses
    """
    try:
        if name == "checklist_get":
            return await _handle_checklist_get(args, mcp_service)
        elif name == "checklist_analyze_exchange":
            return await _handle_analyze_exchange(args, mcp_service)
        elif name == "testing_build_matrix":
            return await _handle_build_matrix(args, mcp_service)
        elif name == "testing_next":
            return await _handle_testing_next(args, mcp_service)
        elif name == "testing_should_continue":
            return await _handle_should_continue(args, mcp_service)
        elif name == "testing_record_auth_change":
            return await _handle_record_auth_change(args, mcp_service)
        elif name == "testing_status":
            return await _handle_testing_status(args, mcp_service)
        else:
            return [TextContent(type="text", text=f"Unknown testing engine tool: {name}")]

    except Exception as e:
        logger.error(f"Error handling {name}: {e}", exc_info=True)
        return [TextContent(type="text", text=f"Error: {str(e)}")]


async def _handle_checklist_get(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle checklist_get tool."""
    checklist = get_vuln_checklist()
    category = args.get('category')
    endpoint_id = args.get('endpoint_id')
    include_counts = args.get('include_payload_counts', False)

    # Get specs
    if category:
        specs = checklist.get_specs_by_category(category)
    elif endpoint_id:
        # Would need endpoint metadata from world model
        # For now, return all specs
        specs = checklist.get_all_specs()
    else:
        specs = checklist.get_all_specs()

    # Get payload counts if requested
    payload_counts = {}
    if include_counts:
        registry = get_payload_registry()
        payload_counts = registry.get_all_payload_counts()

    # Build response
    specs_data = []
    for spec in specs:
        spec_dict = spec.to_dict()
        if include_counts:
            spec_dict['payload_count'] = payload_counts.get(spec.id, 0)
        specs_data.append(spec_dict)

    result = {
        'total_specs': len(specs_data),
        'specs': specs_data,
        'summary': checklist.get_summary()
    }

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_analyze_exchange(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle checklist_analyze_exchange tool."""
    request = args['request']
    response = args['response']

    analyzer = get_exchange_analyzer()
    analysis = analyzer.analyze(request, response)

    # Convert to dict
    result = {
        'recommended_tests': analysis.recommended_tests,
        'parameter_recommendations': analysis.parameter_recommendations,
        'priority_adjustments': analysis.priority_adjustments,
        'detected_technologies': analysis.detected_technologies,
        'risk_signals': analysis.risk_signals,
        'top_recommendations': analyzer.get_top_recommendations(analysis, limit=10)
    }

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_build_matrix(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle testing_build_matrix tool."""
    base_url = args['base_url']
    endpoint_ids = args.get('endpoint_ids')
    categories = args.get('categories')

    # Get assessment ID from context
    assessment_id = mcp_service.current_assessment_id
    if not assessment_id:
        return [TextContent(type="text", text="Error: No active assessment context")]

    # Fetch endpoints from world model
    from lib.world_model_db import get_world_model_db
    db = await get_world_model_db(assessment_id)

    if endpoint_ids:
        placeholders = ", ".join(f"${i+2}" for i in range(len(endpoint_ids)))
        rows = await db._fetchall(
            f"SELECT * FROM wm_endpoints WHERE assessment_id = $1 AND id IN ({placeholders})",
            (db._assessment_id, *endpoint_ids),
        )
    else:
        rows = await db._fetchall(
            "SELECT * FROM wm_endpoints WHERE assessment_id = $1",
            (db._assessment_id,),
        )
    endpoints = [dict(r) for r in rows]

    if not endpoints:
        return [TextContent(type="text", text=json.dumps({
            'base_url': base_url, 'total_cells': 0,
            'message': 'No endpoints found - run crawler first'
        }, indent=2))]

    # Build matrix rows using CoverageTracker static methods
    vuln_classes = categories  # may be None
    matrix_rows = CoverageTracker.build_matrix(endpoints, base_url, vuln_classes)

    # Persist to wm_coverage_matrix
    inserted = 0
    created_ids = []
    import uuid
    from datetime import datetime, timezone
    for row in matrix_rows:
        try:
            cell_id = str(uuid.uuid4())
            now = datetime.now(timezone.utc)
            rows_result = await db._fetchall(
                "INSERT INTO wm_coverage_matrix (id, assessment_id, endpoint_id, vuln_class, parameter, "
                "tool_name, tool_args, priority, status, created_at, updated_at) "
                "VALUES ($1, $2, $3, $4, $5, $6, $7, $8, 'pending', $9, $10) "
                "ON CONFLICT DO NOTHING RETURNING id",
                (cell_id, db._assessment_id, row["endpoint_id"], row["vuln_class"],
                 row.get("parameter", ""), row.get("tool_name", ""),
                 json.dumps(row.get("tool_args", {})), row.get("priority", 50), now, now),
            )
            if rows_result:
                inserted += 1
                created_ids.append(dict(rows_result[0])["id"])
        except Exception as exc:
            logger.debug("Failed to insert coverage row: %s", exc)

    # Compute stats from DB
    stat_rows = await db._fetchall(
        "SELECT vuln_class, COUNT(*) as cnt FROM wm_coverage_matrix "
        "WHERE assessment_id = $1 GROUP BY vuln_class",
        (db._assessment_id,),
    )
    cells_by_category = {dict(r)["vuln_class"]: dict(r)["cnt"] for r in stat_rows}

    ep_stat_rows = await db._fetchall(
        "SELECT endpoint_id, COUNT(*) as cnt FROM wm_coverage_matrix "
        "WHERE assessment_id = $1 GROUP BY endpoint_id",
        (db._assessment_id,),
    )
    cells_by_endpoint = {dict(r)["endpoint_id"]: dict(r)["cnt"] for r in ep_stat_rows}

    result = {
        'base_url': base_url,
        'total_cells': sum(cells_by_category.values()),
        'cells_by_category': cells_by_category,
        'cells_by_endpoint': cells_by_endpoint,
        'matrix_built': True,
        'rows_inserted': inserted,
        'created_cell_ids': created_ids,
    }

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_testing_next(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle testing_next tool."""
    limit = args.get('limit', 5)
    vuln_spec_id = args.get('vuln_spec_id')
    strategy = args.get('strategy', 'hardcoded_first')

    assessment_id = mcp_service.current_assessment_id
    if not assessment_id:
        return [TextContent(type="text", text="Error: No active assessment context")]

    registry = get_payload_registry()

    # Get untested cells from coverage matrix via DB query
    from lib.world_model_db import get_world_model_db
    db = await get_world_model_db(assessment_id)

    if vuln_spec_id:
        rows = await db._fetchall(
            "SELECT * FROM wm_coverage_matrix WHERE assessment_id = $1 AND status = 'pending' "
            "AND vuln_class = $2 ORDER BY priority DESC LIMIT $3",
            (db._assessment_id, vuln_spec_id, limit),
        )
    else:
        rows = await db._fetchall(
            "SELECT * FROM wm_coverage_matrix WHERE assessment_id = $1 AND status = 'pending' "
            "ORDER BY priority DESC LIMIT $2",
            (db._assessment_id, limit),
        )
    untested_cells = [dict(r) for r in rows]

    # Load payloads and spec details for each cell
    checklist = get_vuln_checklist()
    cells_with_payloads = []
    for cell in untested_cells:
        spec_id = cell['vuln_class']
        payloads = registry.get_payloads(spec_id)

        # Convert to strings if needed
        payload_strings = registry.to_string_payloads(payloads)

        # Include checklist spec details for context
        spec = checklist.get_spec_by_id(spec_id)
        tool_name = spec.tool_name if spec else 'fuzz_endpoint'
        base_args = spec.base_args if spec else {}

        cell_data = {
            'cell_id': cell['id'],
            'endpoint_id': cell['endpoint_id'],
            'vuln_spec_id': spec_id,
            'parameter': cell.get('parameter'),
            'payload_count': len(payload_strings),
            'payloads': payload_strings[:10],  # First 10 for preview
            'priority': cell.get('priority', 0),
            'tool_call_hint': {
                'tool': tool_name,
                'args': {
                    **base_args,
                    'vuln_class': spec_id,
                    'payloads': payload_strings
                },
                'note': 'This is a suggested tool call template. Adjust args based on the actual tool signature.'
            }
        }

        if spec:
            cell_data["spec_details"] = {
                "name": spec.name,
                "category": spec.category,
                "tool_name": spec.tool_name,
                "base_args": spec.base_args,
                "severity_range": spec.severity_range,
                "description": spec.description,
            }

        cells_with_payloads.append(cell_data)

    result = {
        'cells_count': len(cells_with_payloads),
        'cells': cells_with_payloads,
        'strategy': strategy
    }

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_should_continue(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle testing_should_continue tool."""
    vuln_spec_id = args['vuln_spec_id']
    endpoint_id = args['endpoint_id']
    parameter = args.get('parameter')
    results_so_far = args.get('results_so_far', [])

    assessment_id = mcp_service.current_assessment_id
    if not assessment_id:
        return [TextContent(type="text", text="Error: No active assessment context")]

    base_url = mcp_service.current_base_url or "unknown"
    orchestrator = get_orchestrator(assessment_id, base_url)

    decision = orchestrator.should_continue_testing(
        vuln_spec_id,
        endpoint_id,
        parameter,
        results_so_far
    )

    result = {
        'should_continue': decision.should_continue,
        'recommendation': decision.recommendation,
        'reasoning': decision.reasoning,
        'confidence': decision.confidence,
        'next_payload_type': decision.next_payload_type
    }

    # When recommending LLM payload generation, include structured context
    if decision.recommendation == 'generate_llm_payloads':
        try:
            result["payload_generation_context"] = orchestrator.get_payload_generation_context(
                vuln_spec_id=vuln_spec_id,
                endpoint_id=endpoint_id,
                parameter=parameter,
                results_so_far=results_so_far,
            )
        except Exception as exc:
            logger.debug("Failed to build payload generation context: %s", exc)

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_record_auth_change(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle testing_record_auth_change tool."""
    identity_id = args['identity_id']
    change_type = args['change_type']
    details = args.get('details', {})

    assessment_id = mcp_service.current_assessment_id
    if not assessment_id:
        return [TextContent(type="text", text="Error: No active assessment context")]

    base_url = mcp_service.current_base_url or "unknown"
    orchestrator = get_orchestrator(assessment_id, base_url)

    result = orchestrator.record_auth_change(identity_id, change_type, details)

    return [TextContent(type="text", text=json.dumps(result, indent=2))]


async def _handle_testing_status(args: Dict[str, Any], mcp_service: Any) -> List[TextContent]:
    """Handle testing_status tool."""
    assessment_id = mcp_service.current_assessment_id
    if not assessment_id:
        return [TextContent(type="text", text="Error: No active assessment context")]

    base_url = mcp_service.current_base_url or "unknown"
    try:
        orchestrator = get_orchestrator(assessment_id, base_url)

        # Sync progress from coverage matrix before reporting
        matrix_count = 0
        try:
            from lib.world_model_db import get_world_model_db
            db = await get_world_model_db(assessment_id)
            progress_rows = await db._fetchall(
                "SELECT status, COUNT(*) as cnt FROM wm_coverage_matrix "
                "WHERE assessment_id = $1 GROUP BY status",
                (db._assessment_id,),
            )
            counts = {dict(r)["status"]: dict(r)["cnt"] for r in progress_rows}
            total = sum(counts.values())
            matrix_count = total
            pending = counts.get("pending", 0)
            orchestrator.update_progress({
                "total_cells": total,
                "tested_cells": total - pending,
                "confirmed_vulns": counts.get("vulnerable", 0),
                "suspicious_cells": counts.get("in_progress", 0),  # suspicious uses in_progress for now
            })
        except Exception:
            pass  # Return stale progress if DB unavailable

        status = orchestrator.get_status()
    except ValueError:
        # No orchestrator yet, return initial state
        status = {
            'state': 'init',
            'progress': {'total_cells': 0, 'tested_cells': 0, 'confirmed_vulns': 0},
            'message': 'Testing not started - run testing_build_matrix to begin'
        }
        matrix_count = 0

    # Add endpoint completeness check (Gap 6)
    try:
        from lib.world_model_db import get_world_model_db
        db = await get_world_model_db(assessment_id)

        all_endpoints = await db._fetchall(
            "SELECT id, method, path FROM wm_endpoints WHERE assessment_id = $1", (db._assessment_id,))
        covered_eps = await db._fetchall(
            "SELECT DISTINCT endpoint_id FROM wm_coverage_matrix WHERE assessment_id = $1", (db._assessment_id,))

        all_ep_ids = {dict(e)["id"] for e in all_endpoints}
        covered_ep_ids = {dict(e)["endpoint_id"] for e in covered_eps}
        uncovered = all_ep_ids - covered_ep_ids

        uncovered_details = []
        if uncovered:
            for ep in all_endpoints:
                ep_dict = dict(ep)
                if ep_dict["id"] in uncovered:
                    uncovered_details.append({
                        "endpoint_id": ep_dict["id"],
                        "method": ep_dict["method"],
                        "path": ep_dict["path"]
                    })

        coverage_pct = round(len(covered_ep_ids) / max(len(all_ep_ids), 1) * 100, 1) if all_ep_ids else 0

        status["endpoint_completeness"] = {
            "total_known_endpoints": len(all_ep_ids),
            "endpoints_in_matrix": len(covered_ep_ids),
            "uncovered_endpoints": len(uncovered),
            "coverage_percentage": coverage_pct,
            "uncovered_details": uncovered_details[:15],  # Limit to first 15
            "recommendation": (
                f"Run testing_build_matrix to add {len(uncovered)} uncovered endpoints"
                if uncovered else "All endpoints covered in matrix"
            ),
        }
    except Exception as exc:
        logger.debug(f"Failed to compute endpoint completeness: {exc}")
        status["endpoint_completeness"] = {"error": str(exc)}

    # Add checklist compliance check (Gap 3)
    try:
        checklist = get_vuln_checklist()
        all_spec_ids = {spec.id for spec in checklist.get_all_specs()}

        if matrix_count > 0:
            from lib.world_model_db import get_world_model_db
            db = await get_world_model_db(assessment_id)
            tested_classes = await db._fetchall(
                "SELECT DISTINCT vuln_class FROM wm_coverage_matrix "
                "WHERE assessment_id = $1 AND status != 'pending'", (db._assessment_id,))
            tested_class_ids = {dict(r)["vuln_class"] for r in tested_classes}
        else:
            tested_class_ids = set()

        untested = all_spec_ids - tested_class_ids
        compliance_pct = round(len(tested_class_ids) / max(len(all_spec_ids), 1) * 100, 1) if all_spec_ids else 0

        status["checklist_compliance"] = {
            "matrix_built": matrix_count > 0,
            "total_vuln_classes": len(all_spec_ids),
            "classes_with_test_activity": len(tested_class_ids),
            "untested_vuln_classes": sorted(list(untested))[:15],  # First 15
            "compliance_percentage": compliance_pct,
            "warning": (
                "COVERAGE MATRIX NOT BUILT - Run testing_build_matrix first"
                if matrix_count == 0 else None
            ),
        }
    except Exception as exc:
        logger.debug(f"Failed to compute checklist compliance: {exc}")
        status["checklist_compliance"] = {"error": str(exc)}

    return [TextContent(type="text", text=json.dumps(status, indent=2))]
